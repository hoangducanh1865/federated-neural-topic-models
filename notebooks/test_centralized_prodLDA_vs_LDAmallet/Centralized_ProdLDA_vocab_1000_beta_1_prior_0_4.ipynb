{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sib1HSks6Xqv"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from scipy.special import softmax\n",
        "import multiprocessing as mp\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "from sklearn.preprocessing import normalize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CObHMSd6LLz"
      },
      "source": [
        "# Installing ProdLDA\n",
        "**Restart notbook after the installation!!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-VDAzxJA6FK5",
        "outputId": "2ec185f0-ce26-4941-eb06-5dcf079b5dff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'PyTorchAVITM'...\n",
            "remote: Enumerating objects: 19052, done.\u001b[K\n",
            "remote: Total 19052 (delta 0), reused 0 (delta 0), pack-reused 19052\u001b[K\n",
            "Receiving objects: 100% (19052/19052), 132.62 MiB | 24.38 MiB/s, done.\n",
            "Resolving deltas: 100% (89/89), done.\n",
            "Checking out files: 100% (37/37), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/estebandito22/PyTorchAVITM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6kW5jO66UKj"
      },
      "source": [
        "# 1. Creation of synthetic corpus\n",
        "\n",
        "We consider a scenario with n parties, each of them as an associated corpus.\n",
        "To generate the corpus associated with each of the parties, we consider a common beta distribution (word-topic distribution), but we freeze different topics/ assign different asymmetric Dirichlet priors favoring different topics at the time of generating the document that composes each party's corpus."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twSZ3G0p6d1z"
      },
      "source": [
        "## 1.1. Function for permuting the Dirichlet prior at each node"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tXdkpdrh6Thn"
      },
      "outputs": [],
      "source": [
        "def rotateArray(arr, n, d):\n",
        "    temp = []\n",
        "    i = 0\n",
        "    while (i < d):\n",
        "        temp.append(arr[i])\n",
        "        i = i + 1\n",
        "    i = 0\n",
        "    while (d < n):\n",
        "        arr[i] = arr[d]\n",
        "        i = i + 1\n",
        "        d = d + 1\n",
        "    arr[:] = arr[: i] + temp\n",
        "    return arr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AyFA9eGH6hGH"
      },
      "source": [
        "## 1.2. Topic modeling and node settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5DmfSiuR6iI0",
        "outputId": "c072aa07-2e13-4cb8-a953-ebec8bf6f134"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.1, 0.1, 0.1, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4]\n"
          ]
        }
      ],
      "source": [
        "# Topic modeling settings\n",
        "vocab_size = 1000\n",
        "n_topics = 10\n",
        "beta = 1\n",
        "alpha = 5/n_topics\n",
        "n_docs = 1000\n",
        "nwords = (150, 450) #Min and max lengths of the documents\n",
        "\n",
        "# Nodes settings\n",
        "n_nodes = 5\n",
        "frozen_topics = 3\n",
        "dirichlet_symmetric = False\n",
        "prior = (n_topics)*[0.4]\n",
        "prior[0] = prior[1] = prior[2] = 0.1\n",
        "print(prior)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ylo9Vsu6zpX"
      },
      "source": [
        "## 1.3. Topics generation (common for all nodes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uy3AuOSx6qc1",
        "outputId": "ec0c2c6f-ce18-44b5-ca02-cc22215e5bbb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ordered probabilities for the first topic vector:\n",
            "[1.00462614e-02 7.32736572e-03 6.36683414e-03 6.28109133e-03\n",
            " 6.09683089e-03 5.09364935e-03 5.09216451e-03 5.00001130e-03\n",
            " 4.88863195e-03 4.81524397e-03 4.69130198e-03 4.60975629e-03\n",
            " 4.60940861e-03 4.59727407e-03 4.59346375e-03 4.16128897e-03\n",
            " 4.08170614e-03 4.05354888e-03 3.87123967e-03 3.81795332e-03\n",
            " 3.77493566e-03 3.62689555e-03 3.61005626e-03 3.60913327e-03\n",
            " 3.56100173e-03 3.50657564e-03 3.48969217e-03 3.48069216e-03\n",
            " 3.43835628e-03 3.40208399e-03 3.38384436e-03 3.38221910e-03\n",
            " 3.32680615e-03 3.31788723e-03 3.31039883e-03 3.26466910e-03\n",
            " 3.24123239e-03 3.17337306e-03 3.16658779e-03 3.15605661e-03\n",
            " 3.12685700e-03 3.12291232e-03 3.09627376e-03 3.06502122e-03\n",
            " 3.05497566e-03 3.05476927e-03 3.02287323e-03 3.00993296e-03\n",
            " 2.99795531e-03 2.99305525e-03 2.98096753e-03 2.97499598e-03\n",
            " 2.96830849e-03 2.96585455e-03 2.92160597e-03 2.91671342e-03\n",
            " 2.91009636e-03 2.87627049e-03 2.87059317e-03 2.86544155e-03\n",
            " 2.83889360e-03 2.80270139e-03 2.79270941e-03 2.76354382e-03\n",
            " 2.76307189e-03 2.76159693e-03 2.74054732e-03 2.73614831e-03\n",
            " 2.71296678e-03 2.68216845e-03 2.65157370e-03 2.64969118e-03\n",
            " 2.64715409e-03 2.63390752e-03 2.62619186e-03 2.60743813e-03\n",
            " 2.60519838e-03 2.59175694e-03 2.56118358e-03 2.55414330e-03\n",
            " 2.54723129e-03 2.54578168e-03 2.52934291e-03 2.51603931e-03\n",
            " 2.50738327e-03 2.50429450e-03 2.50045644e-03 2.47546209e-03\n",
            " 2.46746569e-03 2.46488205e-03 2.44731502e-03 2.44145235e-03\n",
            " 2.43058350e-03 2.42022572e-03 2.41035103e-03 2.39725373e-03\n",
            " 2.39252633e-03 2.38190176e-03 2.36587749e-03 2.34440485e-03\n",
            " 2.34023203e-03 2.33760171e-03 2.32910694e-03 2.32238962e-03\n",
            " 2.29139059e-03 2.28543564e-03 2.26337523e-03 2.25356768e-03\n",
            " 2.25327559e-03 2.25169973e-03 2.23857281e-03 2.22395525e-03\n",
            " 2.19780289e-03 2.18803354e-03 2.15277822e-03 2.14782221e-03\n",
            " 2.14179727e-03 2.12657255e-03 2.11036787e-03 2.10936478e-03\n",
            " 2.10767368e-03 2.06737217e-03 2.05573846e-03 2.05438169e-03\n",
            " 2.04497667e-03 2.03776383e-03 2.03553217e-03 2.01964290e-03\n",
            " 2.01910244e-03 2.01761001e-03 2.00926904e-03 1.99095952e-03\n",
            " 1.98738214e-03 1.98462750e-03 1.98089492e-03 1.97458048e-03\n",
            " 1.96982577e-03 1.95836260e-03 1.95209400e-03 1.94558284e-03\n",
            " 1.94410880e-03 1.94051072e-03 1.93032891e-03 1.91351224e-03\n",
            " 1.90565806e-03 1.89844426e-03 1.88570073e-03 1.88350555e-03\n",
            " 1.87765055e-03 1.87465377e-03 1.85984862e-03 1.85353565e-03\n",
            " 1.83191018e-03 1.82485414e-03 1.82282591e-03 1.82241281e-03\n",
            " 1.81412984e-03 1.80960732e-03 1.79768657e-03 1.78820880e-03\n",
            " 1.78465000e-03 1.77880829e-03 1.77739544e-03 1.76809341e-03\n",
            " 1.76196206e-03 1.73955306e-03 1.73053798e-03 1.72653460e-03\n",
            " 1.70819071e-03 1.69580956e-03 1.68884436e-03 1.68002736e-03\n",
            " 1.67677714e-03 1.66992343e-03 1.66515592e-03 1.65461365e-03\n",
            " 1.65043961e-03 1.64383350e-03 1.63529939e-03 1.62968345e-03\n",
            " 1.62841010e-03 1.62456103e-03 1.62440338e-03 1.62359436e-03\n",
            " 1.61952595e-03 1.61755760e-03 1.61361898e-03 1.60993440e-03\n",
            " 1.60948400e-03 1.60580486e-03 1.60471226e-03 1.60090088e-03\n",
            " 1.59504544e-03 1.59128232e-03 1.58761059e-03 1.58378911e-03\n",
            " 1.58232917e-03 1.57039743e-03 1.55905913e-03 1.55832510e-03\n",
            " 1.55615808e-03 1.55167968e-03 1.54413929e-03 1.54124948e-03\n",
            " 1.53960727e-03 1.53919243e-03 1.52727803e-03 1.52664648e-03\n",
            " 1.52378842e-03 1.52141089e-03 1.51754710e-03 1.51724373e-03\n",
            " 1.51630705e-03 1.51621028e-03 1.51612072e-03 1.51526586e-03\n",
            " 1.51167204e-03 1.50337510e-03 1.50176711e-03 1.50162408e-03\n",
            " 1.49080819e-03 1.48945650e-03 1.48914369e-03 1.48429217e-03\n",
            " 1.48343189e-03 1.48035297e-03 1.48029198e-03 1.48009256e-03\n",
            " 1.47779988e-03 1.47420173e-03 1.47067704e-03 1.46573177e-03\n",
            " 1.46169082e-03 1.45125975e-03 1.44653662e-03 1.43584284e-03\n",
            " 1.42851071e-03 1.42687547e-03 1.42520361e-03 1.42003780e-03\n",
            " 1.41638355e-03 1.41543565e-03 1.40930415e-03 1.40866743e-03\n",
            " 1.40855196e-03 1.40621410e-03 1.39793410e-03 1.39575506e-03\n",
            " 1.39060941e-03 1.39053242e-03 1.38856269e-03 1.38753185e-03\n",
            " 1.38551957e-03 1.38302018e-03 1.38291345e-03 1.38022491e-03\n",
            " 1.37196054e-03 1.36098860e-03 1.35802709e-03 1.35424855e-03\n",
            " 1.35224549e-03 1.35043740e-03 1.33683657e-03 1.33350884e-03\n",
            " 1.32662073e-03 1.32610863e-03 1.32508878e-03 1.32229056e-03\n",
            " 1.31993983e-03 1.31731164e-03 1.31628246e-03 1.30798757e-03\n",
            " 1.30790663e-03 1.30492408e-03 1.30273284e-03 1.30081864e-03\n",
            " 1.29397459e-03 1.28882020e-03 1.28282697e-03 1.27997237e-03\n",
            " 1.27941669e-03 1.27869496e-03 1.27781425e-03 1.27613895e-03\n",
            " 1.26378558e-03 1.26275777e-03 1.25719268e-03 1.25634138e-03\n",
            " 1.24918167e-03 1.24513341e-03 1.24271171e-03 1.24159220e-03\n",
            " 1.24024136e-03 1.23972523e-03 1.23906980e-03 1.23281803e-03\n",
            " 1.23124884e-03 1.22878866e-03 1.22721252e-03 1.22130445e-03\n",
            " 1.21998007e-03 1.21909963e-03 1.21632350e-03 1.21272624e-03\n",
            " 1.21259405e-03 1.21091766e-03 1.20585547e-03 1.20505958e-03\n",
            " 1.20164044e-03 1.19554691e-03 1.18900902e-03 1.18611296e-03\n",
            " 1.18546122e-03 1.18145367e-03 1.18005739e-03 1.17776331e-03\n",
            " 1.17543936e-03 1.17202025e-03 1.17121095e-03 1.17017155e-03\n",
            " 1.16667680e-03 1.16229414e-03 1.16221848e-03 1.16203543e-03\n",
            " 1.16005160e-03 1.15990544e-03 1.15041919e-03 1.15010219e-03\n",
            " 1.14268791e-03 1.14191478e-03 1.14089464e-03 1.13945461e-03\n",
            " 1.13741368e-03 1.13519010e-03 1.13336015e-03 1.12915198e-03\n",
            " 1.12543872e-03 1.12347775e-03 1.11326067e-03 1.11078317e-03\n",
            " 1.10840226e-03 1.10536775e-03 1.08915953e-03 1.08703041e-03\n",
            " 1.08584937e-03 1.08469170e-03 1.08235514e-03 1.08203942e-03\n",
            " 1.08035945e-03 1.07980436e-03 1.07679891e-03 1.07105389e-03\n",
            " 1.06825553e-03 1.06158641e-03 1.06042659e-03 1.04594989e-03\n",
            " 1.04354475e-03 1.04229535e-03 1.04072479e-03 1.04031730e-03\n",
            " 1.03516503e-03 1.00713463e-03 1.00491479e-03 1.00454245e-03\n",
            " 1.00369700e-03 9.98998109e-04 9.98972929e-04 9.98088535e-04\n",
            " 9.91493138e-04 9.89418228e-04 9.87825658e-04 9.86070279e-04\n",
            " 9.85540376e-04 9.84562115e-04 9.82229770e-04 9.78755896e-04\n",
            " 9.75516071e-04 9.73915280e-04 9.72366913e-04 9.70411980e-04\n",
            " 9.69722329e-04 9.65798444e-04 9.65095404e-04 9.62387540e-04\n",
            " 9.61817720e-04 9.61747344e-04 9.58188079e-04 9.54243296e-04\n",
            " 9.49516196e-04 9.45663048e-04 9.42015994e-04 9.40070548e-04\n",
            " 9.39686960e-04 9.32111668e-04 9.30284567e-04 9.27522830e-04\n",
            " 9.23000223e-04 9.21404603e-04 9.16346086e-04 9.15818263e-04\n",
            " 9.09156361e-04 9.05031406e-04 9.02168508e-04 8.98270909e-04\n",
            " 8.95694535e-04 8.95049269e-04 8.94413841e-04 8.92160227e-04\n",
            " 8.91987245e-04 8.83759139e-04 8.80749719e-04 8.80025200e-04\n",
            " 8.72556472e-04 8.69356514e-04 8.65197422e-04 8.61727927e-04\n",
            " 8.59479367e-04 8.58446558e-04 8.55590203e-04 8.54366466e-04\n",
            " 8.52524982e-04 8.48126377e-04 8.47790207e-04 8.47381860e-04\n",
            " 8.46553391e-04 8.44870715e-04 8.43652063e-04 8.42210201e-04\n",
            " 8.36679866e-04 8.31176513e-04 8.29715168e-04 8.29023888e-04\n",
            " 8.12252521e-04 8.09535224e-04 8.09031024e-04 8.08687666e-04\n",
            " 8.03755310e-04 8.02084661e-04 8.01569084e-04 8.00336303e-04\n",
            " 8.00271764e-04 7.99320753e-04 7.98919980e-04 7.95036512e-04\n",
            " 7.94482171e-04 7.94461623e-04 7.89051681e-04 7.85310571e-04\n",
            " 7.80232801e-04 7.78774050e-04 7.74739708e-04 7.74721058e-04\n",
            " 7.73206856e-04 7.72437249e-04 7.66748051e-04 7.64284400e-04\n",
            " 7.61794781e-04 7.59004930e-04 7.58519442e-04 7.54084941e-04\n",
            " 7.47979508e-04 7.47602804e-04 7.44518271e-04 7.44370214e-04\n",
            " 7.42205244e-04 7.40687526e-04 7.40566700e-04 7.39773488e-04\n",
            " 7.38040316e-04 7.30119595e-04 7.28818958e-04 7.20242437e-04\n",
            " 7.18371216e-04 7.16775101e-04 7.15969720e-04 7.14350543e-04\n",
            " 7.13826386e-04 7.13359254e-04 7.12899895e-04 7.11160447e-04\n",
            " 7.11056721e-04 7.10116207e-04 7.06942606e-04 7.04905909e-04\n",
            " 7.04605881e-04 7.00479604e-04 6.98212719e-04 6.92884468e-04\n",
            " 6.91339826e-04 6.91050380e-04 6.90986402e-04 6.90382567e-04\n",
            " 6.89303752e-04 6.88284366e-04 6.87717249e-04 6.86402046e-04\n",
            " 6.80437898e-04 6.80115339e-04 6.79815683e-04 6.74445087e-04\n",
            " 6.71159180e-04 6.69174276e-04 6.67766023e-04 6.66199176e-04\n",
            " 6.62946497e-04 6.62537306e-04 6.55486821e-04 6.54576486e-04\n",
            " 6.51766603e-04 6.51057512e-04 6.48141470e-04 6.47553022e-04\n",
            " 6.47546451e-04 6.46144534e-04 6.43088257e-04 6.39745683e-04\n",
            " 6.38572916e-04 6.37212473e-04 6.35487932e-04 6.35218646e-04\n",
            " 6.34908645e-04 6.33227898e-04 6.33171831e-04 6.33026220e-04\n",
            " 6.33023790e-04 6.32826024e-04 6.29145657e-04 6.28656368e-04\n",
            " 6.25767662e-04 6.25560141e-04 6.25128409e-04 6.23417267e-04\n",
            " 6.22174453e-04 6.19904089e-04 6.17576322e-04 6.16607674e-04\n",
            " 6.10507549e-04 6.09401574e-04 6.06644081e-04 6.04453560e-04\n",
            " 6.04407747e-04 6.04252268e-04 6.01068134e-04 5.93001225e-04\n",
            " 5.92469140e-04 5.92387566e-04 5.92171554e-04 5.92162953e-04\n",
            " 5.90269851e-04 5.88694778e-04 5.86769625e-04 5.78357478e-04\n",
            " 5.77983038e-04 5.74638272e-04 5.74003594e-04 5.73111678e-04\n",
            " 5.72210613e-04 5.65203652e-04 5.62271116e-04 5.61769951e-04\n",
            " 5.58978604e-04 5.56210486e-04 5.56082208e-04 5.55156834e-04\n",
            " 5.54351629e-04 5.52730373e-04 5.50086018e-04 5.48502049e-04\n",
            " 5.44608235e-04 5.43467035e-04 5.43344988e-04 5.42693101e-04\n",
            " 5.38443307e-04 5.35742305e-04 5.35274253e-04 5.34451443e-04\n",
            " 5.29752653e-04 5.27469727e-04 5.26325352e-04 5.26070213e-04\n",
            " 5.24757797e-04 5.24102920e-04 5.24042828e-04 5.20430342e-04\n",
            " 5.19989130e-04 5.19038542e-04 5.17441917e-04 5.16749755e-04\n",
            " 5.14226509e-04 5.12419011e-04 5.11196137e-04 5.10559883e-04\n",
            " 5.06719594e-04 5.02865131e-04 5.02459675e-04 5.02028970e-04\n",
            " 5.00654157e-04 4.99242216e-04 4.95082518e-04 4.93542443e-04\n",
            " 4.93316680e-04 4.93246975e-04 4.91563150e-04 4.89779236e-04\n",
            " 4.85466644e-04 4.83436234e-04 4.82131039e-04 4.79500081e-04\n",
            " 4.79289881e-04 4.78787864e-04 4.78661493e-04 4.77464264e-04\n",
            " 4.74273413e-04 4.74071159e-04 4.69140073e-04 4.68606846e-04\n",
            " 4.68429162e-04 4.67372715e-04 4.65573722e-04 4.65503327e-04\n",
            " 4.65402285e-04 4.62385211e-04 4.60063751e-04 4.58002532e-04\n",
            " 4.56968045e-04 4.56366864e-04 4.54577773e-04 4.54218482e-04\n",
            " 4.53504561e-04 4.53500684e-04 4.52762940e-04 4.52629789e-04\n",
            " 4.50889440e-04 4.50177841e-04 4.49655331e-04 4.48616390e-04\n",
            " 4.47464580e-04 4.46860814e-04 4.46850540e-04 4.45808730e-04\n",
            " 4.41662291e-04 4.41662260e-04 4.38759272e-04 4.36932697e-04\n",
            " 4.33117672e-04 4.33044449e-04 4.32375006e-04 4.32243752e-04\n",
            " 4.30860215e-04 4.28932855e-04 4.27623028e-04 4.23409147e-04\n",
            " 4.19186977e-04 4.19033559e-04 4.18098978e-04 4.17764542e-04\n",
            " 4.16051773e-04 4.15069858e-04 4.14723161e-04 4.12694904e-04\n",
            " 4.12316018e-04 4.12170346e-04 4.07371128e-04 4.07329569e-04\n",
            " 4.06867186e-04 4.06607426e-04 4.06377205e-04 4.05252692e-04\n",
            " 4.02817572e-04 3.97733835e-04 3.92773068e-04 3.92075209e-04\n",
            " 3.92046205e-04 3.91729994e-04 3.90700326e-04 3.90037235e-04\n",
            " 3.88739145e-04 3.87011325e-04 3.85753854e-04 3.82803020e-04\n",
            " 3.82380762e-04 3.81805224e-04 3.79951722e-04 3.79751554e-04\n",
            " 3.79075266e-04 3.77251390e-04 3.76845540e-04 3.76829282e-04\n",
            " 3.74815693e-04 3.69926248e-04 3.69652310e-04 3.69404229e-04\n",
            " 3.67395485e-04 3.66378461e-04 3.65903613e-04 3.65551638e-04\n",
            " 3.64402318e-04 3.61313192e-04 3.60686408e-04 3.59604248e-04\n",
            " 3.57697572e-04 3.52361583e-04 3.51923131e-04 3.50996662e-04\n",
            " 3.50137866e-04 3.49856019e-04 3.49750681e-04 3.48106534e-04\n",
            " 3.47806274e-04 3.47640622e-04 3.43832176e-04 3.42814281e-04\n",
            " 3.42675663e-04 3.41447154e-04 3.36927192e-04 3.36862014e-04\n",
            " 3.30858248e-04 3.28984320e-04 3.28976296e-04 3.26935962e-04\n",
            " 3.26650952e-04 3.24320102e-04 3.21694020e-04 3.19048071e-04\n",
            " 3.16343679e-04 3.15293543e-04 3.15155470e-04 3.15145537e-04\n",
            " 3.15010253e-04 3.14632475e-04 3.14594323e-04 3.13177929e-04\n",
            " 3.13106047e-04 3.11985996e-04 3.06440226e-04 3.06141334e-04\n",
            " 3.05418874e-04 3.03496718e-04 3.03368044e-04 3.01995412e-04\n",
            " 3.01688782e-04 2.99764416e-04 2.99691045e-04 2.98421491e-04\n",
            " 2.98172977e-04 2.97136784e-04 2.96732592e-04 2.96644499e-04\n",
            " 2.94551010e-04 2.93660614e-04 2.93151138e-04 2.90777289e-04\n",
            " 2.88826692e-04 2.88071430e-04 2.85785720e-04 2.84954471e-04\n",
            " 2.84633191e-04 2.82393445e-04 2.81834855e-04 2.78117617e-04\n",
            " 2.77249648e-04 2.76488799e-04 2.76051894e-04 2.75915913e-04\n",
            " 2.75789105e-04 2.75582220e-04 2.75090385e-04 2.74470659e-04\n",
            " 2.73234454e-04 2.71056657e-04 2.70717669e-04 2.70022732e-04\n",
            " 2.69611499e-04 2.68820032e-04 2.68403147e-04 2.67152647e-04\n",
            " 2.66628388e-04 2.66414719e-04 2.66263401e-04 2.66041665e-04\n",
            " 2.63172843e-04 2.60573557e-04 2.59790073e-04 2.59610361e-04\n",
            " 2.57507025e-04 2.54944432e-04 2.53673824e-04 2.53573786e-04\n",
            " 2.50943036e-04 2.49673737e-04 2.48491694e-04 2.45242989e-04\n",
            " 2.44921358e-04 2.43100022e-04 2.42994922e-04 2.40599612e-04\n",
            " 2.40514903e-04 2.40481010e-04 2.36774776e-04 2.35702882e-04\n",
            " 2.35303955e-04 2.34192658e-04 2.32567321e-04 2.30732616e-04\n",
            " 2.30562702e-04 2.30525906e-04 2.30264501e-04 2.29549221e-04\n",
            " 2.23344274e-04 2.21985846e-04 2.21724145e-04 2.18067131e-04\n",
            " 2.17744197e-04 2.17429424e-04 2.16805442e-04 2.16267298e-04\n",
            " 2.15981711e-04 2.14641721e-04 2.14564938e-04 2.13217004e-04\n",
            " 2.09676884e-04 2.09478115e-04 2.09009647e-04 2.05127027e-04\n",
            " 2.03282407e-04 2.00218070e-04 2.00024692e-04 1.98171820e-04\n",
            " 1.97119053e-04 1.95976479e-04 1.93337611e-04 1.90665873e-04\n",
            " 1.88848481e-04 1.88175040e-04 1.86755849e-04 1.85504297e-04\n",
            " 1.83648633e-04 1.79538694e-04 1.78286132e-04 1.78273663e-04\n",
            " 1.77753904e-04 1.76915007e-04 1.69085603e-04 1.68109160e-04\n",
            " 1.67918785e-04 1.67670965e-04 1.67062915e-04 1.66370892e-04\n",
            " 1.65549520e-04 1.64113365e-04 1.63465225e-04 1.63144470e-04\n",
            " 1.62809337e-04 1.61218886e-04 1.60660225e-04 1.54998987e-04\n",
            " 1.54947182e-04 1.54478462e-04 1.53437726e-04 1.50668559e-04\n",
            " 1.50164540e-04 1.49962968e-04 1.48848532e-04 1.48644499e-04\n",
            " 1.45838389e-04 1.45019243e-04 1.44648717e-04 1.42478717e-04\n",
            " 1.42012900e-04 1.41876076e-04 1.41657337e-04 1.41371813e-04\n",
            " 1.39353591e-04 1.39084627e-04 1.38681557e-04 1.37927793e-04\n",
            " 1.37856456e-04 1.36421527e-04 1.36311113e-04 1.36229391e-04\n",
            " 1.36175454e-04 1.35819301e-04 1.34680019e-04 1.33408815e-04\n",
            " 1.33201468e-04 1.32358941e-04 1.32289526e-04 1.30613468e-04\n",
            " 1.30463165e-04 1.25787270e-04 1.25469404e-04 1.24581754e-04\n",
            " 1.24478368e-04 1.23415523e-04 1.21186149e-04 1.17149641e-04\n",
            " 1.13957833e-04 1.13602597e-04 1.12824120e-04 1.12393325e-04\n",
            " 1.12045151e-04 1.11856372e-04 1.10437221e-04 1.07892022e-04\n",
            " 1.07729613e-04 1.06202273e-04 1.05127895e-04 1.04557677e-04\n",
            " 1.04135424e-04 1.03380993e-04 1.02648052e-04 1.02540734e-04\n",
            " 1.02307897e-04 9.99692878e-05 9.85114237e-05 9.59857936e-05\n",
            " 9.47751067e-05 9.42565876e-05 9.41771714e-05 9.39701762e-05\n",
            " 9.24716754e-05 9.20617605e-05 9.02849783e-05 9.01000119e-05\n",
            " 8.85577525e-05 8.51816143e-05 8.51314002e-05 8.38926524e-05\n",
            " 8.27130542e-05 8.25047988e-05 8.25039856e-05 8.09584569e-05\n",
            " 8.00247868e-05 7.85632727e-05 7.59905224e-05 7.35079316e-05\n",
            " 7.29843754e-05 7.02564186e-05 6.93205806e-05 6.90006876e-05\n",
            " 6.80162969e-05 6.76634661e-05 6.59162841e-05 6.54036335e-05\n",
            " 6.44028395e-05 6.43702982e-05 6.25500943e-05 6.22277701e-05\n",
            " 6.12073248e-05 6.04635986e-05 6.02864503e-05 5.93832118e-05\n",
            " 5.65674087e-05 5.63375618e-05 5.55526007e-05 5.34243463e-05\n",
            " 5.15055154e-05 4.98602602e-05 4.71697685e-05 4.59066151e-05\n",
            " 4.56801299e-05 4.53678400e-05 4.45647193e-05 4.40424153e-05\n",
            " 4.37214762e-05 4.32551595e-05 4.23719094e-05 4.12770744e-05\n",
            " 4.07966756e-05 3.79893440e-05 3.76856492e-05 3.70033211e-05\n",
            " 3.40253382e-05 3.34473480e-05 3.18279864e-05 3.04098862e-05\n",
            " 2.93477684e-05 2.69993530e-05 2.66076786e-05 2.54749736e-05\n",
            " 2.33392593e-05 2.33099094e-05 2.24564537e-05 2.17519624e-05\n",
            " 2.10569216e-05 2.10487784e-05 1.81411914e-05 1.60661603e-05\n",
            " 1.51754413e-05 1.51718002e-05 1.27847494e-05 1.25877900e-05\n",
            " 1.19846088e-05 1.18550883e-05 9.87262098e-06 7.15098619e-06\n",
            " 5.66884026e-06 5.17555433e-06 4.10718241e-06 2.99834878e-06]\n",
            "(10, 1000)\n"
          ]
        }
      ],
      "source": [
        "topic_vectors = np.random.dirichlet(vocab_size*[beta], n_topics)\n",
        "print('Ordered probabilities for the first topic vector:')\n",
        "print(np.sort(topic_vectors[0])[::-1])\n",
        "print(topic_vectors.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BaQEe-yD6vl_"
      },
      "source": [
        "## 1.4. Generation of document topic proportions and documents for each node\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N-BziCW56vFL",
        "outputId": "b852308c-3586-4346-e21f-6b884f15a9e4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.1, 0.1, 0.1]\n",
            "Ordered probabilities for the first document - node 0 :\n",
            "[6.88093512e-01 1.55049334e-01 1.32865778e-01 1.62185249e-02\n",
            " 5.27018340e-03 1.89921823e-03 6.01371601e-04 1.64877332e-06\n",
            " 3.20370275e-07 1.07712654e-07]\n",
            "Documents of node 0 generated.\n",
            "[0.4, 0.4, 0.4, 0.4, 0.1, 0.1, 0.1, 0.4, 0.4, 0.4]\n",
            "Ordered probabilities for the first document - node 1 :\n",
            "[5.20347037e-01 2.05038567e-01 9.38538270e-02 9.22610957e-02\n",
            " 7.57159714e-02 1.27592388e-02 1.72284307e-05 6.60350584e-06\n",
            " 3.79878786e-07 5.17564305e-08]\n",
            "Documents of node 1 generated.\n",
            "[0.4, 0.1, 0.1, 0.1, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4]\n",
            "Ordered probabilities for the first document - node 2 :\n",
            "[3.32058559e-01 3.01604401e-01 2.15284878e-01 6.66583376e-02\n",
            " 3.57576345e-02 3.50658961e-02 9.66453040e-03 2.21992216e-03\n",
            " 1.67616446e-03 9.67668291e-06]\n",
            "Documents of node 2 generated.\n",
            "[0.1, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.1, 0.1]\n",
            "Ordered probabilities for the first document - node 3 :\n",
            "[4.74085598e-01 1.91340519e-01 1.64519561e-01 8.04855182e-02\n",
            " 4.56002867e-02 2.12213144e-02 9.02125257e-03 8.09464068e-03\n",
            " 5.62918332e-03 2.12652798e-06]\n",
            "Documents of node 3 generated.\n",
            "[0.4, 0.4, 0.4, 0.4, 0.4, 0.1, 0.1, 0.1, 0.4, 0.4]\n",
            "Ordered probabilities for the first document - node 4 :\n",
            "[2.74900726e-01 2.43454257e-01 2.35014543e-01 1.07741425e-01\n",
            " 7.10689144e-02 2.85157078e-02 1.69894413e-02 1.44621593e-02\n",
            " 7.84503882e-03 7.78637540e-06]\n",
            "Documents of node 4 generated.\n"
          ]
        }
      ],
      "source": [
        "doc_topics_all_gt = []\n",
        "documents_all = []\n",
        "z_all = []\n",
        "for i in np.arange(n_nodes):\n",
        "  # Step 2 - generation of document topic proportions for each node\n",
        "  if dirichlet_symmetric:\n",
        "    doc_topics = np.random.dirichlet((n_topics)*[alpha], n_docs)\n",
        "  else:\n",
        "    doc_topics = np.random.dirichlet(prior, n_docs)\n",
        "    prior = rotateArray(prior, len(prior), 3)\n",
        "    print(prior)\n",
        "  print('Ordered probabilities for the first document - node', str(i), ':')\n",
        "  print(np.sort(doc_topics[0])[::-1])\n",
        "  doc_topics_all_gt.append(doc_topics)\n",
        "  # Step 3 - Document generation\n",
        "  documents = [] # Document words\n",
        "  z = [] # Assignments\n",
        "  for docid in np.arange(n_docs):\n",
        "      doc_len = np.random.randint(low=nwords[0], high=nwords[1])\n",
        "      this_doc_words = []\n",
        "      this_doc_assigns = []\n",
        "      for wd_idx in np.arange(doc_len):\n",
        "          tpc = np.nonzero(np.random.multinomial(1, doc_topics[docid]))[0][0]\n",
        "          this_doc_assigns.append(tpc)\n",
        "          word = np.nonzero(np.random.multinomial(1, topic_vectors[tpc]))[0][0]\n",
        "          this_doc_words.append('wd'+str(word))\n",
        "      z.append(this_doc_assigns)\n",
        "      documents.append(this_doc_words)\n",
        "  print(\"Documents of node\", str(i), \"generated.\")\n",
        "  documents_all.append(documents)\n",
        "  z_all.append(z)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HuJUlOIJ69iw"
      },
      "source": [
        "# 2. Preprocessing, generation of training dataset and training of a ProdLDA model at each node"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1XwiP814FZ5U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e37d3cf5-5bc6-40ea-df80-bf210a71a5ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mPyTorchAVITM\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
          ]
        }
      ],
      "source": [
        "ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "14tnh0ndFpb0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "978772ad-8118-4151-8277-f851aeec49c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/PyTorchAVITM/pytorchavitm/datasets\n"
          ]
        }
      ],
      "source": [
        "cd /content/PyTorchAVITM/pytorchavitm/datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Is7SF6iQcqA"
      },
      "outputs": [],
      "source": [
        "from bow import BOWDataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k4okSYycQaOK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "490d697d-c783-47e8-dbcf-5e9bbf1805ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/PyTorchAVITM\n"
          ]
        }
      ],
      "source": [
        "cd /content/PyTorchAVITM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JrcVglDoQhU0"
      },
      "outputs": [],
      "source": [
        "from pytorchavitm import AVITM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wchhyQ5bDIhK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc2e6312-4758-4a78-b061-914568dd55e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Settings: \n",
            "               N Components: 10\n",
            "               Topic Prior Mean: 0.0\n",
            "               Topic Prior Variance: 0.9\n",
            "               Model Type: prodLDA\n",
            "               Hidden Sizes: (100, 100)\n",
            "               Activation: softplus\n",
            "               Dropout: 0.2\n",
            "               Learn Priors: True\n",
            "               Learning Rate: 0.002\n",
            "               Momentum: 0.99\n",
            "               Reduce On Plateau: False\n",
            "               Save Dir: None\n",
            "Epoch: [1/100]\tSamples: [1000/100000]\tTrain Loss: 2182.83503125\tTime: 0:00:00.422205\n",
            "Epoch: [2/100]\tSamples: [2000/100000]\tTrain Loss: 2163.7754453125\tTime: 0:00:00.210161\n",
            "Epoch: [3/100]\tSamples: [3000/100000]\tTrain Loss: 2153.888640625\tTime: 0:00:00.211286\n",
            "Epoch: [4/100]\tSamples: [4000/100000]\tTrain Loss: 2148.2529453125\tTime: 0:00:00.188458\n",
            "Epoch: [5/100]\tSamples: [5000/100000]\tTrain Loss: 2140.661875\tTime: 0:00:00.192488\n",
            "Epoch: [6/100]\tSamples: [6000/100000]\tTrain Loss: 2137.066484375\tTime: 0:00:00.210398\n",
            "Epoch: [7/100]\tSamples: [7000/100000]\tTrain Loss: 2129.3856171875\tTime: 0:00:00.199733\n",
            "Epoch: [8/100]\tSamples: [8000/100000]\tTrain Loss: 2124.425546875\tTime: 0:00:00.190679\n",
            "Epoch: [9/100]\tSamples: [9000/100000]\tTrain Loss: 2119.6851328125\tTime: 0:00:00.200710\n",
            "Epoch: [10/100]\tSamples: [10000/100000]\tTrain Loss: 2115.39459375\tTime: 0:00:00.190733\n",
            "Epoch: [11/100]\tSamples: [11000/100000]\tTrain Loss: 2109.077921875\tTime: 0:00:00.185009\n",
            "Epoch: [12/100]\tSamples: [12000/100000]\tTrain Loss: 2106.4682109375\tTime: 0:00:00.199012\n",
            "Epoch: [13/100]\tSamples: [13000/100000]\tTrain Loss: 2102.508203125\tTime: 0:00:00.215607\n",
            "Epoch: [14/100]\tSamples: [14000/100000]\tTrain Loss: 2098.6373046875\tTime: 0:00:00.195760\n",
            "Epoch: [15/100]\tSamples: [15000/100000]\tTrain Loss: 2095.8056953125\tTime: 0:00:00.211205\n",
            "Epoch: [16/100]\tSamples: [16000/100000]\tTrain Loss: 2090.818125\tTime: 0:00:00.192397\n",
            "Epoch: [17/100]\tSamples: [17000/100000]\tTrain Loss: 2087.2281328125\tTime: 0:00:00.200987\n",
            "Epoch: [18/100]\tSamples: [18000/100000]\tTrain Loss: 2083.6326953125\tTime: 0:00:00.224163\n",
            "Epoch: [19/100]\tSamples: [19000/100000]\tTrain Loss: 2081.155875\tTime: 0:00:00.198596\n",
            "Epoch: [20/100]\tSamples: [20000/100000]\tTrain Loss: 2079.7742578125\tTime: 0:00:00.199834\n",
            "Epoch: [21/100]\tSamples: [21000/100000]\tTrain Loss: 2074.8870546875\tTime: 0:00:00.202978\n",
            "Epoch: [22/100]\tSamples: [22000/100000]\tTrain Loss: 2072.763484375\tTime: 0:00:00.192805\n",
            "Epoch: [23/100]\tSamples: [23000/100000]\tTrain Loss: 2069.05875\tTime: 0:00:00.212445\n",
            "Epoch: [24/100]\tSamples: [24000/100000]\tTrain Loss: 2067.4578515625\tTime: 0:00:00.229450\n",
            "Epoch: [25/100]\tSamples: [25000/100000]\tTrain Loss: 2065.0354609375\tTime: 0:00:00.202577\n",
            "Epoch: [26/100]\tSamples: [26000/100000]\tTrain Loss: 2063.875046875\tTime: 0:00:00.201673\n",
            "Epoch: [27/100]\tSamples: [27000/100000]\tTrain Loss: 2061.8401015625\tTime: 0:00:00.207197\n",
            "Epoch: [28/100]\tSamples: [28000/100000]\tTrain Loss: 2060.1909921875\tTime: 0:00:00.222979\n",
            "Epoch: [29/100]\tSamples: [29000/100000]\tTrain Loss: 2058.421625\tTime: 0:00:00.195482\n",
            "Epoch: [30/100]\tSamples: [30000/100000]\tTrain Loss: 2056.05315625\tTime: 0:00:00.205167\n",
            "Epoch: [31/100]\tSamples: [31000/100000]\tTrain Loss: 2055.9485390625\tTime: 0:00:00.192301\n",
            "Epoch: [32/100]\tSamples: [32000/100000]\tTrain Loss: 2054.8281484375\tTime: 0:00:00.197247\n",
            "Epoch: [33/100]\tSamples: [33000/100000]\tTrain Loss: 2054.143640625\tTime: 0:00:00.220784\n",
            "Epoch: [34/100]\tSamples: [34000/100000]\tTrain Loss: 2053.121640625\tTime: 0:00:00.210950\n",
            "Epoch: [35/100]\tSamples: [35000/100000]\tTrain Loss: 2051.5959375\tTime: 0:00:00.200406\n",
            "Epoch: [36/100]\tSamples: [36000/100000]\tTrain Loss: 2051.6303046875\tTime: 0:00:00.184456\n",
            "Epoch: [37/100]\tSamples: [37000/100000]\tTrain Loss: 2050.462484375\tTime: 0:00:00.190222\n",
            "Epoch: [38/100]\tSamples: [38000/100000]\tTrain Loss: 2050.0181796875\tTime: 0:00:00.201637\n",
            "Epoch: [39/100]\tSamples: [39000/100000]\tTrain Loss: 2049.2004921875\tTime: 0:00:00.197530\n",
            "Epoch: [40/100]\tSamples: [40000/100000]\tTrain Loss: 2048.764234375\tTime: 0:00:00.202826\n",
            "Epoch: [41/100]\tSamples: [41000/100000]\tTrain Loss: 2049.0859375\tTime: 0:00:00.193130\n",
            "Epoch: [42/100]\tSamples: [42000/100000]\tTrain Loss: 2047.857171875\tTime: 0:00:00.192333\n",
            "Epoch: [43/100]\tSamples: [43000/100000]\tTrain Loss: 2048.2050859375\tTime: 0:00:00.220975\n",
            "Epoch: [44/100]\tSamples: [44000/100000]\tTrain Loss: 2047.7420859375\tTime: 0:00:00.198347\n",
            "Epoch: [45/100]\tSamples: [45000/100000]\tTrain Loss: 2046.8856953125\tTime: 0:00:00.200499\n",
            "Epoch: [46/100]\tSamples: [46000/100000]\tTrain Loss: 2046.883359375\tTime: 0:00:00.190723\n",
            "Epoch: [47/100]\tSamples: [47000/100000]\tTrain Loss: 2046.931671875\tTime: 0:00:00.196058\n",
            "Epoch: [48/100]\tSamples: [48000/100000]\tTrain Loss: 2046.4108203125\tTime: 0:00:00.207614\n",
            "Epoch: [49/100]\tSamples: [49000/100000]\tTrain Loss: 2045.566859375\tTime: 0:00:00.188861\n",
            "Epoch: [50/100]\tSamples: [50000/100000]\tTrain Loss: 2045.8020703125\tTime: 0:00:00.186895\n",
            "Epoch: [51/100]\tSamples: [51000/100000]\tTrain Loss: 2045.798703125\tTime: 0:00:00.182936\n",
            "Epoch: [52/100]\tSamples: [52000/100000]\tTrain Loss: 2045.222921875\tTime: 0:00:00.187981\n",
            "Epoch: [53/100]\tSamples: [53000/100000]\tTrain Loss: 2045.1062421875\tTime: 0:00:00.203837\n",
            "Epoch: [54/100]\tSamples: [54000/100000]\tTrain Loss: 2044.7120703125\tTime: 0:00:00.195468\n",
            "Epoch: [55/100]\tSamples: [55000/100000]\tTrain Loss: 2045.0013671875\tTime: 0:00:00.196218\n",
            "Epoch: [56/100]\tSamples: [56000/100000]\tTrain Loss: 2044.9375078125\tTime: 0:00:00.195472\n",
            "Epoch: [57/100]\tSamples: [57000/100000]\tTrain Loss: 2044.659828125\tTime: 0:00:00.202341\n",
            "Epoch: [58/100]\tSamples: [58000/100000]\tTrain Loss: 2044.6100078125\tTime: 0:00:00.199411\n",
            "Epoch: [59/100]\tSamples: [59000/100000]\tTrain Loss: 2044.351375\tTime: 0:00:00.208472\n",
            "Epoch: [60/100]\tSamples: [60000/100000]\tTrain Loss: 2043.9898046875\tTime: 0:00:00.197705\n",
            "Epoch: [61/100]\tSamples: [61000/100000]\tTrain Loss: 2043.9746015625\tTime: 0:00:00.185013\n",
            "Epoch: [62/100]\tSamples: [62000/100000]\tTrain Loss: 2043.6463046875\tTime: 0:00:00.196136\n",
            "Epoch: [63/100]\tSamples: [63000/100000]\tTrain Loss: 2043.6100390625\tTime: 0:00:00.189758\n",
            "Epoch: [64/100]\tSamples: [64000/100000]\tTrain Loss: 2043.475078125\tTime: 0:00:00.212825\n",
            "Epoch: [65/100]\tSamples: [65000/100000]\tTrain Loss: 2043.30946875\tTime: 0:00:00.199888\n",
            "Epoch: [66/100]\tSamples: [66000/100000]\tTrain Loss: 2043.900171875\tTime: 0:00:00.195275\n",
            "Epoch: [67/100]\tSamples: [67000/100000]\tTrain Loss: 2044.075234375\tTime: 0:00:00.193055\n",
            "Epoch: [68/100]\tSamples: [68000/100000]\tTrain Loss: 2044.462390625\tTime: 0:00:00.208989\n",
            "Epoch: [69/100]\tSamples: [69000/100000]\tTrain Loss: 2043.7643828125\tTime: 0:00:00.200870\n",
            "Epoch: [70/100]\tSamples: [70000/100000]\tTrain Loss: 2044.3499609375\tTime: 0:00:00.202376\n",
            "Epoch: [71/100]\tSamples: [71000/100000]\tTrain Loss: 2043.2243125\tTime: 0:00:00.198831\n",
            "Epoch: [72/100]\tSamples: [72000/100000]\tTrain Loss: 2043.6752109375\tTime: 0:00:00.211088\n",
            "Epoch: [73/100]\tSamples: [73000/100000]\tTrain Loss: 2043.95175\tTime: 0:00:00.211005\n",
            "Epoch: [74/100]\tSamples: [74000/100000]\tTrain Loss: 2042.8300390625\tTime: 0:00:00.207102\n",
            "Epoch: [75/100]\tSamples: [75000/100000]\tTrain Loss: 2043.4314140625\tTime: 0:00:00.206604\n",
            "Epoch: [76/100]\tSamples: [76000/100000]\tTrain Loss: 2043.6392265625\tTime: 0:00:00.205119\n",
            "Epoch: [77/100]\tSamples: [77000/100000]\tTrain Loss: 2043.323484375\tTime: 0:00:00.213200\n",
            "Epoch: [78/100]\tSamples: [78000/100000]\tTrain Loss: 2043.5670390625\tTime: 0:00:00.191689\n",
            "Epoch: [79/100]\tSamples: [79000/100000]\tTrain Loss: 2042.5139921875\tTime: 0:00:00.219046\n",
            "Epoch: [80/100]\tSamples: [80000/100000]\tTrain Loss: 2042.81890625\tTime: 0:00:00.195498\n",
            "Epoch: [81/100]\tSamples: [81000/100000]\tTrain Loss: 2043.062515625\tTime: 0:00:00.190267\n",
            "Epoch: [82/100]\tSamples: [82000/100000]\tTrain Loss: 2043.622671875\tTime: 0:00:00.201473\n",
            "Epoch: [83/100]\tSamples: [83000/100000]\tTrain Loss: 2043.2307734375\tTime: 0:00:00.201265\n",
            "Epoch: [84/100]\tSamples: [84000/100000]\tTrain Loss: 2042.6380078125\tTime: 0:00:00.210153\n",
            "Epoch: [85/100]\tSamples: [85000/100000]\tTrain Loss: 2042.89640625\tTime: 0:00:00.198832\n",
            "Epoch: [86/100]\tSamples: [86000/100000]\tTrain Loss: 2043.4040234375\tTime: 0:00:00.208516\n",
            "Epoch: [87/100]\tSamples: [87000/100000]\tTrain Loss: 2042.945234375\tTime: 0:00:00.201074\n",
            "Epoch: [88/100]\tSamples: [88000/100000]\tTrain Loss: 2043.8246875\tTime: 0:00:00.201425\n",
            "Epoch: [89/100]\tSamples: [89000/100000]\tTrain Loss: 2043.3863828125\tTime: 0:00:00.213526\n",
            "Epoch: [90/100]\tSamples: [90000/100000]\tTrain Loss: 2042.3383671875\tTime: 0:00:00.205385\n",
            "Epoch: [91/100]\tSamples: [91000/100000]\tTrain Loss: 2043.280390625\tTime: 0:00:00.191792\n",
            "Epoch: [92/100]\tSamples: [92000/100000]\tTrain Loss: 2042.847359375\tTime: 0:00:00.210913\n",
            "Epoch: [93/100]\tSamples: [93000/100000]\tTrain Loss: 2043.204875\tTime: 0:00:00.211985\n",
            "Epoch: [94/100]\tSamples: [94000/100000]\tTrain Loss: 2042.3673515625\tTime: 0:00:00.219208\n",
            "Epoch: [95/100]\tSamples: [95000/100000]\tTrain Loss: 2043.2477734375\tTime: 0:00:00.192694\n",
            "Epoch: [96/100]\tSamples: [96000/100000]\tTrain Loss: 2042.367375\tTime: 0:00:00.199656\n",
            "Epoch: [97/100]\tSamples: [97000/100000]\tTrain Loss: 2043.1389140625\tTime: 0:00:00.210846\n",
            "Epoch: [98/100]\tSamples: [98000/100000]\tTrain Loss: 2043.1034609375\tTime: 0:00:00.215809\n",
            "Epoch: [99/100]\tSamples: [99000/100000]\tTrain Loss: 2042.86475\tTime: 0:00:00.220394\n",
            "Epoch: [100/100]\tSamples: [100000/100000]\tTrain Loss: 2042.97315625\tTime: 0:00:00.195818\n",
            "Settings: \n",
            "               N Components: 10\n",
            "               Topic Prior Mean: 0.0\n",
            "               Topic Prior Variance: 0.9\n",
            "               Model Type: prodLDA\n",
            "               Hidden Sizes: (100, 100)\n",
            "               Activation: softplus\n",
            "               Dropout: 0.2\n",
            "               Learn Priors: True\n",
            "               Learning Rate: 0.002\n",
            "               Momentum: 0.99\n",
            "               Reduce On Plateau: False\n",
            "               Save Dir: None\n",
            "Epoch: [1/100]\tSamples: [1000/100000]\tTrain Loss: 2181.011359375\tTime: 0:00:00.198559\n",
            "Epoch: [2/100]\tSamples: [2000/100000]\tTrain Loss: 2165.931953125\tTime: 0:00:00.196442\n",
            "Epoch: [3/100]\tSamples: [3000/100000]\tTrain Loss: 2156.5287265625\tTime: 0:00:00.210097\n",
            "Epoch: [4/100]\tSamples: [4000/100000]\tTrain Loss: 2150.3218984375\tTime: 0:00:00.196141\n",
            "Epoch: [5/100]\tSamples: [5000/100000]\tTrain Loss: 2143.9808203125\tTime: 0:00:00.192940\n",
            "Epoch: [6/100]\tSamples: [6000/100000]\tTrain Loss: 2137.1393203125\tTime: 0:00:00.193790\n",
            "Epoch: [7/100]\tSamples: [7000/100000]\tTrain Loss: 2130.6037890625\tTime: 0:00:00.191571\n",
            "Epoch: [8/100]\tSamples: [8000/100000]\tTrain Loss: 2127.152140625\tTime: 0:00:00.199253\n",
            "Epoch: [9/100]\tSamples: [9000/100000]\tTrain Loss: 2122.4896796875\tTime: 0:00:00.188084\n",
            "Epoch: [10/100]\tSamples: [10000/100000]\tTrain Loss: 2116.2216484375\tTime: 0:00:00.201345\n",
            "Epoch: [11/100]\tSamples: [11000/100000]\tTrain Loss: 2113.171\tTime: 0:00:00.200090\n",
            "Epoch: [12/100]\tSamples: [12000/100000]\tTrain Loss: 2108.684640625\tTime: 0:00:00.217904\n",
            "Epoch: [13/100]\tSamples: [13000/100000]\tTrain Loss: 2102.6812890625\tTime: 0:00:00.227959\n",
            "Epoch: [14/100]\tSamples: [14000/100000]\tTrain Loss: 2098.5436953125\tTime: 0:00:00.206356\n",
            "Epoch: [15/100]\tSamples: [15000/100000]\tTrain Loss: 2095.459984375\tTime: 0:00:00.193952\n",
            "Epoch: [16/100]\tSamples: [16000/100000]\tTrain Loss: 2092.3034453125\tTime: 0:00:00.207069\n",
            "Epoch: [17/100]\tSamples: [17000/100000]\tTrain Loss: 2087.5322734375\tTime: 0:00:00.207443\n",
            "Epoch: [18/100]\tSamples: [18000/100000]\tTrain Loss: 2082.6711015625\tTime: 0:00:00.211513\n",
            "Epoch: [19/100]\tSamples: [19000/100000]\tTrain Loss: 2081.0680546875\tTime: 0:00:00.183884\n",
            "Epoch: [20/100]\tSamples: [20000/100000]\tTrain Loss: 2078.225234375\tTime: 0:00:00.195854\n",
            "Epoch: [21/100]\tSamples: [21000/100000]\tTrain Loss: 2073.5054765625\tTime: 0:00:00.202290\n",
            "Epoch: [22/100]\tSamples: [22000/100000]\tTrain Loss: 2073.19834375\tTime: 0:00:00.207217\n",
            "Epoch: [23/100]\tSamples: [23000/100000]\tTrain Loss: 2069.665796875\tTime: 0:00:00.201436\n",
            "Epoch: [24/100]\tSamples: [24000/100000]\tTrain Loss: 2066.9468359375\tTime: 0:00:00.189602\n",
            "Epoch: [25/100]\tSamples: [25000/100000]\tTrain Loss: 2064.50915625\tTime: 0:00:00.187226\n",
            "Epoch: [26/100]\tSamples: [26000/100000]\tTrain Loss: 2062.8799140625\tTime: 0:00:00.195002\n",
            "Epoch: [27/100]\tSamples: [27000/100000]\tTrain Loss: 2061.367671875\tTime: 0:00:00.187093\n",
            "Epoch: [28/100]\tSamples: [28000/100000]\tTrain Loss: 2060.1704140625\tTime: 0:00:00.197684\n",
            "Epoch: [29/100]\tSamples: [29000/100000]\tTrain Loss: 2058.317765625\tTime: 0:00:00.195923\n",
            "Epoch: [30/100]\tSamples: [30000/100000]\tTrain Loss: 2057.1211953125\tTime: 0:00:00.191379\n",
            "Epoch: [31/100]\tSamples: [31000/100000]\tTrain Loss: 2056.3291796875\tTime: 0:00:00.223157\n",
            "Epoch: [32/100]\tSamples: [32000/100000]\tTrain Loss: 2054.995921875\tTime: 0:00:00.197717\n",
            "Epoch: [33/100]\tSamples: [33000/100000]\tTrain Loss: 2054.5071875\tTime: 0:00:00.206167\n",
            "Epoch: [34/100]\tSamples: [34000/100000]\tTrain Loss: 2053.0963828125\tTime: 0:00:00.197582\n",
            "Epoch: [35/100]\tSamples: [35000/100000]\tTrain Loss: 2053.163921875\tTime: 0:00:00.204448\n",
            "Epoch: [36/100]\tSamples: [36000/100000]\tTrain Loss: 2052.3979296875\tTime: 0:00:00.191890\n",
            "Epoch: [37/100]\tSamples: [37000/100000]\tTrain Loss: 2051.3354375\tTime: 0:00:00.195035\n",
            "Epoch: [38/100]\tSamples: [38000/100000]\tTrain Loss: 2051.263875\tTime: 0:00:00.191712\n",
            "Epoch: [39/100]\tSamples: [39000/100000]\tTrain Loss: 2049.9036015625\tTime: 0:00:00.211075\n",
            "Epoch: [40/100]\tSamples: [40000/100000]\tTrain Loss: 2049.396390625\tTime: 0:00:00.193286\n",
            "Epoch: [41/100]\tSamples: [41000/100000]\tTrain Loss: 2049.0656875\tTime: 0:00:00.194889\n",
            "Epoch: [42/100]\tSamples: [42000/100000]\tTrain Loss: 2047.8881484375\tTime: 0:00:00.186501\n",
            "Epoch: [43/100]\tSamples: [43000/100000]\tTrain Loss: 2047.9748046875\tTime: 0:00:00.194793\n",
            "Epoch: [44/100]\tSamples: [44000/100000]\tTrain Loss: 2047.3410390625\tTime: 0:00:00.206083\n",
            "Epoch: [45/100]\tSamples: [45000/100000]\tTrain Loss: 2047.3862734375\tTime: 0:00:00.211460\n",
            "Epoch: [46/100]\tSamples: [46000/100000]\tTrain Loss: 2046.61534375\tTime: 0:00:00.190389\n",
            "Epoch: [47/100]\tSamples: [47000/100000]\tTrain Loss: 2046.6180078125\tTime: 0:00:00.195297\n",
            "Epoch: [48/100]\tSamples: [48000/100000]\tTrain Loss: 2046.1287578125\tTime: 0:00:00.196670\n",
            "Epoch: [49/100]\tSamples: [49000/100000]\tTrain Loss: 2045.6482109375\tTime: 0:00:00.211500\n",
            "Epoch: [50/100]\tSamples: [50000/100000]\tTrain Loss: 2045.1465390625\tTime: 0:00:00.197586\n",
            "Epoch: [51/100]\tSamples: [51000/100000]\tTrain Loss: 2045.1519609375\tTime: 0:00:00.200767\n",
            "Epoch: [52/100]\tSamples: [52000/100000]\tTrain Loss: 2044.8283515625\tTime: 0:00:00.197129\n",
            "Epoch: [53/100]\tSamples: [53000/100000]\tTrain Loss: 2044.5987734375\tTime: 0:00:00.195549\n",
            "Epoch: [54/100]\tSamples: [54000/100000]\tTrain Loss: 2044.2051484375\tTime: 0:00:00.196125\n",
            "Epoch: [55/100]\tSamples: [55000/100000]\tTrain Loss: 2044.2894140625\tTime: 0:00:00.195886\n",
            "Epoch: [56/100]\tSamples: [56000/100000]\tTrain Loss: 2043.8692578125\tTime: 0:00:00.208333\n",
            "Epoch: [57/100]\tSamples: [57000/100000]\tTrain Loss: 2044.6507734375\tTime: 0:00:00.206750\n",
            "Epoch: [58/100]\tSamples: [58000/100000]\tTrain Loss: 2043.8998984375\tTime: 0:00:00.206068\n",
            "Epoch: [59/100]\tSamples: [59000/100000]\tTrain Loss: 2044.38375\tTime: 0:00:00.215550\n",
            "Epoch: [60/100]\tSamples: [60000/100000]\tTrain Loss: 2043.8714765625\tTime: 0:00:00.207122\n",
            "Epoch: [61/100]\tSamples: [61000/100000]\tTrain Loss: 2043.9625703125\tTime: 0:00:00.201841\n",
            "Epoch: [62/100]\tSamples: [62000/100000]\tTrain Loss: 2043.3035625\tTime: 0:00:00.198693\n",
            "Epoch: [63/100]\tSamples: [63000/100000]\tTrain Loss: 2042.807015625\tTime: 0:00:00.193208\n",
            "Epoch: [64/100]\tSamples: [64000/100000]\tTrain Loss: 2043.7900703125\tTime: 0:00:00.205380\n",
            "Epoch: [65/100]\tSamples: [65000/100000]\tTrain Loss: 2043.1175625\tTime: 0:00:00.195266\n",
            "Epoch: [66/100]\tSamples: [66000/100000]\tTrain Loss: 2043.2320078125\tTime: 0:00:00.197768\n",
            "Epoch: [67/100]\tSamples: [67000/100000]\tTrain Loss: 2042.791328125\tTime: 0:00:00.189685\n",
            "Epoch: [68/100]\tSamples: [68000/100000]\tTrain Loss: 2042.8950546875\tTime: 0:00:00.198844\n",
            "Epoch: [69/100]\tSamples: [69000/100000]\tTrain Loss: 2042.968890625\tTime: 0:00:00.188609\n",
            "Epoch: [70/100]\tSamples: [70000/100000]\tTrain Loss: 2041.9676171875\tTime: 0:00:00.190059\n",
            "Epoch: [71/100]\tSamples: [71000/100000]\tTrain Loss: 2042.5586796875\tTime: 0:00:00.205678\n",
            "Epoch: [72/100]\tSamples: [72000/100000]\tTrain Loss: 2042.816875\tTime: 0:00:00.192507\n",
            "Epoch: [73/100]\tSamples: [73000/100000]\tTrain Loss: 2042.5778828125\tTime: 0:00:00.191916\n",
            "Epoch: [74/100]\tSamples: [74000/100000]\tTrain Loss: 2042.7097265625\tTime: 0:00:00.189797\n",
            "Epoch: [75/100]\tSamples: [75000/100000]\tTrain Loss: 2042.2136875\tTime: 0:00:00.197247\n",
            "Epoch: [76/100]\tSamples: [76000/100000]\tTrain Loss: 2042.361265625\tTime: 0:00:00.209189\n",
            "Epoch: [77/100]\tSamples: [77000/100000]\tTrain Loss: 2042.3154296875\tTime: 0:00:00.196620\n",
            "Epoch: [78/100]\tSamples: [78000/100000]\tTrain Loss: 2042.306453125\tTime: 0:00:00.196589\n",
            "Epoch: [79/100]\tSamples: [79000/100000]\tTrain Loss: 2043.0016953125\tTime: 0:00:00.190388\n",
            "Epoch: [80/100]\tSamples: [80000/100000]\tTrain Loss: 2042.5195546875\tTime: 0:00:00.205057\n",
            "Epoch: [81/100]\tSamples: [81000/100000]\tTrain Loss: 2042.455421875\tTime: 0:00:00.186778\n",
            "Epoch: [82/100]\tSamples: [82000/100000]\tTrain Loss: 2042.6320078125\tTime: 0:00:00.191196\n",
            "Epoch: [83/100]\tSamples: [83000/100000]\tTrain Loss: 2042.4128828125\tTime: 0:00:00.203589\n",
            "Epoch: [84/100]\tSamples: [84000/100000]\tTrain Loss: 2042.7823984375\tTime: 0:00:00.191702\n",
            "Epoch: [85/100]\tSamples: [85000/100000]\tTrain Loss: 2042.6717890625\tTime: 0:00:00.199521\n",
            "Epoch: [86/100]\tSamples: [86000/100000]\tTrain Loss: 2042.1775078125\tTime: 0:00:00.189904\n",
            "Epoch: [87/100]\tSamples: [87000/100000]\tTrain Loss: 2042.3658125\tTime: 0:00:00.201678\n",
            "Epoch: [88/100]\tSamples: [88000/100000]\tTrain Loss: 2042.264453125\tTime: 0:00:00.198884\n",
            "Epoch: [89/100]\tSamples: [89000/100000]\tTrain Loss: 2042.1141484375\tTime: 0:00:00.191025\n",
            "Epoch: [90/100]\tSamples: [90000/100000]\tTrain Loss: 2042.06159375\tTime: 0:00:00.206338\n",
            "Epoch: [91/100]\tSamples: [91000/100000]\tTrain Loss: 2041.2670234375\tTime: 0:00:00.202560\n",
            "Epoch: [92/100]\tSamples: [92000/100000]\tTrain Loss: 2041.1561796875\tTime: 0:00:00.193608\n",
            "Epoch: [93/100]\tSamples: [93000/100000]\tTrain Loss: 2041.875953125\tTime: 0:00:00.197144\n",
            "Epoch: [94/100]\tSamples: [94000/100000]\tTrain Loss: 2041.7526015625\tTime: 0:00:00.189429\n",
            "Epoch: [95/100]\tSamples: [95000/100000]\tTrain Loss: 2041.85603125\tTime: 0:00:00.201237\n",
            "Epoch: [96/100]\tSamples: [96000/100000]\tTrain Loss: 2041.5420625\tTime: 0:00:00.199717\n",
            "Epoch: [97/100]\tSamples: [97000/100000]\tTrain Loss: 2041.17303125\tTime: 0:00:00.192030\n",
            "Epoch: [98/100]\tSamples: [98000/100000]\tTrain Loss: 2042.069171875\tTime: 0:00:00.190111\n",
            "Epoch: [99/100]\tSamples: [99000/100000]\tTrain Loss: 2041.9226328125\tTime: 0:00:00.187537\n",
            "Epoch: [100/100]\tSamples: [100000/100000]\tTrain Loss: 2042.4571328125\tTime: 0:00:00.190362\n",
            "Settings: \n",
            "               N Components: 10\n",
            "               Topic Prior Mean: 0.0\n",
            "               Topic Prior Variance: 0.9\n",
            "               Model Type: prodLDA\n",
            "               Hidden Sizes: (100, 100)\n",
            "               Activation: softplus\n",
            "               Dropout: 0.2\n",
            "               Learn Priors: True\n",
            "               Learning Rate: 0.002\n",
            "               Momentum: 0.99\n",
            "               Reduce On Plateau: False\n",
            "               Save Dir: None\n",
            "Epoch: [1/100]\tSamples: [1000/100000]\tTrain Loss: 2193.8192734375\tTime: 0:00:00.211709\n",
            "Epoch: [2/100]\tSamples: [2000/100000]\tTrain Loss: 2176.7059140625\tTime: 0:00:00.208457\n",
            "Epoch: [3/100]\tSamples: [3000/100000]\tTrain Loss: 2168.1902734375\tTime: 0:00:00.202502\n",
            "Epoch: [4/100]\tSamples: [4000/100000]\tTrain Loss: 2157.603765625\tTime: 0:00:00.204765\n",
            "Epoch: [5/100]\tSamples: [5000/100000]\tTrain Loss: 2150.7643828125\tTime: 0:00:00.195669\n",
            "Epoch: [6/100]\tSamples: [6000/100000]\tTrain Loss: 2144.335421875\tTime: 0:00:00.192079\n",
            "Epoch: [7/100]\tSamples: [7000/100000]\tTrain Loss: 2137.4378359375\tTime: 0:00:00.208368\n",
            "Epoch: [8/100]\tSamples: [8000/100000]\tTrain Loss: 2132.145515625\tTime: 0:00:00.199502\n",
            "Epoch: [9/100]\tSamples: [9000/100000]\tTrain Loss: 2127.7365546875\tTime: 0:00:00.210994\n",
            "Epoch: [10/100]\tSamples: [10000/100000]\tTrain Loss: 2124.4275859375\tTime: 0:00:00.202675\n",
            "Epoch: [11/100]\tSamples: [11000/100000]\tTrain Loss: 2118.41078125\tTime: 0:00:00.205748\n",
            "Epoch: [12/100]\tSamples: [12000/100000]\tTrain Loss: 2114.32696875\tTime: 0:00:00.206690\n",
            "Epoch: [13/100]\tSamples: [13000/100000]\tTrain Loss: 2111.2709765625\tTime: 0:00:00.196522\n",
            "Epoch: [14/100]\tSamples: [14000/100000]\tTrain Loss: 2106.2425625\tTime: 0:00:00.210232\n",
            "Epoch: [15/100]\tSamples: [15000/100000]\tTrain Loss: 2102.2034609375\tTime: 0:00:00.196652\n",
            "Epoch: [16/100]\tSamples: [16000/100000]\tTrain Loss: 2097.828328125\tTime: 0:00:00.202710\n",
            "Epoch: [17/100]\tSamples: [17000/100000]\tTrain Loss: 2095.9351953125\tTime: 0:00:00.191202\n",
            "Epoch: [18/100]\tSamples: [18000/100000]\tTrain Loss: 2091.83078125\tTime: 0:00:00.199677\n",
            "Epoch: [19/100]\tSamples: [19000/100000]\tTrain Loss: 2088.7559296875\tTime: 0:00:00.224580\n",
            "Epoch: [20/100]\tSamples: [20000/100000]\tTrain Loss: 2087.2602734375\tTime: 0:00:00.221737\n",
            "Epoch: [21/100]\tSamples: [21000/100000]\tTrain Loss: 2082.7466015625\tTime: 0:00:00.213606\n",
            "Epoch: [22/100]\tSamples: [22000/100000]\tTrain Loss: 2079.8093671875\tTime: 0:00:00.219340\n",
            "Epoch: [23/100]\tSamples: [23000/100000]\tTrain Loss: 2078.1464453125\tTime: 0:00:00.222883\n",
            "Epoch: [24/100]\tSamples: [24000/100000]\tTrain Loss: 2074.991921875\tTime: 0:00:00.239399\n",
            "Epoch: [25/100]\tSamples: [25000/100000]\tTrain Loss: 2073.098\tTime: 0:00:00.215849\n",
            "Epoch: [26/100]\tSamples: [26000/100000]\tTrain Loss: 2072.34525\tTime: 0:00:00.228411\n",
            "Epoch: [27/100]\tSamples: [27000/100000]\tTrain Loss: 2069.784984375\tTime: 0:00:00.214390\n",
            "Epoch: [28/100]\tSamples: [28000/100000]\tTrain Loss: 2069.5614609375\tTime: 0:00:00.218913\n",
            "Epoch: [29/100]\tSamples: [29000/100000]\tTrain Loss: 2067.285078125\tTime: 0:00:00.236942\n",
            "Epoch: [30/100]\tSamples: [30000/100000]\tTrain Loss: 2066.403125\tTime: 0:00:00.227182\n",
            "Epoch: [31/100]\tSamples: [31000/100000]\tTrain Loss: 2064.6764375\tTime: 0:00:00.217015\n",
            "Epoch: [32/100]\tSamples: [32000/100000]\tTrain Loss: 2064.323640625\tTime: 0:00:00.221376\n",
            "Epoch: [33/100]\tSamples: [33000/100000]\tTrain Loss: 2063.03796875\tTime: 0:00:00.208036\n",
            "Epoch: [34/100]\tSamples: [34000/100000]\tTrain Loss: 2062.902171875\tTime: 0:00:00.223594\n",
            "Epoch: [35/100]\tSamples: [35000/100000]\tTrain Loss: 2061.504828125\tTime: 0:00:00.216742\n",
            "Epoch: [36/100]\tSamples: [36000/100000]\tTrain Loss: 2060.5449375\tTime: 0:00:00.202274\n",
            "Epoch: [37/100]\tSamples: [37000/100000]\tTrain Loss: 2060.469296875\tTime: 0:00:00.192222\n",
            "Epoch: [38/100]\tSamples: [38000/100000]\tTrain Loss: 2058.807734375\tTime: 0:00:00.227391\n",
            "Epoch: [39/100]\tSamples: [39000/100000]\tTrain Loss: 2058.7264765625\tTime: 0:00:00.207890\n",
            "Epoch: [40/100]\tSamples: [40000/100000]\tTrain Loss: 2058.4537421875\tTime: 0:00:00.205629\n",
            "Epoch: [41/100]\tSamples: [41000/100000]\tTrain Loss: 2058.0102265625\tTime: 0:00:00.208942\n",
            "Epoch: [42/100]\tSamples: [42000/100000]\tTrain Loss: 2057.7161953125\tTime: 0:00:00.199045\n",
            "Epoch: [43/100]\tSamples: [43000/100000]\tTrain Loss: 2057.1628515625\tTime: 0:00:00.223409\n",
            "Epoch: [44/100]\tSamples: [44000/100000]\tTrain Loss: 2057.2630390625\tTime: 0:00:00.213045\n",
            "Epoch: [45/100]\tSamples: [45000/100000]\tTrain Loss: 2056.599453125\tTime: 0:00:00.203938\n",
            "Epoch: [46/100]\tSamples: [46000/100000]\tTrain Loss: 2056.0954375\tTime: 0:00:00.210687\n",
            "Epoch: [47/100]\tSamples: [47000/100000]\tTrain Loss: 2056.8583984375\tTime: 0:00:00.224981\n",
            "Epoch: [48/100]\tSamples: [48000/100000]\tTrain Loss: 2056.4687265625\tTime: 0:00:00.217772\n",
            "Epoch: [49/100]\tSamples: [49000/100000]\tTrain Loss: 2055.5085390625\tTime: 0:00:00.214361\n",
            "Epoch: [50/100]\tSamples: [50000/100000]\tTrain Loss: 2055.9997421875\tTime: 0:00:00.226144\n",
            "Epoch: [51/100]\tSamples: [51000/100000]\tTrain Loss: 2055.5980859375\tTime: 0:00:00.210895\n",
            "Epoch: [52/100]\tSamples: [52000/100000]\tTrain Loss: 2054.7798515625\tTime: 0:00:00.207109\n",
            "Epoch: [53/100]\tSamples: [53000/100000]\tTrain Loss: 2054.4492578125\tTime: 0:00:00.230061\n",
            "Epoch: [54/100]\tSamples: [54000/100000]\tTrain Loss: 2054.111796875\tTime: 0:00:00.221643\n",
            "Epoch: [55/100]\tSamples: [55000/100000]\tTrain Loss: 2054.7591328125\tTime: 0:00:00.208015\n",
            "Epoch: [56/100]\tSamples: [56000/100000]\tTrain Loss: 2054.335671875\tTime: 0:00:00.203051\n",
            "Epoch: [57/100]\tSamples: [57000/100000]\tTrain Loss: 2053.9816171875\tTime: 0:00:00.225900\n",
            "Epoch: [58/100]\tSamples: [58000/100000]\tTrain Loss: 2054.27253125\tTime: 0:00:00.221745\n",
            "Epoch: [59/100]\tSamples: [59000/100000]\tTrain Loss: 2053.43225\tTime: 0:00:00.194233\n",
            "Epoch: [60/100]\tSamples: [60000/100000]\tTrain Loss: 2053.6050859375\tTime: 0:00:00.195986\n",
            "Epoch: [61/100]\tSamples: [61000/100000]\tTrain Loss: 2054.4543203125\tTime: 0:00:00.212363\n",
            "Epoch: [62/100]\tSamples: [62000/100000]\tTrain Loss: 2053.1449765625\tTime: 0:00:00.218702\n",
            "Epoch: [63/100]\tSamples: [63000/100000]\tTrain Loss: 2053.69690625\tTime: 0:00:00.214308\n",
            "Epoch: [64/100]\tSamples: [64000/100000]\tTrain Loss: 2053.74815625\tTime: 0:00:00.204410\n",
            "Epoch: [65/100]\tSamples: [65000/100000]\tTrain Loss: 2053.3232734375\tTime: 0:00:00.212487\n",
            "Epoch: [66/100]\tSamples: [66000/100000]\tTrain Loss: 2053.2143515625\tTime: 0:00:00.211498\n",
            "Epoch: [67/100]\tSamples: [67000/100000]\tTrain Loss: 2053.2104921875\tTime: 0:00:00.221247\n",
            "Epoch: [68/100]\tSamples: [68000/100000]\tTrain Loss: 2052.8565078125\tTime: 0:00:00.202840\n",
            "Epoch: [69/100]\tSamples: [69000/100000]\tTrain Loss: 2052.6294453125\tTime: 0:00:00.201030\n",
            "Epoch: [70/100]\tSamples: [70000/100000]\tTrain Loss: 2053.023640625\tTime: 0:00:00.211432\n",
            "Epoch: [71/100]\tSamples: [71000/100000]\tTrain Loss: 2053.4222265625\tTime: 0:00:00.219609\n",
            "Epoch: [72/100]\tSamples: [72000/100000]\tTrain Loss: 2053.5393984375\tTime: 0:00:00.203366\n",
            "Epoch: [73/100]\tSamples: [73000/100000]\tTrain Loss: 2053.3172578125\tTime: 0:00:00.220849\n",
            "Epoch: [74/100]\tSamples: [74000/100000]\tTrain Loss: 2052.9789453125\tTime: 0:00:00.210812\n",
            "Epoch: [75/100]\tSamples: [75000/100000]\tTrain Loss: 2052.401953125\tTime: 0:00:00.202300\n",
            "Epoch: [76/100]\tSamples: [76000/100000]\tTrain Loss: 2052.1980625\tTime: 0:00:00.211585\n",
            "Epoch: [77/100]\tSamples: [77000/100000]\tTrain Loss: 2053.2224765625\tTime: 0:00:00.219097\n",
            "Epoch: [78/100]\tSamples: [78000/100000]\tTrain Loss: 2053.241390625\tTime: 0:00:00.211039\n",
            "Epoch: [79/100]\tSamples: [79000/100000]\tTrain Loss: 2051.7879453125\tTime: 0:00:00.211727\n",
            "Epoch: [80/100]\tSamples: [80000/100000]\tTrain Loss: 2052.5962578125\tTime: 0:00:00.199800\n",
            "Epoch: [81/100]\tSamples: [81000/100000]\tTrain Loss: 2052.9021015625\tTime: 0:00:00.194600\n",
            "Epoch: [82/100]\tSamples: [82000/100000]\tTrain Loss: 2052.163171875\tTime: 0:00:00.201050\n",
            "Epoch: [83/100]\tSamples: [83000/100000]\tTrain Loss: 2052.7699453125\tTime: 0:00:00.205432\n",
            "Epoch: [84/100]\tSamples: [84000/100000]\tTrain Loss: 2051.9522578125\tTime: 0:00:00.212648\n",
            "Epoch: [85/100]\tSamples: [85000/100000]\tTrain Loss: 2053.1605546875\tTime: 0:00:00.205682\n",
            "Epoch: [86/100]\tSamples: [86000/100000]\tTrain Loss: 2051.9856171875\tTime: 0:00:00.238404\n",
            "Epoch: [87/100]\tSamples: [87000/100000]\tTrain Loss: 2052.5326484375\tTime: 0:00:00.211648\n",
            "Epoch: [88/100]\tSamples: [88000/100000]\tTrain Loss: 2051.8577578125\tTime: 0:00:00.222949\n",
            "Epoch: [89/100]\tSamples: [89000/100000]\tTrain Loss: 2052.4134921875\tTime: 0:00:00.207237\n",
            "Epoch: [90/100]\tSamples: [90000/100000]\tTrain Loss: 2052.3533671875\tTime: 0:00:00.205450\n",
            "Epoch: [91/100]\tSamples: [91000/100000]\tTrain Loss: 2052.07534375\tTime: 0:00:00.209057\n",
            "Epoch: [92/100]\tSamples: [92000/100000]\tTrain Loss: 2052.031265625\tTime: 0:00:00.203975\n",
            "Epoch: [93/100]\tSamples: [93000/100000]\tTrain Loss: 2052.317578125\tTime: 0:00:00.217194\n",
            "Epoch: [94/100]\tSamples: [94000/100000]\tTrain Loss: 2052.03890625\tTime: 0:00:00.208855\n",
            "Epoch: [95/100]\tSamples: [95000/100000]\tTrain Loss: 2052.1499140625\tTime: 0:00:00.204489\n",
            "Epoch: [96/100]\tSamples: [96000/100000]\tTrain Loss: 2050.93190625\tTime: 0:00:00.209001\n",
            "Epoch: [97/100]\tSamples: [97000/100000]\tTrain Loss: 2051.806859375\tTime: 0:00:00.205280\n",
            "Epoch: [98/100]\tSamples: [98000/100000]\tTrain Loss: 2051.8481484375\tTime: 0:00:00.208968\n",
            "Epoch: [99/100]\tSamples: [99000/100000]\tTrain Loss: 2051.3555859375\tTime: 0:00:00.211948\n",
            "Epoch: [100/100]\tSamples: [100000/100000]\tTrain Loss: 2052.251796875\tTime: 0:00:00.194097\n",
            "Settings: \n",
            "               N Components: 10\n",
            "               Topic Prior Mean: 0.0\n",
            "               Topic Prior Variance: 0.9\n",
            "               Model Type: prodLDA\n",
            "               Hidden Sizes: (100, 100)\n",
            "               Activation: softplus\n",
            "               Dropout: 0.2\n",
            "               Learn Priors: True\n",
            "               Learning Rate: 0.002\n",
            "               Momentum: 0.99\n",
            "               Reduce On Plateau: False\n",
            "               Save Dir: None\n",
            "Epoch: [1/100]\tSamples: [1000/100000]\tTrain Loss: 2149.2854453125\tTime: 0:00:00.200233\n",
            "Epoch: [2/100]\tSamples: [2000/100000]\tTrain Loss: 2134.53746875\tTime: 0:00:00.209034\n",
            "Epoch: [3/100]\tSamples: [3000/100000]\tTrain Loss: 2124.5187734375\tTime: 0:00:00.201111\n",
            "Epoch: [4/100]\tSamples: [4000/100000]\tTrain Loss: 2114.5108359375\tTime: 0:00:00.200088\n",
            "Epoch: [5/100]\tSamples: [5000/100000]\tTrain Loss: 2109.01721875\tTime: 0:00:00.219246\n",
            "Epoch: [6/100]\tSamples: [6000/100000]\tTrain Loss: 2105.3123828125\tTime: 0:00:00.187606\n",
            "Epoch: [7/100]\tSamples: [7000/100000]\tTrain Loss: 2101.4993359375\tTime: 0:00:00.198061\n",
            "Epoch: [8/100]\tSamples: [8000/100000]\tTrain Loss: 2094.8851640625\tTime: 0:00:00.198614\n",
            "Epoch: [9/100]\tSamples: [9000/100000]\tTrain Loss: 2092.5190234375\tTime: 0:00:00.187970\n",
            "Epoch: [10/100]\tSamples: [10000/100000]\tTrain Loss: 2086.3725078125\tTime: 0:00:00.209305\n",
            "Epoch: [11/100]\tSamples: [11000/100000]\tTrain Loss: 2084.741015625\tTime: 0:00:00.206319\n",
            "Epoch: [12/100]\tSamples: [12000/100000]\tTrain Loss: 2082.5078046875\tTime: 0:00:00.204859\n",
            "Epoch: [13/100]\tSamples: [13000/100000]\tTrain Loss: 2077.933609375\tTime: 0:00:00.189896\n",
            "Epoch: [14/100]\tSamples: [14000/100000]\tTrain Loss: 2073.29909375\tTime: 0:00:00.191420\n",
            "Epoch: [15/100]\tSamples: [15000/100000]\tTrain Loss: 2070.3669375\tTime: 0:00:00.202185\n",
            "Epoch: [16/100]\tSamples: [16000/100000]\tTrain Loss: 2067.8643125\tTime: 0:00:00.195013\n",
            "Epoch: [17/100]\tSamples: [17000/100000]\tTrain Loss: 2063.000671875\tTime: 0:00:00.192205\n",
            "Epoch: [18/100]\tSamples: [18000/100000]\tTrain Loss: 2059.324765625\tTime: 0:00:00.199847\n",
            "Epoch: [19/100]\tSamples: [19000/100000]\tTrain Loss: 2056.4466484375\tTime: 0:00:00.196317\n",
            "Epoch: [20/100]\tSamples: [20000/100000]\tTrain Loss: 2053.4822578125\tTime: 0:00:00.214124\n",
            "Epoch: [21/100]\tSamples: [21000/100000]\tTrain Loss: 2051.4867734375\tTime: 0:00:00.196663\n",
            "Epoch: [22/100]\tSamples: [22000/100000]\tTrain Loss: 2045.62859375\tTime: 0:00:00.187722\n",
            "Epoch: [23/100]\tSamples: [23000/100000]\tTrain Loss: 2042.6340390625\tTime: 0:00:00.200687\n",
            "Epoch: [24/100]\tSamples: [24000/100000]\tTrain Loss: 2038.8803984375\tTime: 0:00:00.215089\n",
            "Epoch: [25/100]\tSamples: [25000/100000]\tTrain Loss: 2036.600625\tTime: 0:00:00.214767\n",
            "Epoch: [26/100]\tSamples: [26000/100000]\tTrain Loss: 2034.250953125\tTime: 0:00:00.194842\n",
            "Epoch: [27/100]\tSamples: [27000/100000]\tTrain Loss: 2031.2369921875\tTime: 0:00:00.194638\n",
            "Epoch: [28/100]\tSamples: [28000/100000]\tTrain Loss: 2029.77096875\tTime: 0:00:00.202650\n",
            "Epoch: [29/100]\tSamples: [29000/100000]\tTrain Loss: 2028.093921875\tTime: 0:00:00.186483\n",
            "Epoch: [30/100]\tSamples: [30000/100000]\tTrain Loss: 2026.0564609375\tTime: 0:00:00.189976\n",
            "Epoch: [31/100]\tSamples: [31000/100000]\tTrain Loss: 2025.09728125\tTime: 0:00:00.205149\n",
            "Epoch: [32/100]\tSamples: [32000/100000]\tTrain Loss: 2023.3125625\tTime: 0:00:00.196428\n",
            "Epoch: [33/100]\tSamples: [33000/100000]\tTrain Loss: 2022.576578125\tTime: 0:00:00.196262\n",
            "Epoch: [34/100]\tSamples: [34000/100000]\tTrain Loss: 2021.5674375\tTime: 0:00:00.190989\n",
            "Epoch: [35/100]\tSamples: [35000/100000]\tTrain Loss: 2020.655296875\tTime: 0:00:00.204633\n",
            "Epoch: [36/100]\tSamples: [36000/100000]\tTrain Loss: 2019.19096875\tTime: 0:00:00.202345\n",
            "Epoch: [37/100]\tSamples: [37000/100000]\tTrain Loss: 2019.086875\tTime: 0:00:00.195163\n",
            "Epoch: [38/100]\tSamples: [38000/100000]\tTrain Loss: 2019.69303125\tTime: 0:00:00.212597\n",
            "Epoch: [39/100]\tSamples: [39000/100000]\tTrain Loss: 2017.6785078125\tTime: 0:00:00.197876\n",
            "Epoch: [40/100]\tSamples: [40000/100000]\tTrain Loss: 2017.4566171875\tTime: 0:00:00.183566\n",
            "Epoch: [41/100]\tSamples: [41000/100000]\tTrain Loss: 2017.0095703125\tTime: 0:00:00.202842\n",
            "Epoch: [42/100]\tSamples: [42000/100000]\tTrain Loss: 2016.7862109375\tTime: 0:00:00.194660\n",
            "Epoch: [43/100]\tSamples: [43000/100000]\tTrain Loss: 2016.3488671875\tTime: 0:00:00.201779\n",
            "Epoch: [44/100]\tSamples: [44000/100000]\tTrain Loss: 2016.1696484375\tTime: 0:00:00.194193\n",
            "Epoch: [45/100]\tSamples: [45000/100000]\tTrain Loss: 2015.19065625\tTime: 0:00:00.205413\n",
            "Epoch: [46/100]\tSamples: [46000/100000]\tTrain Loss: 2015.407765625\tTime: 0:00:00.215562\n",
            "Epoch: [47/100]\tSamples: [47000/100000]\tTrain Loss: 2014.9630078125\tTime: 0:00:00.197155\n",
            "Epoch: [48/100]\tSamples: [48000/100000]\tTrain Loss: 2014.1437734375\tTime: 0:00:00.188400\n",
            "Epoch: [49/100]\tSamples: [49000/100000]\tTrain Loss: 2014.33075\tTime: 0:00:00.198535\n",
            "Epoch: [50/100]\tSamples: [50000/100000]\tTrain Loss: 2014.0359296875\tTime: 0:00:00.191219\n",
            "Epoch: [51/100]\tSamples: [51000/100000]\tTrain Loss: 2013.963296875\tTime: 0:00:00.207984\n",
            "Epoch: [52/100]\tSamples: [52000/100000]\tTrain Loss: 2014.36584375\tTime: 0:00:00.193198\n",
            "Epoch: [53/100]\tSamples: [53000/100000]\tTrain Loss: 2013.144625\tTime: 0:00:00.185167\n",
            "Epoch: [54/100]\tSamples: [54000/100000]\tTrain Loss: 2013.3215859375\tTime: 0:00:00.204363\n",
            "Epoch: [55/100]\tSamples: [55000/100000]\tTrain Loss: 2013.4341328125\tTime: 0:00:00.191063\n",
            "Epoch: [56/100]\tSamples: [56000/100000]\tTrain Loss: 2013.5038984375\tTime: 0:00:00.230414\n",
            "Epoch: [57/100]\tSamples: [57000/100000]\tTrain Loss: 2013.482453125\tTime: 0:00:00.209025\n",
            "Epoch: [58/100]\tSamples: [58000/100000]\tTrain Loss: 2012.900578125\tTime: 0:00:00.216133\n",
            "Epoch: [59/100]\tSamples: [59000/100000]\tTrain Loss: 2012.8262890625\tTime: 0:00:00.190247\n",
            "Epoch: [60/100]\tSamples: [60000/100000]\tTrain Loss: 2012.6091171875\tTime: 0:00:00.202650\n",
            "Epoch: [61/100]\tSamples: [61000/100000]\tTrain Loss: 2012.9330390625\tTime: 0:00:00.221848\n",
            "Epoch: [62/100]\tSamples: [62000/100000]\tTrain Loss: 2012.867328125\tTime: 0:00:00.199052\n",
            "Epoch: [63/100]\tSamples: [63000/100000]\tTrain Loss: 2011.9918359375\tTime: 0:00:00.211823\n",
            "Epoch: [64/100]\tSamples: [64000/100000]\tTrain Loss: 2012.18146875\tTime: 0:00:00.210597\n",
            "Epoch: [65/100]\tSamples: [65000/100000]\tTrain Loss: 2011.7703828125\tTime: 0:00:00.205402\n",
            "Epoch: [66/100]\tSamples: [66000/100000]\tTrain Loss: 2012.5444453125\tTime: 0:00:00.224264\n",
            "Epoch: [67/100]\tSamples: [67000/100000]\tTrain Loss: 2012.05334375\tTime: 0:00:00.209267\n",
            "Epoch: [68/100]\tSamples: [68000/100000]\tTrain Loss: 2012.3861953125\tTime: 0:00:00.199339\n",
            "Epoch: [69/100]\tSamples: [69000/100000]\tTrain Loss: 2012.5896796875\tTime: 0:00:00.192650\n",
            "Epoch: [70/100]\tSamples: [70000/100000]\tTrain Loss: 2011.276125\tTime: 0:00:00.190924\n",
            "Epoch: [71/100]\tSamples: [71000/100000]\tTrain Loss: 2011.179578125\tTime: 0:00:00.208915\n",
            "Epoch: [72/100]\tSamples: [72000/100000]\tTrain Loss: 2011.5942421875\tTime: 0:00:00.196016\n",
            "Epoch: [73/100]\tSamples: [73000/100000]\tTrain Loss: 2011.3616484375\tTime: 0:00:00.207979\n",
            "Epoch: [74/100]\tSamples: [74000/100000]\tTrain Loss: 2011.722125\tTime: 0:00:00.197726\n",
            "Epoch: [75/100]\tSamples: [75000/100000]\tTrain Loss: 2011.85615625\tTime: 0:00:00.193672\n",
            "Epoch: [76/100]\tSamples: [76000/100000]\tTrain Loss: 2010.9175\tTime: 0:00:00.212238\n",
            "Epoch: [77/100]\tSamples: [77000/100000]\tTrain Loss: 2011.431546875\tTime: 0:00:00.208174\n",
            "Epoch: [78/100]\tSamples: [78000/100000]\tTrain Loss: 2012.36865625\tTime: 0:00:00.191625\n",
            "Epoch: [79/100]\tSamples: [79000/100000]\tTrain Loss: 2011.4515078125\tTime: 0:00:00.206285\n",
            "Epoch: [80/100]\tSamples: [80000/100000]\tTrain Loss: 2011.3504609375\tTime: 0:00:00.195484\n",
            "Epoch: [81/100]\tSamples: [81000/100000]\tTrain Loss: 2011.2749375\tTime: 0:00:00.199305\n",
            "Epoch: [82/100]\tSamples: [82000/100000]\tTrain Loss: 2011.7921953125\tTime: 0:00:00.210233\n",
            "Epoch: [83/100]\tSamples: [83000/100000]\tTrain Loss: 2011.281375\tTime: 0:00:00.200940\n",
            "Epoch: [84/100]\tSamples: [84000/100000]\tTrain Loss: 2010.7667890625\tTime: 0:00:00.210466\n",
            "Epoch: [85/100]\tSamples: [85000/100000]\tTrain Loss: 2010.5415390625\tTime: 0:00:00.195226\n",
            "Epoch: [86/100]\tSamples: [86000/100000]\tTrain Loss: 2010.547421875\tTime: 0:00:00.203215\n",
            "Epoch: [87/100]\tSamples: [87000/100000]\tTrain Loss: 2010.5688359375\tTime: 0:00:00.209039\n",
            "Epoch: [88/100]\tSamples: [88000/100000]\tTrain Loss: 2010.8993359375\tTime: 0:00:00.194856\n",
            "Epoch: [89/100]\tSamples: [89000/100000]\tTrain Loss: 2010.730375\tTime: 0:00:00.213437\n",
            "Epoch: [90/100]\tSamples: [90000/100000]\tTrain Loss: 2011.0048203125\tTime: 0:00:00.198174\n",
            "Epoch: [91/100]\tSamples: [91000/100000]\tTrain Loss: 2011.078328125\tTime: 0:00:00.200810\n",
            "Epoch: [92/100]\tSamples: [92000/100000]\tTrain Loss: 2010.3266875\tTime: 0:00:00.198238\n",
            "Epoch: [93/100]\tSamples: [93000/100000]\tTrain Loss: 2010.90540625\tTime: 0:00:00.199661\n",
            "Epoch: [94/100]\tSamples: [94000/100000]\tTrain Loss: 2011.115015625\tTime: 0:00:00.209154\n",
            "Epoch: [95/100]\tSamples: [95000/100000]\tTrain Loss: 2009.8793046875\tTime: 0:00:00.208366\n",
            "Epoch: [96/100]\tSamples: [96000/100000]\tTrain Loss: 2010.779125\tTime: 0:00:00.213268\n",
            "Epoch: [97/100]\tSamples: [97000/100000]\tTrain Loss: 2010.757265625\tTime: 0:00:00.205815\n",
            "Epoch: [98/100]\tSamples: [98000/100000]\tTrain Loss: 2009.8291796875\tTime: 0:00:00.216309\n",
            "Epoch: [99/100]\tSamples: [99000/100000]\tTrain Loss: 2011.205375\tTime: 0:00:00.211661\n",
            "Epoch: [100/100]\tSamples: [100000/100000]\tTrain Loss: 2010.34225\tTime: 0:00:00.196840\n",
            "Settings: \n",
            "               N Components: 10\n",
            "               Topic Prior Mean: 0.0\n",
            "               Topic Prior Variance: 0.9\n",
            "               Model Type: prodLDA\n",
            "               Hidden Sizes: (100, 100)\n",
            "               Activation: softplus\n",
            "               Dropout: 0.2\n",
            "               Learn Priors: True\n",
            "               Learning Rate: 0.002\n",
            "               Momentum: 0.99\n",
            "               Reduce On Plateau: False\n",
            "               Save Dir: None\n",
            "Epoch: [1/100]\tSamples: [1000/100000]\tTrain Loss: 2244.5757890625\tTime: 0:00:00.204518\n",
            "Epoch: [2/100]\tSamples: [2000/100000]\tTrain Loss: 2229.452015625\tTime: 0:00:00.197450\n",
            "Epoch: [3/100]\tSamples: [3000/100000]\tTrain Loss: 2220.4971015625\tTime: 0:00:00.203592\n",
            "Epoch: [4/100]\tSamples: [4000/100000]\tTrain Loss: 2213.154953125\tTime: 0:00:00.197024\n",
            "Epoch: [5/100]\tSamples: [5000/100000]\tTrain Loss: 2205.353734375\tTime: 0:00:00.207380\n",
            "Epoch: [6/100]\tSamples: [6000/100000]\tTrain Loss: 2199.729671875\tTime: 0:00:00.212180\n",
            "Epoch: [7/100]\tSamples: [7000/100000]\tTrain Loss: 2194.08584375\tTime: 0:00:00.214065\n",
            "Epoch: [8/100]\tSamples: [8000/100000]\tTrain Loss: 2187.916609375\tTime: 0:00:00.197575\n",
            "Epoch: [9/100]\tSamples: [9000/100000]\tTrain Loss: 2181.617578125\tTime: 0:00:00.204416\n",
            "Epoch: [10/100]\tSamples: [10000/100000]\tTrain Loss: 2176.487390625\tTime: 0:00:00.213811\n",
            "Epoch: [11/100]\tSamples: [11000/100000]\tTrain Loss: 2172.4594296875\tTime: 0:00:00.193855\n",
            "Epoch: [12/100]\tSamples: [12000/100000]\tTrain Loss: 2167.49478125\tTime: 0:00:00.200952\n",
            "Epoch: [13/100]\tSamples: [13000/100000]\tTrain Loss: 2162.1113515625\tTime: 0:00:00.197066\n",
            "Epoch: [14/100]\tSamples: [14000/100000]\tTrain Loss: 2156.9509375\tTime: 0:00:00.208007\n",
            "Epoch: [15/100]\tSamples: [15000/100000]\tTrain Loss: 2152.1660625\tTime: 0:00:00.228305\n",
            "Epoch: [16/100]\tSamples: [16000/100000]\tTrain Loss: 2148.8790859375\tTime: 0:00:00.212607\n",
            "Epoch: [17/100]\tSamples: [17000/100000]\tTrain Loss: 2143.0329296875\tTime: 0:00:00.216083\n",
            "Epoch: [18/100]\tSamples: [18000/100000]\tTrain Loss: 2138.4831484375\tTime: 0:00:00.211380\n",
            "Epoch: [19/100]\tSamples: [19000/100000]\tTrain Loss: 2135.7469375\tTime: 0:00:00.197198\n",
            "Epoch: [20/100]\tSamples: [20000/100000]\tTrain Loss: 2133.59409375\tTime: 0:00:00.211571\n",
            "Epoch: [21/100]\tSamples: [21000/100000]\tTrain Loss: 2130.02565625\tTime: 0:00:00.194906\n",
            "Epoch: [22/100]\tSamples: [22000/100000]\tTrain Loss: 2128.471578125\tTime: 0:00:00.217454\n",
            "Epoch: [23/100]\tSamples: [23000/100000]\tTrain Loss: 2126.448\tTime: 0:00:00.217016\n",
            "Epoch: [24/100]\tSamples: [24000/100000]\tTrain Loss: 2124.0194453125\tTime: 0:00:00.225136\n",
            "Epoch: [25/100]\tSamples: [25000/100000]\tTrain Loss: 2121.95809375\tTime: 0:00:00.210612\n",
            "Epoch: [26/100]\tSamples: [26000/100000]\tTrain Loss: 2120.968515625\tTime: 0:00:00.198125\n",
            "Epoch: [27/100]\tSamples: [27000/100000]\tTrain Loss: 2119.7896328125\tTime: 0:00:00.218199\n",
            "Epoch: [28/100]\tSamples: [28000/100000]\tTrain Loss: 2117.423734375\tTime: 0:00:00.226632\n",
            "Epoch: [29/100]\tSamples: [29000/100000]\tTrain Loss: 2117.0780703125\tTime: 0:00:00.232741\n",
            "Epoch: [30/100]\tSamples: [30000/100000]\tTrain Loss: 2115.550875\tTime: 0:00:00.204920\n",
            "Epoch: [31/100]\tSamples: [31000/100000]\tTrain Loss: 2115.0790390625\tTime: 0:00:00.206814\n",
            "Epoch: [32/100]\tSamples: [32000/100000]\tTrain Loss: 2113.17771875\tTime: 0:00:00.204996\n",
            "Epoch: [33/100]\tSamples: [33000/100000]\tTrain Loss: 2113.045765625\tTime: 0:00:00.219011\n",
            "Epoch: [34/100]\tSamples: [34000/100000]\tTrain Loss: 2111.8346953125\tTime: 0:00:00.220660\n",
            "Epoch: [35/100]\tSamples: [35000/100000]\tTrain Loss: 2111.126625\tTime: 0:00:00.220219\n",
            "Epoch: [36/100]\tSamples: [36000/100000]\tTrain Loss: 2110.659328125\tTime: 0:00:00.209224\n",
            "Epoch: [37/100]\tSamples: [37000/100000]\tTrain Loss: 2109.3772734375\tTime: 0:00:00.226436\n",
            "Epoch: [38/100]\tSamples: [38000/100000]\tTrain Loss: 2109.1341796875\tTime: 0:00:00.203796\n",
            "Epoch: [39/100]\tSamples: [39000/100000]\tTrain Loss: 2109.1334296875\tTime: 0:00:00.215674\n",
            "Epoch: [40/100]\tSamples: [40000/100000]\tTrain Loss: 2109.030953125\tTime: 0:00:00.215298\n",
            "Epoch: [41/100]\tSamples: [41000/100000]\tTrain Loss: 2108.1077578125\tTime: 0:00:00.203080\n",
            "Epoch: [42/100]\tSamples: [42000/100000]\tTrain Loss: 2108.1080078125\tTime: 0:00:00.206547\n",
            "Epoch: [43/100]\tSamples: [43000/100000]\tTrain Loss: 2107.237546875\tTime: 0:00:00.224738\n",
            "Epoch: [44/100]\tSamples: [44000/100000]\tTrain Loss: 2107.6241484375\tTime: 0:00:00.206527\n",
            "Epoch: [45/100]\tSamples: [45000/100000]\tTrain Loss: 2106.683265625\tTime: 0:00:00.209452\n",
            "Epoch: [46/100]\tSamples: [46000/100000]\tTrain Loss: 2106.7171328125\tTime: 0:00:00.215690\n",
            "Epoch: [47/100]\tSamples: [47000/100000]\tTrain Loss: 2106.6152578125\tTime: 0:00:00.220657\n",
            "Epoch: [48/100]\tSamples: [48000/100000]\tTrain Loss: 2105.890046875\tTime: 0:00:00.205051\n",
            "Epoch: [49/100]\tSamples: [49000/100000]\tTrain Loss: 2106.086109375\tTime: 0:00:00.196091\n",
            "Epoch: [50/100]\tSamples: [50000/100000]\tTrain Loss: 2105.8666484375\tTime: 0:00:00.201358\n",
            "Epoch: [51/100]\tSamples: [51000/100000]\tTrain Loss: 2105.672125\tTime: 0:00:00.193527\n",
            "Epoch: [52/100]\tSamples: [52000/100000]\tTrain Loss: 2105.7143515625\tTime: 0:00:00.204174\n",
            "Epoch: [53/100]\tSamples: [53000/100000]\tTrain Loss: 2105.748984375\tTime: 0:00:00.210773\n",
            "Epoch: [54/100]\tSamples: [54000/100000]\tTrain Loss: 2105.347234375\tTime: 0:00:00.214416\n",
            "Epoch: [55/100]\tSamples: [55000/100000]\tTrain Loss: 2105.110296875\tTime: 0:00:00.205730\n",
            "Epoch: [56/100]\tSamples: [56000/100000]\tTrain Loss: 2105.0175546875\tTime: 0:00:00.202419\n",
            "Epoch: [57/100]\tSamples: [57000/100000]\tTrain Loss: 2104.21346875\tTime: 0:00:00.199266\n",
            "Epoch: [58/100]\tSamples: [58000/100000]\tTrain Loss: 2104.8405390625\tTime: 0:00:00.216410\n",
            "Epoch: [59/100]\tSamples: [59000/100000]\tTrain Loss: 2104.18234375\tTime: 0:00:00.201826\n",
            "Epoch: [60/100]\tSamples: [60000/100000]\tTrain Loss: 2103.8051796875\tTime: 0:00:00.208075\n",
            "Epoch: [61/100]\tSamples: [61000/100000]\tTrain Loss: 2104.2069375\tTime: 0:00:00.200168\n",
            "Epoch: [62/100]\tSamples: [62000/100000]\tTrain Loss: 2103.7926875\tTime: 0:00:00.203829\n",
            "Epoch: [63/100]\tSamples: [63000/100000]\tTrain Loss: 2103.3590078125\tTime: 0:00:00.204206\n",
            "Epoch: [64/100]\tSamples: [64000/100000]\tTrain Loss: 2102.6785546875\tTime: 0:00:00.195183\n",
            "Epoch: [65/100]\tSamples: [65000/100000]\tTrain Loss: 2102.823546875\tTime: 0:00:00.201108\n",
            "Epoch: [66/100]\tSamples: [66000/100000]\tTrain Loss: 2103.2175\tTime: 0:00:00.201122\n",
            "Epoch: [67/100]\tSamples: [67000/100000]\tTrain Loss: 2103.8505546875\tTime: 0:00:00.203523\n",
            "Epoch: [68/100]\tSamples: [68000/100000]\tTrain Loss: 2103.3094609375\tTime: 0:00:00.212859\n",
            "Epoch: [69/100]\tSamples: [69000/100000]\tTrain Loss: 2102.7619375\tTime: 0:00:00.210195\n",
            "Epoch: [70/100]\tSamples: [70000/100000]\tTrain Loss: 2102.974578125\tTime: 0:00:00.206949\n",
            "Epoch: [71/100]\tSamples: [71000/100000]\tTrain Loss: 2102.9912265625\tTime: 0:00:00.205898\n",
            "Epoch: [72/100]\tSamples: [72000/100000]\tTrain Loss: 2102.508375\tTime: 0:00:00.202291\n",
            "Epoch: [73/100]\tSamples: [73000/100000]\tTrain Loss: 2102.214234375\tTime: 0:00:00.214320\n",
            "Epoch: [74/100]\tSamples: [74000/100000]\tTrain Loss: 2102.2809140625\tTime: 0:00:00.206293\n",
            "Epoch: [75/100]\tSamples: [75000/100000]\tTrain Loss: 2102.0995234375\tTime: 0:00:00.203394\n",
            "Epoch: [76/100]\tSamples: [76000/100000]\tTrain Loss: 2101.2171484375\tTime: 0:00:00.204549\n",
            "Epoch: [77/100]\tSamples: [77000/100000]\tTrain Loss: 2102.8507109375\tTime: 0:00:00.204322\n",
            "Epoch: [78/100]\tSamples: [78000/100000]\tTrain Loss: 2102.6384609375\tTime: 0:00:00.205610\n",
            "Epoch: [79/100]\tSamples: [79000/100000]\tTrain Loss: 2101.9767265625\tTime: 0:00:00.201468\n",
            "Epoch: [80/100]\tSamples: [80000/100000]\tTrain Loss: 2101.9162734375\tTime: 0:00:00.206663\n",
            "Epoch: [81/100]\tSamples: [81000/100000]\tTrain Loss: 2102.3101796875\tTime: 0:00:00.208979\n",
            "Epoch: [82/100]\tSamples: [82000/100000]\tTrain Loss: 2102.59990625\tTime: 0:00:00.197680\n",
            "Epoch: [83/100]\tSamples: [83000/100000]\tTrain Loss: 2103.522484375\tTime: 0:00:00.204202\n",
            "Epoch: [84/100]\tSamples: [84000/100000]\tTrain Loss: 2102.0518515625\tTime: 0:00:00.197471\n",
            "Epoch: [85/100]\tSamples: [85000/100000]\tTrain Loss: 2101.9683671875\tTime: 0:00:00.196971\n",
            "Epoch: [86/100]\tSamples: [86000/100000]\tTrain Loss: 2102.3663984375\tTime: 0:00:00.205068\n",
            "Epoch: [87/100]\tSamples: [87000/100000]\tTrain Loss: 2101.5104375\tTime: 0:00:00.207362\n",
            "Epoch: [88/100]\tSamples: [88000/100000]\tTrain Loss: 2102.3201640625\tTime: 0:00:00.228447\n",
            "Epoch: [89/100]\tSamples: [89000/100000]\tTrain Loss: 2103.01071875\tTime: 0:00:00.224799\n",
            "Epoch: [90/100]\tSamples: [90000/100000]\tTrain Loss: 2102.23540625\tTime: 0:00:00.211050\n",
            "Epoch: [91/100]\tSamples: [91000/100000]\tTrain Loss: 2102.697484375\tTime: 0:00:00.197929\n",
            "Epoch: [92/100]\tSamples: [92000/100000]\tTrain Loss: 2101.9262578125\tTime: 0:00:00.198513\n",
            "Epoch: [93/100]\tSamples: [93000/100000]\tTrain Loss: 2102.2458984375\tTime: 0:00:00.234022\n",
            "Epoch: [94/100]\tSamples: [94000/100000]\tTrain Loss: 2101.6081328125\tTime: 0:00:00.205469\n",
            "Epoch: [95/100]\tSamples: [95000/100000]\tTrain Loss: 2101.8115859375\tTime: 0:00:00.224223\n",
            "Epoch: [96/100]\tSamples: [96000/100000]\tTrain Loss: 2101.8902265625\tTime: 0:00:00.220182\n",
            "Epoch: [97/100]\tSamples: [97000/100000]\tTrain Loss: 2102.1368046875\tTime: 0:00:00.201283\n",
            "Epoch: [98/100]\tSamples: [98000/100000]\tTrain Loss: 2101.5990703125\tTime: 0:00:00.207629\n",
            "Epoch: [99/100]\tSamples: [99000/100000]\tTrain Loss: 2102.1089296875\tTime: 0:00:00.203457\n",
            "Epoch: [100/100]\tSamples: [100000/100000]\tTrain Loss: 2101.9654453125\tTime: 0:00:00.213459\n"
          ]
        }
      ],
      "source": [
        "train_datasets = []\n",
        "avitms = []\n",
        "id2tokens = []\n",
        "for corpus_node in documents_all:\n",
        "  cv = CountVectorizer(input='content', lowercase=True, stop_words='english',\n",
        "                      max_df=0.99, min_df=0.01, binary=False)\n",
        "  \n",
        "  docs = [\" \".join(corpus_node[i]) for i in np.arange(len(corpus_node))]\n",
        "\n",
        "  train_bow = cv.fit_transform(docs)\n",
        "  train_bow = train_bow.toarray()\n",
        "\n",
        "  idx2token = cv.get_feature_names()\n",
        "  input_size = len(idx2token)\n",
        "\n",
        "  id2token = {k: v for k, v in zip(range(0, len(idx2token)), idx2token)}\n",
        "  id2tokens.append(id2token)\n",
        "\n",
        "  train_data = BOWDataset(train_bow, idx2token)\n",
        "\n",
        "  avitm = AVITM(input_size=input_size, n_components=10, model_type='prodLDA',\n",
        "                hidden_sizes=(100, 100), activation='softplus', dropout=0.2,\n",
        "                learn_priors=True, batch_size=64, lr=2e-3, momentum=0.99,\n",
        "                solver='adam', num_epochs=100, reduce_on_plateau=False)\n",
        "\n",
        "  avitm.fit(train_data)\n",
        "  avitms.append(avitm)\n",
        "\n",
        "  train_datasets.append(train_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CD0GfefPvJD"
      },
      "source": [
        "# 2.1 Topics at each node"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-X9g0dNGk0bC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "7e8bad87-8d86-4ad4-e443-d8bddd037324"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-2a11e29c-7ee5-44d8-ab8e-109a85f904eb\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>wd677</td>\n",
              "      <td>wd575</td>\n",
              "      <td>wd655</td>\n",
              "      <td>wd878</td>\n",
              "      <td>wd442</td>\n",
              "      <td>wd123</td>\n",
              "      <td>wd486</td>\n",
              "      <td>wd918</td>\n",
              "      <td>wd705</td>\n",
              "      <td>wd199</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>wd67</td>\n",
              "      <td>wd691</td>\n",
              "      <td>wd725</td>\n",
              "      <td>wd219</td>\n",
              "      <td>wd455</td>\n",
              "      <td>wd66</td>\n",
              "      <td>wd653</td>\n",
              "      <td>wd396</td>\n",
              "      <td>wd476</td>\n",
              "      <td>wd183</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>wd725</td>\n",
              "      <td>wd622</td>\n",
              "      <td>wd544</td>\n",
              "      <td>wd447</td>\n",
              "      <td>wd34</td>\n",
              "      <td>wd183</td>\n",
              "      <td>wd990</td>\n",
              "      <td>wd425</td>\n",
              "      <td>wd57</td>\n",
              "      <td>wd354</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>wd203</td>\n",
              "      <td>wd515</td>\n",
              "      <td>wd148</td>\n",
              "      <td>wd513</td>\n",
              "      <td>wd279</td>\n",
              "      <td>wd74</td>\n",
              "      <td>wd491</td>\n",
              "      <td>wd215</td>\n",
              "      <td>wd847</td>\n",
              "      <td>wd856</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>wd265</td>\n",
              "      <td>wd84</td>\n",
              "      <td>wd396</td>\n",
              "      <td>wd326</td>\n",
              "      <td>wd942</td>\n",
              "      <td>wd607</td>\n",
              "      <td>wd66</td>\n",
              "      <td>wd784</td>\n",
              "      <td>wd673</td>\n",
              "      <td>wd988</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>wd0</td>\n",
              "      <td>wd899</td>\n",
              "      <td>wd183</td>\n",
              "      <td>wd790</td>\n",
              "      <td>wd691</td>\n",
              "      <td>wd560</td>\n",
              "      <td>wd612</td>\n",
              "      <td>wd244</td>\n",
              "      <td>wd810</td>\n",
              "      <td>wd824</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>wd82</td>\n",
              "      <td>wd948</td>\n",
              "      <td>wd27</td>\n",
              "      <td>wd131</td>\n",
              "      <td>wd640</td>\n",
              "      <td>wd764</td>\n",
              "      <td>wd812</td>\n",
              "      <td>wd509</td>\n",
              "      <td>wd327</td>\n",
              "      <td>wd18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>wd871</td>\n",
              "      <td>wd381</td>\n",
              "      <td>wd628</td>\n",
              "      <td>wd456</td>\n",
              "      <td>wd174</td>\n",
              "      <td>wd205</td>\n",
              "      <td>wd595</td>\n",
              "      <td>wd937</td>\n",
              "      <td>wd212</td>\n",
              "      <td>wd706</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>wd810</td>\n",
              "      <td>wd186</td>\n",
              "      <td>wd163</td>\n",
              "      <td>wd6</td>\n",
              "      <td>wd659</td>\n",
              "      <td>wd316</td>\n",
              "      <td>wd668</td>\n",
              "      <td>wd537</td>\n",
              "      <td>wd99</td>\n",
              "      <td>wd323</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>wd236</td>\n",
              "      <td>wd61</td>\n",
              "      <td>wd394</td>\n",
              "      <td>wd761</td>\n",
              "      <td>wd562</td>\n",
              "      <td>wd770</td>\n",
              "      <td>wd287</td>\n",
              "      <td>wd887</td>\n",
              "      <td>wd525</td>\n",
              "      <td>wd506</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2a11e29c-7ee5-44d8-ab8e-109a85f904eb')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2a11e29c-7ee5-44d8-ab8e-109a85f904eb button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2a11e29c-7ee5-44d8-ab8e-109a85f904eb');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "       0      1      2      3      4      5      6      7      8      9\n",
              "0  wd677  wd575  wd655  wd878  wd442  wd123  wd486  wd918  wd705  wd199\n",
              "1   wd67  wd691  wd725  wd219  wd455   wd66  wd653  wd396  wd476  wd183\n",
              "2  wd725  wd622  wd544  wd447   wd34  wd183  wd990  wd425   wd57  wd354\n",
              "3  wd203  wd515  wd148  wd513  wd279   wd74  wd491  wd215  wd847  wd856\n",
              "4  wd265   wd84  wd396  wd326  wd942  wd607   wd66  wd784  wd673  wd988\n",
              "5    wd0  wd899  wd183  wd790  wd691  wd560  wd612  wd244  wd810  wd824\n",
              "6   wd82  wd948   wd27  wd131  wd640  wd764  wd812  wd509  wd327   wd18\n",
              "7  wd871  wd381  wd628  wd456  wd174  wd205  wd595  wd937  wd212  wd706\n",
              "8  wd810  wd186  wd163    wd6  wd659  wd316  wd668  wd537   wd99  wd323\n",
              "9  wd236   wd61  wd394  wd761  wd562  wd770  wd287  wd887  wd525  wd506"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "topics_all = []\n",
        "for node in np.arange(n_nodes):\n",
        "  topics = pd.DataFrame(avitm.get_topics(10)).T\n",
        "  topics_all.append(topics)\n",
        "topics_all[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4kezlbmRaRV"
      },
      "source": [
        "## 2.2 Document-topic distributions at each node"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "orpK--hqxQkG"
      },
      "outputs": [],
      "source": [
        "def get_doc_topic_distribution(avitm, dataset, n_samples=20):\n",
        "    avitm.model.eval()\n",
        "\n",
        "    loader = DataLoader(\n",
        "            avitm.train_data, batch_size=avitm.batch_size, shuffle=True,\n",
        "            num_workers=mp.cpu_count())\n",
        "\n",
        "    pbar = tqdm(n_samples, position=0, leave=True)\n",
        "\n",
        "    final_thetas = []\n",
        "    for sample_index in range(n_samples):\n",
        "        with torch.no_grad():\n",
        "            collect_theta = []\n",
        "\n",
        "            for batch_samples in loader:\n",
        "                X = batch_samples['X']\n",
        "\n",
        "                if avitm.USE_CUDA:\n",
        "                  X = X.cuda()\n",
        "\n",
        "                # forward pass\n",
        "                avitm.model.zero_grad()\n",
        "                \n",
        "                with torch.no_grad():\n",
        "                  posterior_mu, posterior_log_sigma = avitm.model.inf_net(X)\n",
        "\n",
        "                  # Generate samples from theta\n",
        "                  theta = F.softmax(\n",
        "                          avitm.model.reparameterize(posterior_mu, posterior_log_sigma), dim=1)\n",
        "                  theta = avitm.model.drop_theta(theta)\n",
        "\n",
        "                collect_theta.extend(theta.cpu().numpy().tolist())\n",
        "\n",
        "            pbar.update(1)\n",
        "            pbar.set_description(\"Sampling: [{}/{}]\".format(sample_index + 1, n_samples))\n",
        "\n",
        "            final_thetas.append(np.array(collect_theta))\n",
        "    pbar.close()\n",
        "    return np.sum(final_thetas, axis=0) / n_samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PXTjce2LlVRG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d75e2cc-61e7-426a-d7c6-ddb4f1e9bd7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Sampling: [5/5]: : 5it [00:00,  6.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document-topic distribution node 0 \n",
            "(1000, 10)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Sampling: [5/5]: : 5it [00:00,  6.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document-topic distribution node 1 \n",
            "(1000, 10)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Sampling: [5/5]: : 5it [00:00,  6.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document-topic distribution node 2 \n",
            "(1000, 10)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Sampling: [5/5]: : 5it [00:00,  7.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document-topic distribution node 3 \n",
            "(1000, 10)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Sampling: [5/5]: : 5it [00:00,  7.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document-topic distribution node 4 \n",
            "(1000, 10)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "doc_topic_all = []\n",
        "for node in np.arange(n_nodes):\n",
        "  doc_topic = get_doc_topic_distribution(avitms[node], train_datasets[node], n_samples=5) # get all the topic predictions\n",
        "  print(\"Document-topic distribution node\", str(node), \"\")\n",
        "  doc_topic_all.append(doc_topic)\n",
        "  print(np.array(doc_topics).shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aEE_kyedRgYv"
      },
      "source": [
        "## 2.3 Word-topic distributions attained at each node"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "douV1llyTM4b"
      },
      "outputs": [],
      "source": [
        "def get_topic_word_distribution(avtim_model):\n",
        "  topic_word_matrix = avtim_model.model.beta.cpu().detach().numpy()\n",
        "  return softmax(topic_word_matrix, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BdM2jxRJUvk3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38840be0-880c-40a4-a1ce-56952a15c98d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['wd1', 'wd2', 'wd3', 'wd4', 'wd5', 'wd6', 'wd7', 'wd8', 'wd9', 'wd10', 'wd11', 'wd12', 'wd13', 'wd14', 'wd15', 'wd16', 'wd17', 'wd18', 'wd19', 'wd20', 'wd21', 'wd22', 'wd23', 'wd24', 'wd25', 'wd26', 'wd27', 'wd28', 'wd29', 'wd30', 'wd31', 'wd32', 'wd33', 'wd34', 'wd35', 'wd36', 'wd37', 'wd38', 'wd39', 'wd40', 'wd41', 'wd42', 'wd43', 'wd44', 'wd45', 'wd46', 'wd47', 'wd48', 'wd49', 'wd50', 'wd51', 'wd52', 'wd53', 'wd54', 'wd55', 'wd56', 'wd57', 'wd58', 'wd59', 'wd60', 'wd61', 'wd62', 'wd63', 'wd64', 'wd65', 'wd66', 'wd67', 'wd68', 'wd69', 'wd70', 'wd71', 'wd72', 'wd73', 'wd74', 'wd75', 'wd76', 'wd77', 'wd78', 'wd79', 'wd80', 'wd81', 'wd82', 'wd83', 'wd84', 'wd85', 'wd86', 'wd87', 'wd88', 'wd89', 'wd90', 'wd91', 'wd92', 'wd93', 'wd94', 'wd95', 'wd96', 'wd97', 'wd98', 'wd99', 'wd100', 'wd101', 'wd102', 'wd103', 'wd104', 'wd105', 'wd106', 'wd107', 'wd108', 'wd109', 'wd110', 'wd111', 'wd112', 'wd113', 'wd114', 'wd115', 'wd116', 'wd117', 'wd118', 'wd119', 'wd120', 'wd121', 'wd122', 'wd123', 'wd124', 'wd125', 'wd126', 'wd127', 'wd128', 'wd129', 'wd130', 'wd131', 'wd132', 'wd133', 'wd134', 'wd135', 'wd136', 'wd137', 'wd138', 'wd139', 'wd140', 'wd141', 'wd142', 'wd143', 'wd144', 'wd145', 'wd146', 'wd147', 'wd148', 'wd149', 'wd150', 'wd151', 'wd152', 'wd153', 'wd154', 'wd155', 'wd156', 'wd157', 'wd158', 'wd159', 'wd160', 'wd161', 'wd162', 'wd163', 'wd164', 'wd165', 'wd166', 'wd167', 'wd168', 'wd169', 'wd170', 'wd171', 'wd172', 'wd173', 'wd174', 'wd175', 'wd176', 'wd177', 'wd178', 'wd179', 'wd180', 'wd181', 'wd182', 'wd183', 'wd184', 'wd185', 'wd186', 'wd187', 'wd188', 'wd189', 'wd190', 'wd191', 'wd192', 'wd193', 'wd194', 'wd195', 'wd196', 'wd197', 'wd198', 'wd199', 'wd200', 'wd201', 'wd202', 'wd203', 'wd204', 'wd205', 'wd206', 'wd207', 'wd208', 'wd209', 'wd210', 'wd211', 'wd212', 'wd213', 'wd214', 'wd215', 'wd216', 'wd217', 'wd218', 'wd219', 'wd220', 'wd221', 'wd222', 'wd223', 'wd224', 'wd225', 'wd226', 'wd227', 'wd228', 'wd229', 'wd230', 'wd231', 'wd232', 'wd233', 'wd234', 'wd235', 'wd236', 'wd237', 'wd238', 'wd239', 'wd240', 'wd241', 'wd242', 'wd243', 'wd244', 'wd245', 'wd246', 'wd247', 'wd248', 'wd249', 'wd250', 'wd251', 'wd252', 'wd253', 'wd254', 'wd255', 'wd256', 'wd257', 'wd258', 'wd259', 'wd260', 'wd261', 'wd262', 'wd263', 'wd264', 'wd265', 'wd266', 'wd267', 'wd268', 'wd269', 'wd270', 'wd271', 'wd272', 'wd273', 'wd274', 'wd275', 'wd276', 'wd277', 'wd278', 'wd279', 'wd280', 'wd281', 'wd282', 'wd283', 'wd284', 'wd285', 'wd286', 'wd287', 'wd288', 'wd289', 'wd290', 'wd291', 'wd292', 'wd293', 'wd294', 'wd295', 'wd296', 'wd297', 'wd298', 'wd299', 'wd300', 'wd301', 'wd302', 'wd303', 'wd304', 'wd305', 'wd306', 'wd307', 'wd308', 'wd309', 'wd310', 'wd311', 'wd312', 'wd313', 'wd314', 'wd315', 'wd316', 'wd317', 'wd318', 'wd319', 'wd320', 'wd321', 'wd322', 'wd323', 'wd324', 'wd325', 'wd326', 'wd327', 'wd328', 'wd329', 'wd330', 'wd331', 'wd332', 'wd333', 'wd334', 'wd335', 'wd336', 'wd337', 'wd338', 'wd339', 'wd340', 'wd341', 'wd342', 'wd343', 'wd344', 'wd345', 'wd346', 'wd347', 'wd348', 'wd349', 'wd350', 'wd351', 'wd352', 'wd353', 'wd354', 'wd355', 'wd356', 'wd357', 'wd358', 'wd359', 'wd360', 'wd361', 'wd362', 'wd363', 'wd364', 'wd365', 'wd366', 'wd367', 'wd368', 'wd369', 'wd370', 'wd371', 'wd372', 'wd373', 'wd374', 'wd375', 'wd376', 'wd377', 'wd378', 'wd379', 'wd380', 'wd381', 'wd382', 'wd383', 'wd384', 'wd385', 'wd386', 'wd387', 'wd388', 'wd389', 'wd390', 'wd391', 'wd392', 'wd393', 'wd394', 'wd395', 'wd396', 'wd397', 'wd398', 'wd399', 'wd400', 'wd401', 'wd402', 'wd403', 'wd404', 'wd405', 'wd406', 'wd407', 'wd408', 'wd409', 'wd410', 'wd411', 'wd412', 'wd413', 'wd414', 'wd415', 'wd416', 'wd417', 'wd418', 'wd419', 'wd420', 'wd421', 'wd422', 'wd423', 'wd424', 'wd425', 'wd426', 'wd427', 'wd428', 'wd429', 'wd430', 'wd431', 'wd432', 'wd433', 'wd434', 'wd435', 'wd436', 'wd437', 'wd438', 'wd439', 'wd440', 'wd441', 'wd442', 'wd443', 'wd444', 'wd445', 'wd446', 'wd447', 'wd448', 'wd449', 'wd450', 'wd451', 'wd452', 'wd453', 'wd454', 'wd455', 'wd456', 'wd457', 'wd458', 'wd459', 'wd460', 'wd461', 'wd462', 'wd463', 'wd464', 'wd465', 'wd466', 'wd467', 'wd468', 'wd469', 'wd470', 'wd471', 'wd472', 'wd473', 'wd474', 'wd475', 'wd476', 'wd477', 'wd478', 'wd479', 'wd480', 'wd481', 'wd482', 'wd483', 'wd484', 'wd485', 'wd486', 'wd487', 'wd488', 'wd489', 'wd490', 'wd491', 'wd492', 'wd493', 'wd494', 'wd495', 'wd496', 'wd497', 'wd498', 'wd499', 'wd500', 'wd501', 'wd502', 'wd503', 'wd504', 'wd505', 'wd506', 'wd507', 'wd508', 'wd509', 'wd510', 'wd511', 'wd512', 'wd513', 'wd514', 'wd515', 'wd516', 'wd517', 'wd518', 'wd519', 'wd520', 'wd521', 'wd522', 'wd523', 'wd524', 'wd525', 'wd526', 'wd527', 'wd528', 'wd529', 'wd530', 'wd531', 'wd532', 'wd533', 'wd534', 'wd535', 'wd536', 'wd537', 'wd538', 'wd539', 'wd540', 'wd541', 'wd542', 'wd543', 'wd544', 'wd545', 'wd546', 'wd547', 'wd548', 'wd549', 'wd550', 'wd551', 'wd552', 'wd553', 'wd554', 'wd555', 'wd556', 'wd557', 'wd558', 'wd559', 'wd560', 'wd561', 'wd562', 'wd563', 'wd564', 'wd565', 'wd566', 'wd567', 'wd568', 'wd569', 'wd570', 'wd571', 'wd572', 'wd573', 'wd574', 'wd575', 'wd576', 'wd577', 'wd578', 'wd579', 'wd580', 'wd581', 'wd582', 'wd583', 'wd584', 'wd585', 'wd586', 'wd587', 'wd588', 'wd589', 'wd590', 'wd591', 'wd592', 'wd593', 'wd594', 'wd595', 'wd596', 'wd597', 'wd598', 'wd599', 'wd600', 'wd601', 'wd602', 'wd603', 'wd604', 'wd605', 'wd606', 'wd607', 'wd608', 'wd609', 'wd610', 'wd611', 'wd612', 'wd613', 'wd614', 'wd615', 'wd616', 'wd617', 'wd618', 'wd619', 'wd620', 'wd621', 'wd622', 'wd623', 'wd624', 'wd625', 'wd626', 'wd627', 'wd628', 'wd629', 'wd630', 'wd631', 'wd632', 'wd633', 'wd634', 'wd635', 'wd636', 'wd637', 'wd638', 'wd639', 'wd640', 'wd641', 'wd642', 'wd643', 'wd644', 'wd645', 'wd646', 'wd647', 'wd648', 'wd649', 'wd650', 'wd651', 'wd652', 'wd653', 'wd654', 'wd655', 'wd656', 'wd657', 'wd658', 'wd659', 'wd660', 'wd661', 'wd662', 'wd663', 'wd664', 'wd665', 'wd666', 'wd667', 'wd668', 'wd669', 'wd670', 'wd671', 'wd672', 'wd673', 'wd674', 'wd675', 'wd676', 'wd677', 'wd678', 'wd679', 'wd680', 'wd681', 'wd682', 'wd683', 'wd684', 'wd685', 'wd686', 'wd687', 'wd688', 'wd689', 'wd690', 'wd691', 'wd692', 'wd693', 'wd694', 'wd695', 'wd696', 'wd697', 'wd698', 'wd699', 'wd700', 'wd701', 'wd702', 'wd703', 'wd704', 'wd705', 'wd706', 'wd707', 'wd708', 'wd709', 'wd710', 'wd711', 'wd712', 'wd713', 'wd714', 'wd715', 'wd716', 'wd717', 'wd718', 'wd719', 'wd720', 'wd721', 'wd722', 'wd723', 'wd724', 'wd725', 'wd726', 'wd727', 'wd728', 'wd729', 'wd730', 'wd731', 'wd732', 'wd733', 'wd734', 'wd735', 'wd736', 'wd737', 'wd738', 'wd739', 'wd740', 'wd741', 'wd742', 'wd743', 'wd744', 'wd745', 'wd746', 'wd747', 'wd748', 'wd749', 'wd750', 'wd751', 'wd752', 'wd753', 'wd754', 'wd755', 'wd756', 'wd757', 'wd758', 'wd759', 'wd760', 'wd761', 'wd762', 'wd763', 'wd764', 'wd765', 'wd766', 'wd767', 'wd768', 'wd769', 'wd770', 'wd771', 'wd772', 'wd773', 'wd774', 'wd775', 'wd776', 'wd777', 'wd778', 'wd779', 'wd780', 'wd781', 'wd782', 'wd783', 'wd784', 'wd785', 'wd786', 'wd787', 'wd788', 'wd789', 'wd790', 'wd791', 'wd792', 'wd793', 'wd794', 'wd795', 'wd796', 'wd797', 'wd798', 'wd799', 'wd800', 'wd801', 'wd802', 'wd803', 'wd804', 'wd805', 'wd806', 'wd807', 'wd808', 'wd809', 'wd810', 'wd811', 'wd812', 'wd813', 'wd814', 'wd815', 'wd816', 'wd817', 'wd818', 'wd819', 'wd820', 'wd821', 'wd822', 'wd823', 'wd824', 'wd825', 'wd826', 'wd827', 'wd828', 'wd829', 'wd830', 'wd831', 'wd832', 'wd833', 'wd834', 'wd835', 'wd836', 'wd837', 'wd838', 'wd839', 'wd840', 'wd841', 'wd842', 'wd843', 'wd844', 'wd845', 'wd846', 'wd847', 'wd848', 'wd849', 'wd850', 'wd851', 'wd852', 'wd853', 'wd854', 'wd855', 'wd856', 'wd857', 'wd858', 'wd859', 'wd860', 'wd861', 'wd862', 'wd863', 'wd864', 'wd865', 'wd866', 'wd867', 'wd868', 'wd869', 'wd870', 'wd871', 'wd872', 'wd873', 'wd874', 'wd875', 'wd876', 'wd877', 'wd878', 'wd879', 'wd880', 'wd881', 'wd882', 'wd883', 'wd884', 'wd885', 'wd886', 'wd887', 'wd888', 'wd889', 'wd890', 'wd891', 'wd892', 'wd893', 'wd894', 'wd895', 'wd896', 'wd897', 'wd898', 'wd899', 'wd900', 'wd901', 'wd902', 'wd903', 'wd904', 'wd905', 'wd906', 'wd907', 'wd908', 'wd909', 'wd910', 'wd911', 'wd912', 'wd913', 'wd914', 'wd915', 'wd916', 'wd917', 'wd918', 'wd919', 'wd920', 'wd921', 'wd922', 'wd923', 'wd924', 'wd925', 'wd926', 'wd927', 'wd928', 'wd929', 'wd930', 'wd931', 'wd932', 'wd933', 'wd934', 'wd935', 'wd936', 'wd937', 'wd938', 'wd939', 'wd940', 'wd941', 'wd942', 'wd943', 'wd944', 'wd945', 'wd946', 'wd947', 'wd948', 'wd949', 'wd950', 'wd951', 'wd952', 'wd953', 'wd954', 'wd955', 'wd956', 'wd957', 'wd958', 'wd959', 'wd960', 'wd961', 'wd962', 'wd963', 'wd964', 'wd965', 'wd966', 'wd967', 'wd968', 'wd969', 'wd970', 'wd971', 'wd972', 'wd973', 'wd974', 'wd975', 'wd976', 'wd977', 'wd978', 'wd979', 'wd980', 'wd981', 'wd982', 'wd983', 'wd984', 'wd985', 'wd986', 'wd987', 'wd988', 'wd989', 'wd990', 'wd991', 'wd992', 'wd993', 'wd994', 'wd995', 'wd996', 'wd997', 'wd998', 'wd999', 'wd1000']\n"
          ]
        }
      ],
      "source": [
        "all_words = []\n",
        "for word in np.arange(vocab_size+1):\n",
        "  if word > 0:\n",
        "    all_words.append('wd'+str(word))\n",
        "print(all_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MJfQe7PdUmkC"
      },
      "outputs": [],
      "source": [
        "topic_word_all = []\n",
        "for node in np.arange(n_nodes):\n",
        "  w_t_distrib = np.zeros((10,vocab_size), dtype=np.float64) \n",
        "  wd = get_topic_word_distribution(avitms[node])\n",
        "  for i in np.arange(10):\n",
        "    for idx, word in id2tokens[node].items():\n",
        "      for j in np.arange(len(all_words)):\n",
        "        if all_words[j] == word:\n",
        "          w_t_distrib[i,j] = wd[i][idx]\n",
        "          break\n",
        "  sum_of_rows = w_t_distrib.sum(axis=1)\n",
        "  normalized_array = w_t_distrib / sum_of_rows[:, np.newaxis]\n",
        "  topic_word_all.append(normalized_array)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QieP5DdU7zY0"
      },
      "source": [
        "# 3. Centralized model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wTQrxfRpRrD1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5f8a06b-c96d-4ce6-9cd6-ac6bb7a4d3e7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5000"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "documents_centr = [*documents_all[0], *documents_all[1], *documents_all[2], *documents_all[3], *documents_all[4]]\n",
        "len(documents_centr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kxW_tMtFRyfX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bda99ca1-8c63-44a0-d417-8c0929ce48a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Settings: \n",
            "               N Components: 10\n",
            "               Topic Prior Mean: 0.0\n",
            "               Topic Prior Variance: 0.9\n",
            "               Model Type: prodLDA\n",
            "               Hidden Sizes: (100, 100)\n",
            "               Activation: softplus\n",
            "               Dropout: 0.2\n",
            "               Learn Priors: True\n",
            "               Learning Rate: 0.002\n",
            "               Momentum: 0.99\n",
            "               Reduce On Plateau: False\n",
            "               Save Dir: None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [1/100]\tSamples: [5000/500000]\tTrain Loss: 2168.373964453125\tTime: 0:00:00.766800\n",
            "Epoch: [2/100]\tSamples: [10000/500000]\tTrain Loss: 2139.284569921875\tTime: 0:00:00.789064\n",
            "Epoch: [3/100]\tSamples: [15000/500000]\tTrain Loss: 2119.59677265625\tTime: 0:00:00.788946\n",
            "Epoch: [4/100]\tSamples: [20000/500000]\tTrain Loss: 2104.879643359375\tTime: 0:00:00.777195\n",
            "Epoch: [5/100]\tSamples: [25000/500000]\tTrain Loss: 2089.17069296875\tTime: 0:00:00.716656\n",
            "Epoch: [6/100]\tSamples: [30000/500000]\tTrain Loss: 2076.245284375\tTime: 0:00:00.739070\n",
            "Epoch: [7/100]\tSamples: [35000/500000]\tTrain Loss: 2068.176354296875\tTime: 0:00:00.781246\n",
            "Epoch: [8/100]\tSamples: [40000/500000]\tTrain Loss: 2063.36745703125\tTime: 0:00:00.782577\n",
            "Epoch: [9/100]\tSamples: [45000/500000]\tTrain Loss: 2060.1215228515625\tTime: 0:00:00.746390\n",
            "Epoch: [10/100]\tSamples: [50000/500000]\tTrain Loss: 2058.096546484375\tTime: 0:00:00.759749\n",
            "Epoch: [11/100]\tSamples: [55000/500000]\tTrain Loss: 2056.7045859375\tTime: 0:00:00.735178\n",
            "Epoch: [12/100]\tSamples: [60000/500000]\tTrain Loss: 2055.571068359375\tTime: 0:00:00.709057\n",
            "Epoch: [13/100]\tSamples: [65000/500000]\tTrain Loss: 2054.460523828125\tTime: 0:00:00.729031\n",
            "Epoch: [14/100]\tSamples: [70000/500000]\tTrain Loss: 2053.678829296875\tTime: 0:00:00.750236\n",
            "Epoch: [15/100]\tSamples: [75000/500000]\tTrain Loss: 2053.0091328125\tTime: 0:00:00.717409\n",
            "Epoch: [16/100]\tSamples: [80000/500000]\tTrain Loss: 2052.16049921875\tTime: 0:00:00.744736\n",
            "Epoch: [17/100]\tSamples: [85000/500000]\tTrain Loss: 2052.081953125\tTime: 0:00:00.733058\n",
            "Epoch: [18/100]\tSamples: [90000/500000]\tTrain Loss: 2051.61953203125\tTime: 0:00:00.770802\n",
            "Epoch: [19/100]\tSamples: [95000/500000]\tTrain Loss: 2051.446196484375\tTime: 0:00:00.737370\n",
            "Epoch: [20/100]\tSamples: [100000/500000]\tTrain Loss: 2051.5038296875\tTime: 0:00:00.744363\n",
            "Epoch: [21/100]\tSamples: [105000/500000]\tTrain Loss: 2051.605473046875\tTime: 0:00:00.734898\n",
            "Epoch: [22/100]\tSamples: [110000/500000]\tTrain Loss: 2051.344008203125\tTime: 0:00:00.766446\n",
            "Epoch: [23/100]\tSamples: [115000/500000]\tTrain Loss: 2051.147664453125\tTime: 0:00:00.738131\n",
            "Epoch: [24/100]\tSamples: [120000/500000]\tTrain Loss: 2051.047653515625\tTime: 0:00:00.752904\n",
            "Epoch: [25/100]\tSamples: [125000/500000]\tTrain Loss: 2050.94858203125\tTime: 0:00:00.767549\n",
            "Epoch: [26/100]\tSamples: [130000/500000]\tTrain Loss: 2051.096448046875\tTime: 0:00:00.742867\n",
            "Epoch: [27/100]\tSamples: [135000/500000]\tTrain Loss: 2050.96179921875\tTime: 0:00:00.743753\n",
            "Epoch: [28/100]\tSamples: [140000/500000]\tTrain Loss: 2051.0483451171876\tTime: 0:00:00.738093\n",
            "Epoch: [29/100]\tSamples: [145000/500000]\tTrain Loss: 2050.99377109375\tTime: 0:00:00.752022\n",
            "Epoch: [30/100]\tSamples: [150000/500000]\tTrain Loss: 2050.8558345703127\tTime: 0:00:00.749817\n",
            "Epoch: [31/100]\tSamples: [155000/500000]\tTrain Loss: 2050.6801953125\tTime: 0:00:00.736094\n",
            "Epoch: [32/100]\tSamples: [160000/500000]\tTrain Loss: 2051.14831796875\tTime: 0:00:00.753376\n",
            "Epoch: [33/100]\tSamples: [165000/500000]\tTrain Loss: 2050.766305859375\tTime: 0:00:00.749946\n",
            "Epoch: [34/100]\tSamples: [170000/500000]\tTrain Loss: 2050.805329296875\tTime: 0:00:00.741088\n",
            "Epoch: [35/100]\tSamples: [175000/500000]\tTrain Loss: 2050.52093203125\tTime: 0:00:00.731888\n",
            "Epoch: [36/100]\tSamples: [180000/500000]\tTrain Loss: 2050.75412734375\tTime: 0:00:00.743477\n",
            "Epoch: [37/100]\tSamples: [185000/500000]\tTrain Loss: 2050.67845\tTime: 0:00:00.734365\n",
            "Epoch: [38/100]\tSamples: [190000/500000]\tTrain Loss: 2050.3397296875\tTime: 0:00:00.750911\n",
            "Epoch: [39/100]\tSamples: [195000/500000]\tTrain Loss: 2050.72633828125\tTime: 0:00:00.750825\n",
            "Epoch: [40/100]\tSamples: [200000/500000]\tTrain Loss: 2050.248483203125\tTime: 0:00:00.744951\n",
            "Epoch: [41/100]\tSamples: [205000/500000]\tTrain Loss: 2050.5249080078124\tTime: 0:00:00.731109\n",
            "Epoch: [42/100]\tSamples: [210000/500000]\tTrain Loss: 2050.6572203125\tTime: 0:00:00.734997\n",
            "Epoch: [43/100]\tSamples: [215000/500000]\tTrain Loss: 2050.585973046875\tTime: 0:00:00.750062\n",
            "Epoch: [44/100]\tSamples: [220000/500000]\tTrain Loss: 2050.484326953125\tTime: 0:00:00.770882\n",
            "Epoch: [45/100]\tSamples: [225000/500000]\tTrain Loss: 2050.5758859375\tTime: 0:00:00.737902\n",
            "Epoch: [46/100]\tSamples: [230000/500000]\tTrain Loss: 2050.5079759765626\tTime: 0:00:00.757395\n",
            "Epoch: [47/100]\tSamples: [235000/500000]\tTrain Loss: 2050.854956640625\tTime: 0:00:00.791233\n",
            "Epoch: [48/100]\tSamples: [240000/500000]\tTrain Loss: 2050.705470703125\tTime: 0:00:00.752990\n",
            "Epoch: [49/100]\tSamples: [245000/500000]\tTrain Loss: 2050.62050546875\tTime: 0:00:00.792571\n",
            "Epoch: [50/100]\tSamples: [250000/500000]\tTrain Loss: 2050.60049296875\tTime: 0:00:00.722499\n",
            "Epoch: [51/100]\tSamples: [255000/500000]\tTrain Loss: 2050.77448671875\tTime: 0:00:00.716596\n",
            "Epoch: [52/100]\tSamples: [260000/500000]\tTrain Loss: 2050.589387109375\tTime: 0:00:00.729952\n",
            "Epoch: [53/100]\tSamples: [265000/500000]\tTrain Loss: 2050.343099609375\tTime: 0:00:00.743295\n",
            "Epoch: [54/100]\tSamples: [270000/500000]\tTrain Loss: 2050.31529609375\tTime: 0:00:00.724054\n",
            "Epoch: [55/100]\tSamples: [275000/500000]\tTrain Loss: 2050.5993673828125\tTime: 0:00:00.728477\n",
            "Epoch: [56/100]\tSamples: [280000/500000]\tTrain Loss: 2050.6000984375\tTime: 0:00:00.734253\n",
            "Epoch: [57/100]\tSamples: [285000/500000]\tTrain Loss: 2050.46583515625\tTime: 0:00:00.728111\n",
            "Epoch: [58/100]\tSamples: [290000/500000]\tTrain Loss: 2050.30716171875\tTime: 0:00:00.752808\n",
            "Epoch: [59/100]\tSamples: [295000/500000]\tTrain Loss: 2050.43736796875\tTime: 0:00:00.765619\n",
            "Epoch: [60/100]\tSamples: [300000/500000]\tTrain Loss: 2050.38823203125\tTime: 0:00:00.731789\n",
            "Epoch: [61/100]\tSamples: [305000/500000]\tTrain Loss: 2049.98037890625\tTime: 0:00:00.749904\n",
            "Epoch: [62/100]\tSamples: [310000/500000]\tTrain Loss: 2050.633331640625\tTime: 0:00:00.788074\n",
            "Epoch: [63/100]\tSamples: [315000/500000]\tTrain Loss: 2050.495973046875\tTime: 0:00:00.785094\n",
            "Epoch: [64/100]\tSamples: [320000/500000]\tTrain Loss: 2050.271276953125\tTime: 0:00:00.746979\n",
            "Epoch: [65/100]\tSamples: [325000/500000]\tTrain Loss: 2050.6162271484377\tTime: 0:00:00.752007\n",
            "Epoch: [66/100]\tSamples: [330000/500000]\tTrain Loss: 2050.101605859375\tTime: 0:00:00.747796\n",
            "Epoch: [67/100]\tSamples: [335000/500000]\tTrain Loss: 2050.3980015625\tTime: 0:00:00.778594\n",
            "Epoch: [68/100]\tSamples: [340000/500000]\tTrain Loss: 2050.0646984375\tTime: 0:00:00.745818\n",
            "Epoch: [69/100]\tSamples: [345000/500000]\tTrain Loss: 2050.100326953125\tTime: 0:00:00.757422\n",
            "Epoch: [70/100]\tSamples: [350000/500000]\tTrain Loss: 2050.643171484375\tTime: 0:00:00.746915\n",
            "Epoch: [71/100]\tSamples: [355000/500000]\tTrain Loss: 2050.569469921875\tTime: 0:00:00.723627\n",
            "Epoch: [72/100]\tSamples: [360000/500000]\tTrain Loss: 2050.210526171875\tTime: 0:00:00.729370\n",
            "Epoch: [73/100]\tSamples: [365000/500000]\tTrain Loss: 2050.327583203125\tTime: 0:00:00.765427\n",
            "Epoch: [74/100]\tSamples: [370000/500000]\tTrain Loss: 2050.38530859375\tTime: 0:00:00.823023\n",
            "Epoch: [75/100]\tSamples: [375000/500000]\tTrain Loss: 2050.680272265625\tTime: 0:00:00.869977\n",
            "Epoch: [76/100]\tSamples: [380000/500000]\tTrain Loss: 2050.3868818359374\tTime: 0:00:00.737480\n",
            "Epoch: [77/100]\tSamples: [385000/500000]\tTrain Loss: 2050.273221875\tTime: 0:00:00.774699\n",
            "Epoch: [78/100]\tSamples: [390000/500000]\tTrain Loss: 2050.3784126953124\tTime: 0:00:00.826905\n",
            "Epoch: [79/100]\tSamples: [395000/500000]\tTrain Loss: 2050.14082734375\tTime: 0:00:00.893354\n",
            "Epoch: [80/100]\tSamples: [400000/500000]\tTrain Loss: 2050.29081953125\tTime: 0:00:00.781817\n",
            "Epoch: [81/100]\tSamples: [405000/500000]\tTrain Loss: 2050.02037421875\tTime: 0:00:00.750505\n",
            "Epoch: [82/100]\tSamples: [410000/500000]\tTrain Loss: 2050.305112109375\tTime: 0:00:00.753034\n",
            "Epoch: [83/100]\tSamples: [415000/500000]\tTrain Loss: 2050.192476171875\tTime: 0:00:00.774657\n",
            "Epoch: [84/100]\tSamples: [420000/500000]\tTrain Loss: 2050.089608984375\tTime: 0:00:00.761836\n",
            "Epoch: [85/100]\tSamples: [425000/500000]\tTrain Loss: 2049.9529369140623\tTime: 0:00:00.772064\n",
            "Epoch: [86/100]\tSamples: [430000/500000]\tTrain Loss: 2049.88930390625\tTime: 0:00:00.757603\n",
            "Epoch: [87/100]\tSamples: [435000/500000]\tTrain Loss: 2050.08393046875\tTime: 0:00:00.775928\n",
            "Epoch: [88/100]\tSamples: [440000/500000]\tTrain Loss: 2050.04598203125\tTime: 0:00:00.798151\n",
            "Epoch: [89/100]\tSamples: [445000/500000]\tTrain Loss: 2049.89900234375\tTime: 0:00:00.771183\n",
            "Epoch: [90/100]\tSamples: [450000/500000]\tTrain Loss: 2049.9949328125\tTime: 0:00:00.795903\n",
            "Epoch: [91/100]\tSamples: [455000/500000]\tTrain Loss: 2050.249453125\tTime: 0:00:00.760004\n",
            "Epoch: [92/100]\tSamples: [460000/500000]\tTrain Loss: 2050.340819921875\tTime: 0:00:00.774271\n",
            "Epoch: [93/100]\tSamples: [465000/500000]\tTrain Loss: 2050.0920125\tTime: 0:00:00.744891\n",
            "Epoch: [94/100]\tSamples: [470000/500000]\tTrain Loss: 2049.98515\tTime: 0:00:00.741495\n",
            "Epoch: [95/100]\tSamples: [475000/500000]\tTrain Loss: 2049.873890625\tTime: 0:00:00.772374\n",
            "Epoch: [96/100]\tSamples: [480000/500000]\tTrain Loss: 2049.97223671875\tTime: 0:00:00.760753\n",
            "Epoch: [97/100]\tSamples: [485000/500000]\tTrain Loss: 2049.883440625\tTime: 0:00:00.739551\n",
            "Epoch: [98/100]\tSamples: [490000/500000]\tTrain Loss: 2050.240855078125\tTime: 0:00:00.741822\n",
            "Epoch: [99/100]\tSamples: [495000/500000]\tTrain Loss: 2050.11234609375\tTime: 0:00:00.755676\n",
            "Epoch: [100/100]\tSamples: [500000/500000]\tTrain Loss: 2050.21869453125\tTime: 0:00:00.721022\n"
          ]
        }
      ],
      "source": [
        "cv = CountVectorizer(input='content', lowercase=True, stop_words='english',\n",
        "                      max_df=0.99, min_df=0.01, binary=False)\n",
        "  \n",
        "docs_centr = [\" \".join(documents_centr[i]) for i in np.arange(len(documents_centr))]\n",
        "\n",
        "train_bow_centr = cv.fit_transform(docs_centr)\n",
        "train_bow_centr = train_bow_centr.toarray()\n",
        "\n",
        "idx2token_centr = cv.get_feature_names()\n",
        "input_size_centr = len(idx2token_centr)\n",
        "\n",
        "id2token_centr = {k: v for k, v in zip(range(0, len(idx2token_centr)), idx2token_centr)}\n",
        "\n",
        "train_data_centr = BOWDataset(train_bow_centr, idx2token_centr)\n",
        "\n",
        "avitm_centr = AVITM(input_size=input_size_centr, n_components=10, model_type='prodLDA',\n",
        "              hidden_sizes=(100, 100), activation='softplus', dropout=0.2,\n",
        "              learn_priors=True, batch_size=64, lr=2e-3, momentum=0.99,\n",
        "              solver='adam', num_epochs=100, reduce_on_plateau=False)\n",
        "\n",
        "avitm_centr.fit(train_data_centr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A3_xM2LUSYAN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "6445d265-c94e-4a28-967d-ba379525460f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-025d189d-b3d3-496e-81b4-d39c95228f02\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>wd184</td>\n",
              "      <td>wd683</td>\n",
              "      <td>wd408</td>\n",
              "      <td>wd917</td>\n",
              "      <td>wd97</td>\n",
              "      <td>wd48</td>\n",
              "      <td>wd673</td>\n",
              "      <td>wd745</td>\n",
              "      <td>wd303</td>\n",
              "      <td>wd35</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>wd148</td>\n",
              "      <td>wd255</td>\n",
              "      <td>wd515</td>\n",
              "      <td>wd491</td>\n",
              "      <td>wd71</td>\n",
              "      <td>wd203</td>\n",
              "      <td>wd856</td>\n",
              "      <td>wd839</td>\n",
              "      <td>wd847</td>\n",
              "      <td>wd235</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>wd186</td>\n",
              "      <td>wd659</td>\n",
              "      <td>wd668</td>\n",
              "      <td>wd790</td>\n",
              "      <td>wd810</td>\n",
              "      <td>wd632</td>\n",
              "      <td>wd6</td>\n",
              "      <td>wd163</td>\n",
              "      <td>wd475</td>\n",
              "      <td>wd89</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>wd812</td>\n",
              "      <td>wd509</td>\n",
              "      <td>wd948</td>\n",
              "      <td>wd327</td>\n",
              "      <td>wd834</td>\n",
              "      <td>wd27</td>\n",
              "      <td>wd640</td>\n",
              "      <td>wd183</td>\n",
              "      <td>wd131</td>\n",
              "      <td>wd577</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>wd182</td>\n",
              "      <td>wd815</td>\n",
              "      <td>wd988</td>\n",
              "      <td>wd875</td>\n",
              "      <td>wd523</td>\n",
              "      <td>wd704</td>\n",
              "      <td>wd197</td>\n",
              "      <td>wd460</td>\n",
              "      <td>wd953</td>\n",
              "      <td>wd692</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>wd371</td>\n",
              "      <td>wd991</td>\n",
              "      <td>wd203</td>\n",
              "      <td>wd436</td>\n",
              "      <td>wd395</td>\n",
              "      <td>wd311</td>\n",
              "      <td>wd199</td>\n",
              "      <td>wd151</td>\n",
              "      <td>wd892</td>\n",
              "      <td>wd746</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>wd677</td>\n",
              "      <td>wd575</td>\n",
              "      <td>wd655</td>\n",
              "      <td>wd878</td>\n",
              "      <td>wd763</td>\n",
              "      <td>wd979</td>\n",
              "      <td>wd442</td>\n",
              "      <td>wd199</td>\n",
              "      <td>wd486</td>\n",
              "      <td>wd618</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>wd386</td>\n",
              "      <td>wd907</td>\n",
              "      <td>wd382</td>\n",
              "      <td>wd543</td>\n",
              "      <td>wd154</td>\n",
              "      <td>wd215</td>\n",
              "      <td>wd764</td>\n",
              "      <td>wd605</td>\n",
              "      <td>wd866</td>\n",
              "      <td>wd588</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>wd770</td>\n",
              "      <td>wd236</td>\n",
              "      <td>wd525</td>\n",
              "      <td>wd671</td>\n",
              "      <td>wd394</td>\n",
              "      <td>wd61</td>\n",
              "      <td>wd761</td>\n",
              "      <td>wd59</td>\n",
              "      <td>wd522</td>\n",
              "      <td>wd670</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>wd307</td>\n",
              "      <td>wd724</td>\n",
              "      <td>wd619</td>\n",
              "      <td>wd667</td>\n",
              "      <td>wd837</td>\n",
              "      <td>wd278</td>\n",
              "      <td>wd926</td>\n",
              "      <td>wd373</td>\n",
              "      <td>wd7</td>\n",
              "      <td>wd332</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-025d189d-b3d3-496e-81b4-d39c95228f02')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-025d189d-b3d3-496e-81b4-d39c95228f02 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-025d189d-b3d3-496e-81b4-d39c95228f02');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "       0      1      2      3      4      5      6      7      8      9\n",
              "0  wd184  wd683  wd408  wd917   wd97   wd48  wd673  wd745  wd303   wd35\n",
              "1  wd148  wd255  wd515  wd491   wd71  wd203  wd856  wd839  wd847  wd235\n",
              "2  wd186  wd659  wd668  wd790  wd810  wd632    wd6  wd163  wd475   wd89\n",
              "3  wd812  wd509  wd948  wd327  wd834   wd27  wd640  wd183  wd131  wd577\n",
              "4  wd182  wd815  wd988  wd875  wd523  wd704  wd197  wd460  wd953  wd692\n",
              "5  wd371  wd991  wd203  wd436  wd395  wd311  wd199  wd151  wd892  wd746\n",
              "6  wd677  wd575  wd655  wd878  wd763  wd979  wd442  wd199  wd486  wd618\n",
              "7  wd386  wd907  wd382  wd543  wd154  wd215  wd764  wd605  wd866  wd588\n",
              "8  wd770  wd236  wd525  wd671  wd394   wd61  wd761   wd59  wd522  wd670\n",
              "9  wd307  wd724  wd619  wd667  wd837  wd278  wd926  wd373    wd7  wd332"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "topics_centr = pd.DataFrame(avitm_centr.get_topics(10)).T\n",
        "topics_centr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z1w7cb9r7QAl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92c1d3e2-3d99-4675-a7a9-e5bf33b81724"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Sampling: [5/5]: : 5it [00:01,  2.79it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5000, 10)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "doc_topic_centr = get_doc_topic_distribution(avitm_centr, train_data_centr, n_samples=5) # get all the topic predictions\n",
        "print(doc_topic_centr.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YHry56Bz7sbV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0172996a-cbc3-48c4-83f7-33c5f2ebd79a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.00100477 0.0010237  0.00102813 ... 0.00108335 0.00104689 0.        ]\n",
            " [0.00096004 0.00100935 0.00096311 ... 0.00091989 0.00094781 0.        ]\n",
            " [0.00094721 0.00098949 0.00101394 ... 0.00086728 0.00101072 0.        ]\n",
            " ...\n",
            " [0.00098012 0.00092885 0.00103538 ... 0.00080422 0.00097879 0.        ]\n",
            " [0.00113785 0.00109086 0.0010497  ... 0.00101823 0.0009678  0.        ]\n",
            " [0.00099336 0.00095731 0.00104003 ... 0.00106424 0.00110662 0.        ]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9999999999999999"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "w_t_distrib_centr = np.zeros((10,vocab_size), dtype=np.float64) # vocab_size = 10000\n",
        "wd = get_topic_word_distribution(avitm_centr)\n",
        "for i in np.arange(10):\n",
        "  for idx, word in id2token_centr.items():\n",
        "    for j in np.arange(len(all_words)):\n",
        "      if all_words[j] == word:\n",
        "        w_t_distrib_centr[i,j] = wd[i][idx]\n",
        "        break\n",
        "sum_of_rows = w_t_distrib_centr.sum(axis=1)\n",
        "w_t_distrib_centr_norm = w_t_distrib_centr / sum_of_rows[:, np.newaxis]\n",
        "print(w_t_distrib_centr_norm)\n",
        "sum(w_t_distrib_centr_norm[8,:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usiERbR-8Roe"
      },
      "source": [
        "# 4. Get similarity through Frobenius"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ng1ykc1A9wkn"
      },
      "outputs": [],
      "source": [
        "doc_topic_centr_all = []\n",
        "doc_topic_centr_all.append(doc_topic_centr[0:1000,:])\n",
        "doc_topic_centr_all.append(doc_topic_centr[1000:2000,:])\n",
        "doc_topic_centr_all.append(doc_topic_centr[2000:3000,:])\n",
        "doc_topic_centr_all.append(doc_topic_centr[3000:4000,:])\n",
        "doc_topic_centr_all.append(doc_topic_centr[4000:5000,:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-NR5xZgQ8S8W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8e66920-10bc-4c91-ef90-90e7c5c621c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NODE 0\n",
            "GT vs inferred in node: 322.53214800892107\n",
            "GT vs centralized in node 325.8532927053037\n",
            "***************************************************************\n",
            "NODE 1\n",
            "GT vs inferred in node: 322.1198500918354\n",
            "GT vs centralized in node 325.8684822487592\n",
            "***************************************************************\n",
            "NODE 2\n",
            "GT vs inferred in node: 328.594896978386\n",
            "GT vs centralized in node 328.8623284920214\n",
            "***************************************************************\n",
            "NODE 3\n",
            "GT vs inferred in node: 326.75718166121004\n",
            "GT vs centralized in node 326.79522793289897\n",
            "***************************************************************\n",
            "NODE 4\n",
            "GT vs inferred in node: 326.5825994106268\n",
            "GT vs centralized in node 326.61481470650057\n",
            "***************************************************************\n"
          ]
        }
      ],
      "source": [
        "for node in np.arange(n_nodes):\n",
        "  # Ground truth in node vs inferred in node\n",
        "  doc_topics_avitm_sqrt_node = np.sqrt(doc_topic_all[node])\n",
        "  similarity_avitm_node = doc_topics_avitm_sqrt_node.dot(doc_topics_avitm_sqrt_node.T)\n",
        "\n",
        "  doc_topics_gt_sqrt_node = np.sqrt(doc_topics_all_gt[node])\n",
        "  similarity_gt = doc_topics_gt_sqrt_node.dot(doc_topics_gt_sqrt_node.T)\n",
        "\n",
        "  diff_sims = similarity_avitm_node - similarity_gt\n",
        "  frobenius_diff_sims_node = np.linalg.norm(diff_sims,'fro')\n",
        "\n",
        "  # Ground truth in node vs centralized (for documents of such a node)\n",
        "  doc_topics_avitm_sqrt_centr_node = np.sqrt(doc_topic_centr_all[node])\n",
        "  similarity_avitm_centr = doc_topics_avitm_sqrt_centr_node.dot(doc_topics_avitm_sqrt_centr_node.T)\n",
        "\n",
        "  diff_sims = similarity_avitm_centr - similarity_gt\n",
        "  frobenius_diff_sims_avg = np.linalg.norm(diff_sims,'fro')\n",
        "\n",
        "  print(\"NODE\", str(node))\n",
        "  print(\"GT vs inferred in node:\", frobenius_diff_sims_node)\n",
        "  print(\"GT vs centralized in node\", frobenius_diff_sims_avg)\n",
        "  print(\"***************************************************************\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uQ6YfIbw-72y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3de608a2-3d48-4af4-f9a5-1389c26a45d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NODE 0\n",
            "Original vs inferred in node sum max row: 8.867762436247403\n",
            "***************************************************************\n",
            "NODE 1\n",
            "Original vs inferred in node sum max row: 8.867790853292146\n",
            "***************************************************************\n",
            "NODE 2\n",
            "Original vs inferred in node sum max row: 8.866969091465988\n",
            "***************************************************************\n",
            "NODE 3\n",
            "Original vs inferred in node sum max row: 8.867964958631385\n",
            "***************************************************************\n",
            "NODE 4\n",
            "Original vs inferred in node sum max row: 8.867697366762412\n",
            "***************************************************************\n",
            "CENTRALIZED\n",
            "Original vs avg of inferred in nodes sum max row 8.86696569730843\n"
          ]
        }
      ],
      "source": [
        "for node in np.arange(n_nodes):\n",
        "  # GT vs inferred in node\n",
        "  topic_words_gt_sqrt = np.sqrt(topic_vectors)\n",
        "  topic_words_avtim_node_sqrt = np.sqrt(topic_word_all[node])\n",
        "  simmat_t_w = topic_words_gt_sqrt.dot(topic_words_avtim_node_sqrt.T)\n",
        "\n",
        "  simmat_t_w_pd = pd.DataFrame(simmat_t_w)\n",
        "  maxValues_rows = simmat_t_w_pd.max(axis = 1)\n",
        "  max_values_rows_sum = maxValues_rows.sum()\n",
        "\n",
        "  print(\"NODE\", str(node))\n",
        "  print(\"Original vs inferred in node sum max row:\", max_values_rows_sum)\n",
        "  print(\"***************************************************************\")\n",
        "\n",
        "# GT vs centralized\n",
        "topic_words_avtim_centr_sqrt = np.sqrt(w_t_distrib_centr_norm)\n",
        "simmat_t_w = topic_words_gt_sqrt.dot(topic_words_avtim_centr_sqrt.T)\n",
        "\n",
        "simmat_t_w_pd = pd.DataFrame(simmat_t_w)\n",
        "maxValues_rows = simmat_t_w_pd.max(axis = 1)\n",
        "max_values_rows_sum_centr = maxValues_rows.sum()\n",
        "\n",
        "print(\"CENTRALIZED\")\n",
        "print(\"Original vs avg of inferred in nodes sum max row\", max_values_rows_sum_centr)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.imshow(simmat_t_w)\n",
        "plt.colorbar()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "f62uaxhLKIOF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "fb3a6025-b078-4902-a523-574d5ba0aa6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Centralized-ProdLDA-vocab-1000_beta-1_prior-0.4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}