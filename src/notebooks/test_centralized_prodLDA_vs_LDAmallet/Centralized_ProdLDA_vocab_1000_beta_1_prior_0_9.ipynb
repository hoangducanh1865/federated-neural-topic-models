{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sib1HSks6Xqv"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from scipy.special import softmax\n",
        "import multiprocessing as mp\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "from sklearn.preprocessing import normalize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CObHMSd6LLz"
      },
      "source": [
        "# Installing ProdLDA\n",
        "**Restart notbook after the installation!!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-VDAzxJA6FK5",
        "outputId": "d2b75766-1b91-4329-a8ef-c047053da614"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'PyTorchAVITM'...\n",
            "remote: Enumerating objects: 19052, done.\u001b[K\n",
            "remote: Total 19052 (delta 0), reused 0 (delta 0), pack-reused 19052\u001b[K\n",
            "Receiving objects: 100% (19052/19052), 132.62 MiB | 24.69 MiB/s, done.\n",
            "Resolving deltas: 100% (89/89), done.\n",
            "Checking out files: 100% (37/37), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/estebandito22/PyTorchAVITM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6kW5jO66UKj"
      },
      "source": [
        "# 1. Creation of synthetic corpus\n",
        "\n",
        "We consider a scenario with n parties, each of them as an associated corpus.\n",
        "To generate the corpus associated with each of the parties, we consider a common beta distribution (word-topic distribution), but we freeze different topics/ assign different asymmetric Dirichlet priors favoring different topics at the time of generating the document that composes each party's corpus."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twSZ3G0p6d1z"
      },
      "source": [
        "## 1.1. Function for permuting the Dirichlet prior at each node"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tXdkpdrh6Thn"
      },
      "outputs": [],
      "source": [
        "def rotateArray(arr, n, d):\n",
        "    temp = []\n",
        "    i = 0\n",
        "    while (i < d):\n",
        "        temp.append(arr[i])\n",
        "        i = i + 1\n",
        "    i = 0\n",
        "    while (d < n):\n",
        "        arr[i] = arr[d]\n",
        "        i = i + 1\n",
        "        d = d + 1\n",
        "    arr[:] = arr[: i] + temp\n",
        "    return arr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AyFA9eGH6hGH"
      },
      "source": [
        "## 1.2. Topic modeling and node settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5DmfSiuR6iI0",
        "outputId": "e319514c-7e5d-4125-e106-09f87a2172bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.1, 0.1, 0.1, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9]\n"
          ]
        }
      ],
      "source": [
        "# Topic modeling settings\n",
        "vocab_size = 1000\n",
        "n_topics = 10\n",
        "beta = 1\n",
        "alpha = 5/n_topics\n",
        "n_docs = 1000\n",
        "nwords = (150, 450) #Min and max lengths of the documents\n",
        "\n",
        "# Nodes settings\n",
        "n_nodes = 5\n",
        "frozen_topics = 3\n",
        "dirichlet_symmetric = False\n",
        "prior = (n_topics)*[0.9]\n",
        "prior[0] = prior[1] = prior[2] = 0.1\n",
        "print(prior)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ylo9Vsu6zpX"
      },
      "source": [
        "## 1.3. Topics generation (common for all nodes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uy3AuOSx6qc1",
        "outputId": "b6a34abf-2ab5-4c30-d1da-21fd6f4437af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ordered probabilities for the first topic vector:\n",
            "[6.05274935e-03 5.95187337e-03 5.59015357e-03 5.30620192e-03\n",
            " 5.26211126e-03 5.01229109e-03 4.76557969e-03 4.61905350e-03\n",
            " 4.57461338e-03 4.55464531e-03 4.52191321e-03 4.45583587e-03\n",
            " 4.44971062e-03 4.42328101e-03 4.38276496e-03 4.32861387e-03\n",
            " 4.30294407e-03 4.24317716e-03 4.14489083e-03 3.99361411e-03\n",
            " 3.94348904e-03 3.93922048e-03 3.92918021e-03 3.92417206e-03\n",
            " 3.88352350e-03 3.84521627e-03 3.70964873e-03 3.68576190e-03\n",
            " 3.67424604e-03 3.55057351e-03 3.52482371e-03 3.51942851e-03\n",
            " 3.45054395e-03 3.43302863e-03 3.36493880e-03 3.33464883e-03\n",
            " 3.31799126e-03 3.29222279e-03 3.27522691e-03 3.27310998e-03\n",
            " 3.24727357e-03 3.21070726e-03 3.18320570e-03 3.14321880e-03\n",
            " 3.11045290e-03 3.07616502e-03 3.06708231e-03 3.06284609e-03\n",
            " 3.03241138e-03 3.00973568e-03 2.98317009e-03 2.95969259e-03\n",
            " 2.95848592e-03 2.95107368e-03 2.92658437e-03 2.91700309e-03\n",
            " 2.86476610e-03 2.85733968e-03 2.85666024e-03 2.81864601e-03\n",
            " 2.81752598e-03 2.81380840e-03 2.81205800e-03 2.79298724e-03\n",
            " 2.76694272e-03 2.74765378e-03 2.73075450e-03 2.71492969e-03\n",
            " 2.67855975e-03 2.67076151e-03 2.67005924e-03 2.65872721e-03\n",
            " 2.65841768e-03 2.62761501e-03 2.62023549e-03 2.61648852e-03\n",
            " 2.61325537e-03 2.61194613e-03 2.60088226e-03 2.58155087e-03\n",
            " 2.55797723e-03 2.53524117e-03 2.52713855e-03 2.52657638e-03\n",
            " 2.51919880e-03 2.51780676e-03 2.46158563e-03 2.45368606e-03\n",
            " 2.45076302e-03 2.43663299e-03 2.42031049e-03 2.42003509e-03\n",
            " 2.40046503e-03 2.37909533e-03 2.37185602e-03 2.36517474e-03\n",
            " 2.36325860e-03 2.35513856e-03 2.34280862e-03 2.31039924e-03\n",
            " 2.30943982e-03 2.27356025e-03 2.25641415e-03 2.20462235e-03\n",
            " 2.19724207e-03 2.19573161e-03 2.18989662e-03 2.17903672e-03\n",
            " 2.17697429e-03 2.17331123e-03 2.15958915e-03 2.15709358e-03\n",
            " 2.15027961e-03 2.13116371e-03 2.12820414e-03 2.12137292e-03\n",
            " 2.11833421e-03 2.10955198e-03 2.10775801e-03 2.09881275e-03\n",
            " 2.09149580e-03 2.08478587e-03 2.08161618e-03 2.07478106e-03\n",
            " 2.05324980e-03 2.01330343e-03 2.01101737e-03 2.00634546e-03\n",
            " 1.99187391e-03 1.98393468e-03 1.98341331e-03 1.97190019e-03\n",
            " 1.96655638e-03 1.95876829e-03 1.95825845e-03 1.94807613e-03\n",
            " 1.93685641e-03 1.93659297e-03 1.93592493e-03 1.93511043e-03\n",
            " 1.92901996e-03 1.92834320e-03 1.91677457e-03 1.88610771e-03\n",
            " 1.87475188e-03 1.84791475e-03 1.84011276e-03 1.83902391e-03\n",
            " 1.83753330e-03 1.83660692e-03 1.83629239e-03 1.83601750e-03\n",
            " 1.81918474e-03 1.81325674e-03 1.81172684e-03 1.81063770e-03\n",
            " 1.80874501e-03 1.80826166e-03 1.80560873e-03 1.79284596e-03\n",
            " 1.78473500e-03 1.78268004e-03 1.78144464e-03 1.78070499e-03\n",
            " 1.77183511e-03 1.75624942e-03 1.74294722e-03 1.73780680e-03\n",
            " 1.73394907e-03 1.72651768e-03 1.72435507e-03 1.72182858e-03\n",
            " 1.72037508e-03 1.71545393e-03 1.71483176e-03 1.71450959e-03\n",
            " 1.71136470e-03 1.70834431e-03 1.70691814e-03 1.69458969e-03\n",
            " 1.69254285e-03 1.67537128e-03 1.67248218e-03 1.66946773e-03\n",
            " 1.65758643e-03 1.65537922e-03 1.65189656e-03 1.64665251e-03\n",
            " 1.64635972e-03 1.64484421e-03 1.62748867e-03 1.62162833e-03\n",
            " 1.62009631e-03 1.61759033e-03 1.60977111e-03 1.59564187e-03\n",
            " 1.58902648e-03 1.58835965e-03 1.58526618e-03 1.57842942e-03\n",
            " 1.57453373e-03 1.57443088e-03 1.57380837e-03 1.57354639e-03\n",
            " 1.56663156e-03 1.56077478e-03 1.55009932e-03 1.54990598e-03\n",
            " 1.54622698e-03 1.54577272e-03 1.54174667e-03 1.54094493e-03\n",
            " 1.54083256e-03 1.54066325e-03 1.54060907e-03 1.53754886e-03\n",
            " 1.52588659e-03 1.52393904e-03 1.52391369e-03 1.52381670e-03\n",
            " 1.52241766e-03 1.52211693e-03 1.50424644e-03 1.50334830e-03\n",
            " 1.50314101e-03 1.49764291e-03 1.49320732e-03 1.49250477e-03\n",
            " 1.49187511e-03 1.48531429e-03 1.48197100e-03 1.47570561e-03\n",
            " 1.47495204e-03 1.46759964e-03 1.46201337e-03 1.44673375e-03\n",
            " 1.44537846e-03 1.44433886e-03 1.44097682e-03 1.42893833e-03\n",
            " 1.42306301e-03 1.41985485e-03 1.41068237e-03 1.40697678e-03\n",
            " 1.40038583e-03 1.39473255e-03 1.38351967e-03 1.38088754e-03\n",
            " 1.37879911e-03 1.37032957e-03 1.36531927e-03 1.35991777e-03\n",
            " 1.35309012e-03 1.34735629e-03 1.34439835e-03 1.33644147e-03\n",
            " 1.33620229e-03 1.32515327e-03 1.32218781e-03 1.32194300e-03\n",
            " 1.32079801e-03 1.31719129e-03 1.31491820e-03 1.31198802e-03\n",
            " 1.31114366e-03 1.30078700e-03 1.29997553e-03 1.29577390e-03\n",
            " 1.28789031e-03 1.28758335e-03 1.28326891e-03 1.28137165e-03\n",
            " 1.28031721e-03 1.26937980e-03 1.25167713e-03 1.25100575e-03\n",
            " 1.24883135e-03 1.24865275e-03 1.24278716e-03 1.23764348e-03\n",
            " 1.23674072e-03 1.23354323e-03 1.23046523e-03 1.22963139e-03\n",
            " 1.22883213e-03 1.22413831e-03 1.22148452e-03 1.21904568e-03\n",
            " 1.21590765e-03 1.21427284e-03 1.21130759e-03 1.20591770e-03\n",
            " 1.20537668e-03 1.20455937e-03 1.19636791e-03 1.19194357e-03\n",
            " 1.18712272e-03 1.18490388e-03 1.17501016e-03 1.17372994e-03\n",
            " 1.17290812e-03 1.17102280e-03 1.16962411e-03 1.16880623e-03\n",
            " 1.16861934e-03 1.16597098e-03 1.16516592e-03 1.16036482e-03\n",
            " 1.15831370e-03 1.15625889e-03 1.15452620e-03 1.15409154e-03\n",
            " 1.15373984e-03 1.14904038e-03 1.14382498e-03 1.14095630e-03\n",
            " 1.13489445e-03 1.13386797e-03 1.13239082e-03 1.13081178e-03\n",
            " 1.12957388e-03 1.12555482e-03 1.12470006e-03 1.12396676e-03\n",
            " 1.12283448e-03 1.12188885e-03 1.11975513e-03 1.11741989e-03\n",
            " 1.11604981e-03 1.11495260e-03 1.11046303e-03 1.10726915e-03\n",
            " 1.10629413e-03 1.10343572e-03 1.10030760e-03 1.09577399e-03\n",
            " 1.08736468e-03 1.08657749e-03 1.08554371e-03 1.08314222e-03\n",
            " 1.08256756e-03 1.08142293e-03 1.07983522e-03 1.07949422e-03\n",
            " 1.07376631e-03 1.07361824e-03 1.06880490e-03 1.06589230e-03\n",
            " 1.06430971e-03 1.06397505e-03 1.06341146e-03 1.06108755e-03\n",
            " 1.05601319e-03 1.05257444e-03 1.05201207e-03 1.04939655e-03\n",
            " 1.04423153e-03 1.04368253e-03 1.04279839e-03 1.04161824e-03\n",
            " 1.04122745e-03 1.03478992e-03 1.03415638e-03 1.02777388e-03\n",
            " 1.02476629e-03 1.02306783e-03 1.01907877e-03 1.01713356e-03\n",
            " 1.01017529e-03 1.00136367e-03 9.99800833e-04 9.98992454e-04\n",
            " 9.97506008e-04 9.92372550e-04 9.91785313e-04 9.90970230e-04\n",
            " 9.90743312e-04 9.88930943e-04 9.82938554e-04 9.81722245e-04\n",
            " 9.78049902e-04 9.76219620e-04 9.75961187e-04 9.73885824e-04\n",
            " 9.68976454e-04 9.68711188e-04 9.52302515e-04 9.51836357e-04\n",
            " 9.50556842e-04 9.46630261e-04 9.46146913e-04 9.46140596e-04\n",
            " 9.44349862e-04 9.42050826e-04 9.36982676e-04 9.34202743e-04\n",
            " 9.32122800e-04 9.30629077e-04 9.30417322e-04 9.28400228e-04\n",
            " 9.27939231e-04 9.26257232e-04 9.22729696e-04 9.20039639e-04\n",
            " 9.18913369e-04 9.14924095e-04 9.14369468e-04 9.12302512e-04\n",
            " 9.09757710e-04 8.99854632e-04 8.98638031e-04 8.97595544e-04\n",
            " 8.94521359e-04 8.92871539e-04 8.87040874e-04 8.84995710e-04\n",
            " 8.81126333e-04 8.76242922e-04 8.73601560e-04 8.72465209e-04\n",
            " 8.72431327e-04 8.72267382e-04 8.72085975e-04 8.68726623e-04\n",
            " 8.67854437e-04 8.67145968e-04 8.62363871e-04 8.60842504e-04\n",
            " 8.60392248e-04 8.56679214e-04 8.56135934e-04 8.53163198e-04\n",
            " 8.49944832e-04 8.45106462e-04 8.43772584e-04 8.43324798e-04\n",
            " 8.42343783e-04 8.37953465e-04 8.36800070e-04 8.35724507e-04\n",
            " 8.32998221e-04 8.32467447e-04 8.28654703e-04 8.26714650e-04\n",
            " 8.25287702e-04 8.25117232e-04 8.24095647e-04 8.20840535e-04\n",
            " 8.18299630e-04 8.18242367e-04 8.16299838e-04 8.10423153e-04\n",
            " 8.06192020e-04 8.03455306e-04 7.99217254e-04 7.97720335e-04\n",
            " 7.94897079e-04 7.93538341e-04 7.91364910e-04 7.87156294e-04\n",
            " 7.84696250e-04 7.84463174e-04 7.81824925e-04 7.78782149e-04\n",
            " 7.77164880e-04 7.69657125e-04 7.69397963e-04 7.67941861e-04\n",
            " 7.64382384e-04 7.64102827e-04 7.60537052e-04 7.58263401e-04\n",
            " 7.57418705e-04 7.52518219e-04 7.48598238e-04 7.47818146e-04\n",
            " 7.47328527e-04 7.46989059e-04 7.40570747e-04 7.40370145e-04\n",
            " 7.38990835e-04 7.31672731e-04 7.29709089e-04 7.29547134e-04\n",
            " 7.25829118e-04 7.25824412e-04 7.25283528e-04 7.13453257e-04\n",
            " 7.11691269e-04 7.08327759e-04 7.06997810e-04 7.05770304e-04\n",
            " 7.04114744e-04 7.03603537e-04 7.03163809e-04 7.00100853e-04\n",
            " 6.99422812e-04 6.96404929e-04 6.94058642e-04 6.92845188e-04\n",
            " 6.89774651e-04 6.88143257e-04 6.87354491e-04 6.87347023e-04\n",
            " 6.86607717e-04 6.86348014e-04 6.85980465e-04 6.85818891e-04\n",
            " 6.80607268e-04 6.80426465e-04 6.77455451e-04 6.76976390e-04\n",
            " 6.75515023e-04 6.74882810e-04 6.74501745e-04 6.72578195e-04\n",
            " 6.69987842e-04 6.69259274e-04 6.66905172e-04 6.66888524e-04\n",
            " 6.64886738e-04 6.63943486e-04 6.63534139e-04 6.60500141e-04\n",
            " 6.56200697e-04 6.53486337e-04 6.53470640e-04 6.53325524e-04\n",
            " 6.51279874e-04 6.48305192e-04 6.45223731e-04 6.42749996e-04\n",
            " 6.42471389e-04 6.41557223e-04 6.39818550e-04 6.33511168e-04\n",
            " 6.29943319e-04 6.29513603e-04 6.25533147e-04 6.23255369e-04\n",
            " 6.22912812e-04 6.21726742e-04 6.21577648e-04 6.19808777e-04\n",
            " 6.16927870e-04 6.16394659e-04 6.14797295e-04 6.13766729e-04\n",
            " 6.12804517e-04 6.06424185e-04 6.06158138e-04 6.03476938e-04\n",
            " 6.02301281e-04 5.99558948e-04 5.97280878e-04 5.91416420e-04\n",
            " 5.91047542e-04 5.89650767e-04 5.86297065e-04 5.84966630e-04\n",
            " 5.77146313e-04 5.76216933e-04 5.74714623e-04 5.72956118e-04\n",
            " 5.68529018e-04 5.68058222e-04 5.67165115e-04 5.60460273e-04\n",
            " 5.58505911e-04 5.56584934e-04 5.56037878e-04 5.54579127e-04\n",
            " 5.53307772e-04 5.51490933e-04 5.47933748e-04 5.46754430e-04\n",
            " 5.45618817e-04 5.44614241e-04 5.44128882e-04 5.43334886e-04\n",
            " 5.42300688e-04 5.40837252e-04 5.38254512e-04 5.38159995e-04\n",
            " 5.37762037e-04 5.36427233e-04 5.30872555e-04 5.30867834e-04\n",
            " 5.30796305e-04 5.30692651e-04 5.30207230e-04 5.29667483e-04\n",
            " 5.27666208e-04 5.24884470e-04 5.18267705e-04 5.16662083e-04\n",
            " 5.15826414e-04 5.15545573e-04 5.14150082e-04 5.13595489e-04\n",
            " 5.08882524e-04 5.07856993e-04 5.07144542e-04 5.06521472e-04\n",
            " 5.06143445e-04 5.05847956e-04 5.05628263e-04 5.00309768e-04\n",
            " 4.96607656e-04 4.96316233e-04 4.95203788e-04 4.93903264e-04\n",
            " 4.92805030e-04 4.92660577e-04 4.91126725e-04 4.84102634e-04\n",
            " 4.83123068e-04 4.80095979e-04 4.79376295e-04 4.78299826e-04\n",
            " 4.77594027e-04 4.76396005e-04 4.74617034e-04 4.71838912e-04\n",
            " 4.70440809e-04 4.70118452e-04 4.69649852e-04 4.66668930e-04\n",
            " 4.66530824e-04 4.65744952e-04 4.63532238e-04 4.62225353e-04\n",
            " 4.61710789e-04 4.60360302e-04 4.59698384e-04 4.55517920e-04\n",
            " 4.53029437e-04 4.50733373e-04 4.50367810e-04 4.47913945e-04\n",
            " 4.46701707e-04 4.44122666e-04 4.43599012e-04 4.43404803e-04\n",
            " 4.42814739e-04 4.39850591e-04 4.39340329e-04 4.35281779e-04\n",
            " 4.33661538e-04 4.32086792e-04 4.32043058e-04 4.30841239e-04\n",
            " 4.29232120e-04 4.28459208e-04 4.28278097e-04 4.27254244e-04\n",
            " 4.26796662e-04 4.26520282e-04 4.26240051e-04 4.24317883e-04\n",
            " 4.23981627e-04 4.23927702e-04 4.22289951e-04 4.22274584e-04\n",
            " 4.19839288e-04 4.19446158e-04 4.17700809e-04 4.14763860e-04\n",
            " 4.12606000e-04 4.12284848e-04 4.10699138e-04 4.10281437e-04\n",
            " 4.09360336e-04 4.08214886e-04 4.08082656e-04 4.06622525e-04\n",
            " 4.05661919e-04 4.04186049e-04 4.01157552e-04 4.00013826e-04\n",
            " 3.97167983e-04 3.96840867e-04 3.96795212e-04 3.95674736e-04\n",
            " 3.94740257e-04 3.94599323e-04 3.94018185e-04 3.93601189e-04\n",
            " 3.93401655e-04 3.92916808e-04 3.91295049e-04 3.88955320e-04\n",
            " 3.88590299e-04 3.88468163e-04 3.88363936e-04 3.86574126e-04\n",
            " 3.86357604e-04 3.84690620e-04 3.83861208e-04 3.83554810e-04\n",
            " 3.82556048e-04 3.80656625e-04 3.80045512e-04 3.79186630e-04\n",
            " 3.75671583e-04 3.75059186e-04 3.71950326e-04 3.71704993e-04\n",
            " 3.67701863e-04 3.67365324e-04 3.66335821e-04 3.65807918e-04\n",
            " 3.65298309e-04 3.62474570e-04 3.61839839e-04 3.61605319e-04\n",
            " 3.59645129e-04 3.58716417e-04 3.58499925e-04 3.53799814e-04\n",
            " 3.49981696e-04 3.49685245e-04 3.49024636e-04 3.47530171e-04\n",
            " 3.46612604e-04 3.45913658e-04 3.42029859e-04 3.41331968e-04\n",
            " 3.41265581e-04 3.40930611e-04 3.40261325e-04 3.39452006e-04\n",
            " 3.37487982e-04 3.36982390e-04 3.36803909e-04 3.36426490e-04\n",
            " 3.32635755e-04 3.32148051e-04 3.29456623e-04 3.28878222e-04\n",
            " 3.27477388e-04 3.18779542e-04 3.17116376e-04 3.16755413e-04\n",
            " 3.14240921e-04 3.13277445e-04 3.12918975e-04 3.12798948e-04\n",
            " 3.11307740e-04 3.10436878e-04 3.09962537e-04 3.09006274e-04\n",
            " 3.05189899e-04 3.04514019e-04 3.03341728e-04 3.02598419e-04\n",
            " 3.00276228e-04 2.99843804e-04 2.98703123e-04 2.98251691e-04\n",
            " 2.98138089e-04 2.98025789e-04 2.95300734e-04 2.94438221e-04\n",
            " 2.94140690e-04 2.93399960e-04 2.92120777e-04 2.90000098e-04\n",
            " 2.89448566e-04 2.88771616e-04 2.88460718e-04 2.87612247e-04\n",
            " 2.86727411e-04 2.86365214e-04 2.79343016e-04 2.79248623e-04\n",
            " 2.75231489e-04 2.73668333e-04 2.72103163e-04 2.70834602e-04\n",
            " 2.70504339e-04 2.70357553e-04 2.66779589e-04 2.65257439e-04\n",
            " 2.65166617e-04 2.64504022e-04 2.63941375e-04 2.61983486e-04\n",
            " 2.61853806e-04 2.60413084e-04 2.60291377e-04 2.59553039e-04\n",
            " 2.56347522e-04 2.52972660e-04 2.52059892e-04 2.49880688e-04\n",
            " 2.46686840e-04 2.46534044e-04 2.45049508e-04 2.42360332e-04\n",
            " 2.40564600e-04 2.39312004e-04 2.38974797e-04 2.38858115e-04\n",
            " 2.37577551e-04 2.36608722e-04 2.36073110e-04 2.35847458e-04\n",
            " 2.35416981e-04 2.33980417e-04 2.31173324e-04 2.30598139e-04\n",
            " 2.30352686e-04 2.29963948e-04 2.29460488e-04 2.25224745e-04\n",
            " 2.20664287e-04 2.16493057e-04 2.16291591e-04 2.15178083e-04\n",
            " 2.14225833e-04 2.14162638e-04 2.11285859e-04 2.10853935e-04\n",
            " 2.09222952e-04 2.08781367e-04 2.08057968e-04 2.07184707e-04\n",
            " 2.03059915e-04 2.01569992e-04 2.00265131e-04 1.99228555e-04\n",
            " 1.98325405e-04 1.97959641e-04 1.96749573e-04 1.95140414e-04\n",
            " 1.94447379e-04 1.91117329e-04 1.84244143e-04 1.83689732e-04\n",
            " 1.81738913e-04 1.81550986e-04 1.81081107e-04 1.79812692e-04\n",
            " 1.79236512e-04 1.78579175e-04 1.78137444e-04 1.77900277e-04\n",
            " 1.77501051e-04 1.75540915e-04 1.70689718e-04 1.70688114e-04\n",
            " 1.69382700e-04 1.69303098e-04 1.64210182e-04 1.64067305e-04\n",
            " 1.63612920e-04 1.62987967e-04 1.61490883e-04 1.61375877e-04\n",
            " 1.59203048e-04 1.58747633e-04 1.55976205e-04 1.54993148e-04\n",
            " 1.54628937e-04 1.51580601e-04 1.51073414e-04 1.50662554e-04\n",
            " 1.47954572e-04 1.47467393e-04 1.45595907e-04 1.45409144e-04\n",
            " 1.41232650e-04 1.40191581e-04 1.40035827e-04 1.39115876e-04\n",
            " 1.38066000e-04 1.37414126e-04 1.37354891e-04 1.36608785e-04\n",
            " 1.36288514e-04 1.35958506e-04 1.35046824e-04 1.34542303e-04\n",
            " 1.26375455e-04 1.26264343e-04 1.24552943e-04 1.22640744e-04\n",
            " 1.22498909e-04 1.22335016e-04 1.21507502e-04 1.18046239e-04\n",
            " 1.17301538e-04 1.16746353e-04 1.16393459e-04 1.16273351e-04\n",
            " 1.14697670e-04 1.14394137e-04 1.11662978e-04 1.10485569e-04\n",
            " 1.09318239e-04 1.09252330e-04 1.08973887e-04 1.08220117e-04\n",
            " 1.07927923e-04 1.06789033e-04 1.05494147e-04 1.04850386e-04\n",
            " 1.04641128e-04 1.00765686e-04 9.97562360e-05 9.83024212e-05\n",
            " 9.69047639e-05 9.45557351e-05 9.44294064e-05 9.37310038e-05\n",
            " 9.30598066e-05 9.21687488e-05 9.17858912e-05 8.99767247e-05\n",
            " 8.84863643e-05 8.84422201e-05 8.27439032e-05 8.16214868e-05\n",
            " 8.10435229e-05 8.02354307e-05 7.71051766e-05 7.67039165e-05\n",
            " 7.64656270e-05 7.62785679e-05 7.60796289e-05 7.55081035e-05\n",
            " 7.49383800e-05 7.34415293e-05 7.30937196e-05 7.14680229e-05\n",
            " 7.09156997e-05 7.08471702e-05 7.07683133e-05 7.06159671e-05\n",
            " 7.00740139e-05 7.00206469e-05 6.92671616e-05 6.62672586e-05\n",
            " 6.23269499e-05 6.14582712e-05 6.04637681e-05 5.91669127e-05\n",
            " 5.88953835e-05 5.47195437e-05 5.45617497e-05 5.43973145e-05\n",
            " 5.27187903e-05 5.11379523e-05 4.49151496e-05 4.34768694e-05\n",
            " 4.12561138e-05 4.06552793e-05 4.06501933e-05 3.96680549e-05\n",
            " 3.84240610e-05 3.79228373e-05 3.78340277e-05 3.70605335e-05\n",
            " 3.68042795e-05 3.60109881e-05 3.57400531e-05 3.53356744e-05\n",
            " 3.49928364e-05 3.48202419e-05 3.38073694e-05 3.34706830e-05\n",
            " 3.27243850e-05 3.26919852e-05 3.10991433e-05 3.06285797e-05\n",
            " 3.04401937e-05 2.75032121e-05 2.26900802e-05 1.94858643e-05\n",
            " 1.69821639e-05 1.68147338e-05 1.67091696e-05 1.55532111e-05\n",
            " 1.48189425e-05 1.43136895e-05 1.39103188e-05 1.25608716e-05\n",
            " 1.12402042e-05 1.06580340e-05 6.67621424e-06 6.09502111e-06\n",
            " 5.81675217e-06 5.71557659e-06 3.90364203e-06 3.81323658e-06\n",
            " 3.68160131e-06 3.47805168e-06 3.23492002e-06 1.72319978e-06]\n",
            "(10, 1000)\n"
          ]
        }
      ],
      "source": [
        "topic_vectors = np.random.dirichlet(vocab_size*[beta], n_topics)\n",
        "print('Ordered probabilities for the first topic vector:')\n",
        "print(np.sort(topic_vectors[0])[::-1])\n",
        "print(topic_vectors.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BaQEe-yD6vl_"
      },
      "source": [
        "## 1.4. Generation of document topic proportions and documents for each node\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N-BziCW56vFL",
        "outputId": "356663fc-a26c-48be-eabf-fe87b5cd2338",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.1, 0.1, 0.1]\n",
            "Ordered probabilities for the first document - node 0 :\n",
            "[2.82081362e-01 1.69135035e-01 1.51202396e-01 1.46899017e-01\n",
            " 8.42271405e-02 7.24495932e-02 7.14080233e-02 2.23439105e-02\n",
            " 1.83315225e-04 7.02080329e-05]\n",
            "Documents of node 0 generated.\n",
            "[0.9, 0.9, 0.9, 0.9, 0.1, 0.1, 0.1, 0.9, 0.9, 0.9]\n",
            "Ordered probabilities for the first document - node 1 :\n",
            "[3.47157571e-01 2.34287591e-01 1.93285377e-01 8.67849841e-02\n",
            " 6.83659209e-02 5.79956673e-02 1.12721734e-02 7.73229844e-04\n",
            " 6.70489600e-05 1.04369943e-05]\n",
            "Documents of node 1 generated.\n",
            "[0.9, 0.1, 0.1, 0.1, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9]\n",
            "Ordered probabilities for the first document - node 2 :\n",
            "[3.99375171e-01 2.60696427e-01 2.06816687e-01 3.95883100e-02\n",
            " 3.78834605e-02 2.20399071e-02 1.59131794e-02 9.37854123e-03\n",
            " 8.12029416e-03 1.88022236e-04]\n",
            "Documents of node 2 generated.\n",
            "[0.1, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.1, 0.1]\n",
            "Ordered probabilities for the first document - node 3 :\n",
            "[3.39227948e-01 2.52488799e-01 1.38156653e-01 1.30898073e-01\n",
            " 1.22047012e-01 1.26526157e-02 4.37178257e-03 1.40010159e-04\n",
            " 1.71063050e-05 6.07864132e-10]\n",
            "Documents of node 3 generated.\n",
            "[0.9, 0.9, 0.9, 0.9, 0.9, 0.1, 0.1, 0.1, 0.9, 0.9]\n",
            "Ordered probabilities for the first document - node 4 :\n",
            "[1.83214079e-01 1.62951363e-01 1.62250958e-01 1.38128260e-01\n",
            " 1.35372883e-01 1.10683400e-01 8.36234060e-02 2.37300732e-02\n",
            " 4.55072599e-05 7.12831618e-08]\n",
            "Documents of node 4 generated.\n"
          ]
        }
      ],
      "source": [
        "doc_topics_all_gt = []\n",
        "documents_all = []\n",
        "z_all = []\n",
        "for i in np.arange(n_nodes):\n",
        "  # Step 2 - generation of document topic proportions for each node\n",
        "  if dirichlet_symmetric:\n",
        "    doc_topics = np.random.dirichlet((n_topics)*[alpha], n_docs)\n",
        "  else:\n",
        "    doc_topics = np.random.dirichlet(prior, n_docs)\n",
        "    prior = rotateArray(prior, len(prior), 3)\n",
        "    print(prior)\n",
        "  print('Ordered probabilities for the first document - node', str(i), ':')\n",
        "  print(np.sort(doc_topics[0])[::-1])\n",
        "  doc_topics_all_gt.append(doc_topics)\n",
        "  # Step 3 - Document generation\n",
        "  documents = [] # Document words\n",
        "  z = [] # Assignments\n",
        "  for docid in np.arange(n_docs):\n",
        "      doc_len = np.random.randint(low=nwords[0], high=nwords[1])\n",
        "      this_doc_words = []\n",
        "      this_doc_assigns = []\n",
        "      for wd_idx in np.arange(doc_len):\n",
        "          tpc = np.nonzero(np.random.multinomial(1, doc_topics[docid]))[0][0]\n",
        "          this_doc_assigns.append(tpc)\n",
        "          word = np.nonzero(np.random.multinomial(1, topic_vectors[tpc]))[0][0]\n",
        "          this_doc_words.append('wd'+str(word))\n",
        "      z.append(this_doc_assigns)\n",
        "      documents.append(this_doc_words)\n",
        "  print(\"Documents of node\", str(i), \"generated.\")\n",
        "  documents_all.append(documents)\n",
        "  z_all.append(z)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HuJUlOIJ69iw"
      },
      "source": [
        "# 2. Preprocessing, generation of training dataset and training of a ProdLDA model at each node"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1XwiP814FZ5U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2330736f-d167-4b16-e8ff-9786348b4fe9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mPyTorchAVITM\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
          ]
        }
      ],
      "source": [
        "ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "14tnh0ndFpb0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88303170-07e7-458a-974f-4d435e9923e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/PyTorchAVITM/pytorchavitm/datasets\n"
          ]
        }
      ],
      "source": [
        "cd /content/PyTorchAVITM/pytorchavitm/datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Is7SF6iQcqA"
      },
      "outputs": [],
      "source": [
        "from bow import BOWDataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k4okSYycQaOK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed5b821f-25b9-4de7-c6be-b61b95e6bb1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/PyTorchAVITM\n"
          ]
        }
      ],
      "source": [
        "cd /content/PyTorchAVITM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JrcVglDoQhU0"
      },
      "outputs": [],
      "source": [
        "from pytorchavitm import AVITM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wchhyQ5bDIhK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4bd7d164-853c-4fe8-a760-255beec49493"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Settings: \n",
            "               N Components: 10\n",
            "               Topic Prior Mean: 0.0\n",
            "               Topic Prior Variance: 0.9\n",
            "               Model Type: prodLDA\n",
            "               Hidden Sizes: (100, 100)\n",
            "               Activation: softplus\n",
            "               Dropout: 0.2\n",
            "               Learn Priors: True\n",
            "               Learning Rate: 0.002\n",
            "               Momentum: 0.99\n",
            "               Reduce On Plateau: False\n",
            "               Save Dir: None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [1/100]\tSamples: [1000/100000]\tTrain Loss: 2207.4982890625\tTime: 0:00:00.386401\n",
            "Epoch: [2/100]\tSamples: [2000/100000]\tTrain Loss: 2192.4028671875\tTime: 0:00:00.209599\n",
            "Epoch: [3/100]\tSamples: [3000/100000]\tTrain Loss: 2180.642625\tTime: 0:00:00.211461\n",
            "Epoch: [4/100]\tSamples: [4000/100000]\tTrain Loss: 2176.7716875\tTime: 0:00:00.205422\n",
            "Epoch: [5/100]\tSamples: [5000/100000]\tTrain Loss: 2169.6767265625\tTime: 0:00:00.185285\n",
            "Epoch: [6/100]\tSamples: [6000/100000]\tTrain Loss: 2164.6504140625\tTime: 0:00:00.198016\n",
            "Epoch: [7/100]\tSamples: [7000/100000]\tTrain Loss: 2158.79903125\tTime: 0:00:00.198232\n",
            "Epoch: [8/100]\tSamples: [8000/100000]\tTrain Loss: 2152.8133125\tTime: 0:00:00.199779\n",
            "Epoch: [9/100]\tSamples: [9000/100000]\tTrain Loss: 2149.604671875\tTime: 0:00:00.213688\n",
            "Epoch: [10/100]\tSamples: [10000/100000]\tTrain Loss: 2143.7403984375\tTime: 0:00:00.198788\n",
            "Epoch: [11/100]\tSamples: [11000/100000]\tTrain Loss: 2142.6097109375\tTime: 0:00:00.194083\n",
            "Epoch: [12/100]\tSamples: [12000/100000]\tTrain Loss: 2137.281765625\tTime: 0:00:00.214742\n",
            "Epoch: [13/100]\tSamples: [13000/100000]\tTrain Loss: 2132.166578125\tTime: 0:00:00.217777\n",
            "Epoch: [14/100]\tSamples: [14000/100000]\tTrain Loss: 2129.579234375\tTime: 0:00:00.203570\n",
            "Epoch: [15/100]\tSamples: [15000/100000]\tTrain Loss: 2124.6638046875\tTime: 0:00:00.196046\n",
            "Epoch: [16/100]\tSamples: [16000/100000]\tTrain Loss: 2122.73228125\tTime: 0:00:00.189825\n",
            "Epoch: [17/100]\tSamples: [17000/100000]\tTrain Loss: 2117.871046875\tTime: 0:00:00.198528\n",
            "Epoch: [18/100]\tSamples: [18000/100000]\tTrain Loss: 2114.787\tTime: 0:00:00.215360\n",
            "Epoch: [19/100]\tSamples: [19000/100000]\tTrain Loss: 2111.8728125\tTime: 0:00:00.195776\n",
            "Epoch: [20/100]\tSamples: [20000/100000]\tTrain Loss: 2108.9492421875\tTime: 0:00:00.206187\n",
            "Epoch: [21/100]\tSamples: [21000/100000]\tTrain Loss: 2104.7151796875\tTime: 0:00:00.209972\n",
            "Epoch: [22/100]\tSamples: [22000/100000]\tTrain Loss: 2101.38809375\tTime: 0:00:00.222523\n",
            "Epoch: [23/100]\tSamples: [23000/100000]\tTrain Loss: 2099.58771875\tTime: 0:00:00.211363\n",
            "Epoch: [24/100]\tSamples: [24000/100000]\tTrain Loss: 2097.3991171875\tTime: 0:00:00.217699\n",
            "Epoch: [25/100]\tSamples: [25000/100000]\tTrain Loss: 2095.521859375\tTime: 0:00:00.201106\n",
            "Epoch: [26/100]\tSamples: [26000/100000]\tTrain Loss: 2093.279453125\tTime: 0:00:00.195816\n",
            "Epoch: [27/100]\tSamples: [27000/100000]\tTrain Loss: 2091.1868984375\tTime: 0:00:00.208234\n",
            "Epoch: [28/100]\tSamples: [28000/100000]\tTrain Loss: 2090.0521484375\tTime: 0:00:00.200757\n",
            "Epoch: [29/100]\tSamples: [29000/100000]\tTrain Loss: 2088.613203125\tTime: 0:00:00.198169\n",
            "Epoch: [30/100]\tSamples: [30000/100000]\tTrain Loss: 2086.6940703125\tTime: 0:00:00.206620\n",
            "Epoch: [31/100]\tSamples: [31000/100000]\tTrain Loss: 2086.80571875\tTime: 0:00:00.209272\n",
            "Epoch: [32/100]\tSamples: [32000/100000]\tTrain Loss: 2085.782125\tTime: 0:00:00.212841\n",
            "Epoch: [33/100]\tSamples: [33000/100000]\tTrain Loss: 2084.191125\tTime: 0:00:00.216592\n",
            "Epoch: [34/100]\tSamples: [34000/100000]\tTrain Loss: 2083.6021171875\tTime: 0:00:00.203915\n",
            "Epoch: [35/100]\tSamples: [35000/100000]\tTrain Loss: 2082.8061015625\tTime: 0:00:00.207955\n",
            "Epoch: [36/100]\tSamples: [36000/100000]\tTrain Loss: 2081.9296796875\tTime: 0:00:00.196412\n",
            "Epoch: [37/100]\tSamples: [37000/100000]\tTrain Loss: 2081.6141875\tTime: 0:00:00.203927\n",
            "Epoch: [38/100]\tSamples: [38000/100000]\tTrain Loss: 2081.1070546875\tTime: 0:00:00.199368\n",
            "Epoch: [39/100]\tSamples: [39000/100000]\tTrain Loss: 2080.4311875\tTime: 0:00:00.198898\n",
            "Epoch: [40/100]\tSamples: [40000/100000]\tTrain Loss: 2080.11465625\tTime: 0:00:00.207018\n",
            "Epoch: [41/100]\tSamples: [41000/100000]\tTrain Loss: 2079.890421875\tTime: 0:00:00.227017\n",
            "Epoch: [42/100]\tSamples: [42000/100000]\tTrain Loss: 2079.2835703125\tTime: 0:00:00.190517\n",
            "Epoch: [43/100]\tSamples: [43000/100000]\tTrain Loss: 2078.8318828125\tTime: 0:00:00.202511\n",
            "Epoch: [44/100]\tSamples: [44000/100000]\tTrain Loss: 2078.4352109375\tTime: 0:00:00.212486\n",
            "Epoch: [45/100]\tSamples: [45000/100000]\tTrain Loss: 2078.3997421875\tTime: 0:00:00.191094\n",
            "Epoch: [46/100]\tSamples: [46000/100000]\tTrain Loss: 2077.98253125\tTime: 0:00:00.199170\n",
            "Epoch: [47/100]\tSamples: [47000/100000]\tTrain Loss: 2077.6929375\tTime: 0:00:00.199126\n",
            "Epoch: [48/100]\tSamples: [48000/100000]\tTrain Loss: 2077.7974375\tTime: 0:00:00.208539\n",
            "Epoch: [49/100]\tSamples: [49000/100000]\tTrain Loss: 2077.6216640625\tTime: 0:00:00.211999\n",
            "Epoch: [50/100]\tSamples: [50000/100000]\tTrain Loss: 2077.2759453125\tTime: 0:00:00.200472\n",
            "Epoch: [51/100]\tSamples: [51000/100000]\tTrain Loss: 2076.8723515625\tTime: 0:00:00.210233\n",
            "Epoch: [52/100]\tSamples: [52000/100000]\tTrain Loss: 2076.7878828125\tTime: 0:00:00.215075\n",
            "Epoch: [53/100]\tSamples: [53000/100000]\tTrain Loss: 2076.55965625\tTime: 0:00:00.200908\n",
            "Epoch: [54/100]\tSamples: [54000/100000]\tTrain Loss: 2076.396984375\tTime: 0:00:00.194413\n",
            "Epoch: [55/100]\tSamples: [55000/100000]\tTrain Loss: 2076.3619453125\tTime: 0:00:00.199856\n",
            "Epoch: [56/100]\tSamples: [56000/100000]\tTrain Loss: 2075.9310546875\tTime: 0:00:00.195707\n",
            "Epoch: [57/100]\tSamples: [57000/100000]\tTrain Loss: 2075.7575703125\tTime: 0:00:00.193658\n",
            "Epoch: [58/100]\tSamples: [58000/100000]\tTrain Loss: 2075.5897734375\tTime: 0:00:00.205182\n",
            "Epoch: [59/100]\tSamples: [59000/100000]\tTrain Loss: 2075.5272265625\tTime: 0:00:00.201170\n",
            "Epoch: [60/100]\tSamples: [60000/100000]\tTrain Loss: 2075.421640625\tTime: 0:00:00.192621\n",
            "Epoch: [61/100]\tSamples: [61000/100000]\tTrain Loss: 2075.1551640625\tTime: 0:00:00.222214\n",
            "Epoch: [62/100]\tSamples: [62000/100000]\tTrain Loss: 2074.943203125\tTime: 0:00:00.214609\n",
            "Epoch: [63/100]\tSamples: [63000/100000]\tTrain Loss: 2074.9593984375\tTime: 0:00:00.205613\n",
            "Epoch: [64/100]\tSamples: [64000/100000]\tTrain Loss: 2074.7642265625\tTime: 0:00:00.232031\n",
            "Epoch: [65/100]\tSamples: [65000/100000]\tTrain Loss: 2074.8112578125\tTime: 0:00:00.188548\n",
            "Epoch: [66/100]\tSamples: [66000/100000]\tTrain Loss: 2074.290015625\tTime: 0:00:00.202339\n",
            "Epoch: [67/100]\tSamples: [67000/100000]\tTrain Loss: 2074.1266171875\tTime: 0:00:00.189444\n",
            "Epoch: [68/100]\tSamples: [68000/100000]\tTrain Loss: 2074.367796875\tTime: 0:00:00.219375\n",
            "Epoch: [69/100]\tSamples: [69000/100000]\tTrain Loss: 2074.175671875\tTime: 0:00:00.197145\n",
            "Epoch: [70/100]\tSamples: [70000/100000]\tTrain Loss: 2074.473640625\tTime: 0:00:00.203894\n",
            "Epoch: [71/100]\tSamples: [71000/100000]\tTrain Loss: 2074.208390625\tTime: 0:00:00.208165\n",
            "Epoch: [72/100]\tSamples: [72000/100000]\tTrain Loss: 2073.895171875\tTime: 0:00:00.195606\n",
            "Epoch: [73/100]\tSamples: [73000/100000]\tTrain Loss: 2073.84515625\tTime: 0:00:00.215008\n",
            "Epoch: [74/100]\tSamples: [74000/100000]\tTrain Loss: 2074.11578125\tTime: 0:00:00.214369\n",
            "Epoch: [75/100]\tSamples: [75000/100000]\tTrain Loss: 2073.8249765625\tTime: 0:00:00.192978\n",
            "Epoch: [76/100]\tSamples: [76000/100000]\tTrain Loss: 2073.73125\tTime: 0:00:00.204869\n",
            "Epoch: [77/100]\tSamples: [77000/100000]\tTrain Loss: 2073.57840625\tTime: 0:00:00.199356\n",
            "Epoch: [78/100]\tSamples: [78000/100000]\tTrain Loss: 2073.3966796875\tTime: 0:00:00.209001\n",
            "Epoch: [79/100]\tSamples: [79000/100000]\tTrain Loss: 2073.65553125\tTime: 0:00:00.197779\n",
            "Epoch: [80/100]\tSamples: [80000/100000]\tTrain Loss: 2073.17509375\tTime: 0:00:00.212581\n",
            "Epoch: [81/100]\tSamples: [81000/100000]\tTrain Loss: 2073.797953125\tTime: 0:00:00.203869\n",
            "Epoch: [82/100]\tSamples: [82000/100000]\tTrain Loss: 2073.3026875\tTime: 0:00:00.211302\n",
            "Epoch: [83/100]\tSamples: [83000/100000]\tTrain Loss: 2073.1111875\tTime: 0:00:00.220261\n",
            "Epoch: [84/100]\tSamples: [84000/100000]\tTrain Loss: 2073.1121953125\tTime: 0:00:00.192733\n",
            "Epoch: [85/100]\tSamples: [85000/100000]\tTrain Loss: 2073.49903125\tTime: 0:00:00.201738\n",
            "Epoch: [86/100]\tSamples: [86000/100000]\tTrain Loss: 2073.5750078125\tTime: 0:00:00.205961\n",
            "Epoch: [87/100]\tSamples: [87000/100000]\tTrain Loss: 2072.90103125\tTime: 0:00:00.203702\n",
            "Epoch: [88/100]\tSamples: [88000/100000]\tTrain Loss: 2073.1080390625\tTime: 0:00:00.218229\n",
            "Epoch: [89/100]\tSamples: [89000/100000]\tTrain Loss: 2072.955125\tTime: 0:00:00.200423\n",
            "Epoch: [90/100]\tSamples: [90000/100000]\tTrain Loss: 2073.1764453125\tTime: 0:00:00.213757\n",
            "Epoch: [91/100]\tSamples: [91000/100000]\tTrain Loss: 2073.0420703125\tTime: 0:00:00.204878\n",
            "Epoch: [92/100]\tSamples: [92000/100000]\tTrain Loss: 2072.7693203125\tTime: 0:00:00.199740\n",
            "Epoch: [93/100]\tSamples: [93000/100000]\tTrain Loss: 2072.735765625\tTime: 0:00:00.232820\n",
            "Epoch: [94/100]\tSamples: [94000/100000]\tTrain Loss: 2073.218875\tTime: 0:00:00.207086\n",
            "Epoch: [95/100]\tSamples: [95000/100000]\tTrain Loss: 2072.6338203125\tTime: 0:00:00.213123\n",
            "Epoch: [96/100]\tSamples: [96000/100000]\tTrain Loss: 2072.77303125\tTime: 0:00:00.216403\n",
            "Epoch: [97/100]\tSamples: [97000/100000]\tTrain Loss: 2073.0395546875\tTime: 0:00:00.217025\n",
            "Epoch: [98/100]\tSamples: [98000/100000]\tTrain Loss: 2072.34790625\tTime: 0:00:00.213775\n",
            "Epoch: [99/100]\tSamples: [99000/100000]\tTrain Loss: 2072.551046875\tTime: 0:00:00.216779\n",
            "Epoch: [100/100]\tSamples: [100000/100000]\tTrain Loss: 2072.3326640625\tTime: 0:00:00.207145\n",
            "Settings: \n",
            "               N Components: 10\n",
            "               Topic Prior Mean: 0.0\n",
            "               Topic Prior Variance: 0.9\n",
            "               Model Type: prodLDA\n",
            "               Hidden Sizes: (100, 100)\n",
            "               Activation: softplus\n",
            "               Dropout: 0.2\n",
            "               Learn Priors: True\n",
            "               Learning Rate: 0.002\n",
            "               Momentum: 0.99\n",
            "               Reduce On Plateau: False\n",
            "               Save Dir: None\n",
            "Epoch: [1/100]\tSamples: [1000/100000]\tTrain Loss: 2190.7323125\tTime: 0:00:00.230950\n",
            "Epoch: [2/100]\tSamples: [2000/100000]\tTrain Loss: 2176.6694453125\tTime: 0:00:00.214896\n",
            "Epoch: [3/100]\tSamples: [3000/100000]\tTrain Loss: 2167.114765625\tTime: 0:00:00.214601\n",
            "Epoch: [4/100]\tSamples: [4000/100000]\tTrain Loss: 2160.2946328125\tTime: 0:00:00.213615\n",
            "Epoch: [5/100]\tSamples: [5000/100000]\tTrain Loss: 2155.1450078125\tTime: 0:00:00.207619\n",
            "Epoch: [6/100]\tSamples: [6000/100000]\tTrain Loss: 2148.5711328125\tTime: 0:00:00.222127\n",
            "Epoch: [7/100]\tSamples: [7000/100000]\tTrain Loss: 2143.131828125\tTime: 0:00:00.229792\n",
            "Epoch: [8/100]\tSamples: [8000/100000]\tTrain Loss: 2136.71978125\tTime: 0:00:00.227676\n",
            "Epoch: [9/100]\tSamples: [9000/100000]\tTrain Loss: 2133.3638828125\tTime: 0:00:00.212595\n",
            "Epoch: [10/100]\tSamples: [10000/100000]\tTrain Loss: 2126.316640625\tTime: 0:00:00.224518\n",
            "Epoch: [11/100]\tSamples: [11000/100000]\tTrain Loss: 2120.0787734375\tTime: 0:00:00.193858\n",
            "Epoch: [12/100]\tSamples: [12000/100000]\tTrain Loss: 2114.2598671875\tTime: 0:00:00.199101\n",
            "Epoch: [13/100]\tSamples: [13000/100000]\tTrain Loss: 2109.305484375\tTime: 0:00:00.197571\n",
            "Epoch: [14/100]\tSamples: [14000/100000]\tTrain Loss: 2105.2976953125\tTime: 0:00:00.200746\n",
            "Epoch: [15/100]\tSamples: [15000/100000]\tTrain Loss: 2101.7891640625\tTime: 0:00:00.214246\n",
            "Epoch: [16/100]\tSamples: [16000/100000]\tTrain Loss: 2097.111640625\tTime: 0:00:00.205620\n",
            "Epoch: [17/100]\tSamples: [17000/100000]\tTrain Loss: 2092.7948984375\tTime: 0:00:00.215312\n",
            "Epoch: [18/100]\tSamples: [18000/100000]\tTrain Loss: 2088.896671875\tTime: 0:00:00.201238\n",
            "Epoch: [19/100]\tSamples: [19000/100000]\tTrain Loss: 2086.191625\tTime: 0:00:00.213714\n",
            "Epoch: [20/100]\tSamples: [20000/100000]\tTrain Loss: 2083.9826796875\tTime: 0:00:00.209326\n",
            "Epoch: [21/100]\tSamples: [21000/100000]\tTrain Loss: 2082.3627421875\tTime: 0:00:00.211917\n",
            "Epoch: [22/100]\tSamples: [22000/100000]\tTrain Loss: 2079.214078125\tTime: 0:00:00.202386\n",
            "Epoch: [23/100]\tSamples: [23000/100000]\tTrain Loss: 2077.1952109375\tTime: 0:00:00.201392\n",
            "Epoch: [24/100]\tSamples: [24000/100000]\tTrain Loss: 2074.9701171875\tTime: 0:00:00.199003\n",
            "Epoch: [25/100]\tSamples: [25000/100000]\tTrain Loss: 2074.551203125\tTime: 0:00:00.207587\n",
            "Epoch: [26/100]\tSamples: [26000/100000]\tTrain Loss: 2073.1411875\tTime: 0:00:00.207052\n",
            "Epoch: [27/100]\tSamples: [27000/100000]\tTrain Loss: 2071.7785078125\tTime: 0:00:00.212885\n",
            "Epoch: [28/100]\tSamples: [28000/100000]\tTrain Loss: 2071.4874765625\tTime: 0:00:00.204187\n",
            "Epoch: [29/100]\tSamples: [29000/100000]\tTrain Loss: 2070.3390078125\tTime: 0:00:00.200272\n",
            "Epoch: [30/100]\tSamples: [30000/100000]\tTrain Loss: 2069.8167421875\tTime: 0:00:00.204310\n",
            "Epoch: [31/100]\tSamples: [31000/100000]\tTrain Loss: 2069.0334375\tTime: 0:00:00.210996\n",
            "Epoch: [32/100]\tSamples: [32000/100000]\tTrain Loss: 2067.881625\tTime: 0:00:00.206955\n",
            "Epoch: [33/100]\tSamples: [33000/100000]\tTrain Loss: 2067.41675\tTime: 0:00:00.204608\n",
            "Epoch: [34/100]\tSamples: [34000/100000]\tTrain Loss: 2066.969875\tTime: 0:00:00.211475\n",
            "Epoch: [35/100]\tSamples: [35000/100000]\tTrain Loss: 2065.9674609375\tTime: 0:00:00.216954\n",
            "Epoch: [36/100]\tSamples: [36000/100000]\tTrain Loss: 2065.8236875\tTime: 0:00:00.218393\n",
            "Epoch: [37/100]\tSamples: [37000/100000]\tTrain Loss: 2065.4239375\tTime: 0:00:00.199189\n",
            "Epoch: [38/100]\tSamples: [38000/100000]\tTrain Loss: 2064.6458359375\tTime: 0:00:00.208193\n",
            "Epoch: [39/100]\tSamples: [39000/100000]\tTrain Loss: 2064.40553125\tTime: 0:00:00.194535\n",
            "Epoch: [40/100]\tSamples: [40000/100000]\tTrain Loss: 2063.6987734375\tTime: 0:00:00.209025\n",
            "Epoch: [41/100]\tSamples: [41000/100000]\tTrain Loss: 2063.84678125\tTime: 0:00:00.194398\n",
            "Epoch: [42/100]\tSamples: [42000/100000]\tTrain Loss: 2063.4176171875\tTime: 0:00:00.191921\n",
            "Epoch: [43/100]\tSamples: [43000/100000]\tTrain Loss: 2062.6646953125\tTime: 0:00:00.208683\n",
            "Epoch: [44/100]\tSamples: [44000/100000]\tTrain Loss: 2062.751421875\tTime: 0:00:00.216811\n",
            "Epoch: [45/100]\tSamples: [45000/100000]\tTrain Loss: 2062.827890625\tTime: 0:00:00.213775\n",
            "Epoch: [46/100]\tSamples: [46000/100000]\tTrain Loss: 2062.295609375\tTime: 0:00:00.210526\n",
            "Epoch: [47/100]\tSamples: [47000/100000]\tTrain Loss: 2061.9342890625\tTime: 0:00:00.209481\n",
            "Epoch: [48/100]\tSamples: [48000/100000]\tTrain Loss: 2061.6132421875\tTime: 0:00:00.194451\n",
            "Epoch: [49/100]\tSamples: [49000/100000]\tTrain Loss: 2061.1735234375\tTime: 0:00:00.212223\n",
            "Epoch: [50/100]\tSamples: [50000/100000]\tTrain Loss: 2061.4210546875\tTime: 0:00:00.202810\n",
            "Epoch: [51/100]\tSamples: [51000/100000]\tTrain Loss: 2060.79359375\tTime: 0:00:00.196243\n",
            "Epoch: [52/100]\tSamples: [52000/100000]\tTrain Loss: 2060.987\tTime: 0:00:00.194849\n",
            "Epoch: [53/100]\tSamples: [53000/100000]\tTrain Loss: 2060.833953125\tTime: 0:00:00.203595\n",
            "Epoch: [54/100]\tSamples: [54000/100000]\tTrain Loss: 2060.9218828125\tTime: 0:00:00.189813\n",
            "Epoch: [55/100]\tSamples: [55000/100000]\tTrain Loss: 2060.601390625\tTime: 0:00:00.210459\n",
            "Epoch: [56/100]\tSamples: [56000/100000]\tTrain Loss: 2060.50675\tTime: 0:00:00.222268\n",
            "Epoch: [57/100]\tSamples: [57000/100000]\tTrain Loss: 2060.3714765625\tTime: 0:00:00.208325\n",
            "Epoch: [58/100]\tSamples: [58000/100000]\tTrain Loss: 2060.0392421875\tTime: 0:00:00.204607\n",
            "Epoch: [59/100]\tSamples: [59000/100000]\tTrain Loss: 2060.2012578125\tTime: 0:00:00.210569\n",
            "Epoch: [60/100]\tSamples: [60000/100000]\tTrain Loss: 2059.847171875\tTime: 0:00:00.223288\n",
            "Epoch: [61/100]\tSamples: [61000/100000]\tTrain Loss: 2059.826921875\tTime: 0:00:00.195209\n",
            "Epoch: [62/100]\tSamples: [62000/100000]\tTrain Loss: 2059.4743828125\tTime: 0:00:00.198452\n",
            "Epoch: [63/100]\tSamples: [63000/100000]\tTrain Loss: 2059.2003671875\tTime: 0:00:00.194928\n",
            "Epoch: [64/100]\tSamples: [64000/100000]\tTrain Loss: 2059.707\tTime: 0:00:00.203461\n",
            "Epoch: [65/100]\tSamples: [65000/100000]\tTrain Loss: 2059.397125\tTime: 0:00:00.229588\n",
            "Epoch: [66/100]\tSamples: [66000/100000]\tTrain Loss: 2059.5957890625\tTime: 0:00:00.214622\n",
            "Epoch: [67/100]\tSamples: [67000/100000]\tTrain Loss: 2059.024109375\tTime: 0:00:00.208664\n",
            "Epoch: [68/100]\tSamples: [68000/100000]\tTrain Loss: 2058.99103125\tTime: 0:00:00.207090\n",
            "Epoch: [69/100]\tSamples: [69000/100000]\tTrain Loss: 2058.8204453125\tTime: 0:00:00.195641\n",
            "Epoch: [70/100]\tSamples: [70000/100000]\tTrain Loss: 2058.99159375\tTime: 0:00:00.208661\n",
            "Epoch: [71/100]\tSamples: [71000/100000]\tTrain Loss: 2058.91125\tTime: 0:00:00.203005\n",
            "Epoch: [72/100]\tSamples: [72000/100000]\tTrain Loss: 2059.1446484375\tTime: 0:00:00.209072\n",
            "Epoch: [73/100]\tSamples: [73000/100000]\tTrain Loss: 2058.7753671875\tTime: 0:00:00.201259\n",
            "Epoch: [74/100]\tSamples: [74000/100000]\tTrain Loss: 2058.4156796875\tTime: 0:00:00.208515\n",
            "Epoch: [75/100]\tSamples: [75000/100000]\tTrain Loss: 2058.7708203125\tTime: 0:00:00.231737\n",
            "Epoch: [76/100]\tSamples: [76000/100000]\tTrain Loss: 2058.6928828125\tTime: 0:00:00.199044\n",
            "Epoch: [77/100]\tSamples: [77000/100000]\tTrain Loss: 2058.739796875\tTime: 0:00:00.206202\n",
            "Epoch: [78/100]\tSamples: [78000/100000]\tTrain Loss: 2057.9822421875\tTime: 0:00:00.208824\n",
            "Epoch: [79/100]\tSamples: [79000/100000]\tTrain Loss: 2058.1401328125\tTime: 0:00:00.202422\n",
            "Epoch: [80/100]\tSamples: [80000/100000]\tTrain Loss: 2058.14596875\tTime: 0:00:00.220097\n",
            "Epoch: [81/100]\tSamples: [81000/100000]\tTrain Loss: 2058.127984375\tTime: 0:00:00.208839\n",
            "Epoch: [82/100]\tSamples: [82000/100000]\tTrain Loss: 2058.443140625\tTime: 0:00:00.213148\n",
            "Epoch: [83/100]\tSamples: [83000/100000]\tTrain Loss: 2058.0426953125\tTime: 0:00:00.207574\n",
            "Epoch: [84/100]\tSamples: [84000/100000]\tTrain Loss: 2058.1614375\tTime: 0:00:00.225668\n",
            "Epoch: [85/100]\tSamples: [85000/100000]\tTrain Loss: 2057.64090625\tTime: 0:00:00.215061\n",
            "Epoch: [86/100]\tSamples: [86000/100000]\tTrain Loss: 2058.1696640625\tTime: 0:00:00.208032\n",
            "Epoch: [87/100]\tSamples: [87000/100000]\tTrain Loss: 2057.7937734375\tTime: 0:00:00.220626\n",
            "Epoch: [88/100]\tSamples: [88000/100000]\tTrain Loss: 2058.371375\tTime: 0:00:00.194948\n",
            "Epoch: [89/100]\tSamples: [89000/100000]\tTrain Loss: 2057.9121640625\tTime: 0:00:00.221737\n",
            "Epoch: [90/100]\tSamples: [90000/100000]\tTrain Loss: 2058.185140625\tTime: 0:00:00.197173\n",
            "Epoch: [91/100]\tSamples: [91000/100000]\tTrain Loss: 2058.219046875\tTime: 0:00:00.205235\n",
            "Epoch: [92/100]\tSamples: [92000/100000]\tTrain Loss: 2057.409625\tTime: 0:00:00.204928\n",
            "Epoch: [93/100]\tSamples: [93000/100000]\tTrain Loss: 2057.915578125\tTime: 0:00:00.184442\n",
            "Epoch: [94/100]\tSamples: [94000/100000]\tTrain Loss: 2057.1368671875\tTime: 0:00:00.220038\n",
            "Epoch: [95/100]\tSamples: [95000/100000]\tTrain Loss: 2057.8395703125\tTime: 0:00:00.198945\n",
            "Epoch: [96/100]\tSamples: [96000/100000]\tTrain Loss: 2057.93346875\tTime: 0:00:00.196005\n",
            "Epoch: [97/100]\tSamples: [97000/100000]\tTrain Loss: 2057.6018671875\tTime: 0:00:00.214780\n",
            "Epoch: [98/100]\tSamples: [98000/100000]\tTrain Loss: 2058.0290234375\tTime: 0:00:00.190926\n",
            "Epoch: [99/100]\tSamples: [99000/100000]\tTrain Loss: 2057.5793359375\tTime: 0:00:00.210291\n",
            "Epoch: [100/100]\tSamples: [100000/100000]\tTrain Loss: 2057.3154765625\tTime: 0:00:00.203572\n",
            "Settings: \n",
            "               N Components: 10\n",
            "               Topic Prior Mean: 0.0\n",
            "               Topic Prior Variance: 0.9\n",
            "               Model Type: prodLDA\n",
            "               Hidden Sizes: (100, 100)\n",
            "               Activation: softplus\n",
            "               Dropout: 0.2\n",
            "               Learn Priors: True\n",
            "               Learning Rate: 0.002\n",
            "               Momentum: 0.99\n",
            "               Reduce On Plateau: False\n",
            "               Save Dir: None\n",
            "Epoch: [1/100]\tSamples: [1000/100000]\tTrain Loss: 2173.33271875\tTime: 0:00:00.193994\n",
            "Epoch: [2/100]\tSamples: [2000/100000]\tTrain Loss: 2158.7566171875\tTime: 0:00:00.200677\n",
            "Epoch: [3/100]\tSamples: [3000/100000]\tTrain Loss: 2147.3986328125\tTime: 0:00:00.221108\n",
            "Epoch: [4/100]\tSamples: [4000/100000]\tTrain Loss: 2141.249453125\tTime: 0:00:00.211639\n",
            "Epoch: [5/100]\tSamples: [5000/100000]\tTrain Loss: 2134.07828125\tTime: 0:00:00.209530\n",
            "Epoch: [6/100]\tSamples: [6000/100000]\tTrain Loss: 2131.3039921875\tTime: 0:00:00.209727\n",
            "Epoch: [7/100]\tSamples: [7000/100000]\tTrain Loss: 2121.4442578125\tTime: 0:00:00.203653\n",
            "Epoch: [8/100]\tSamples: [8000/100000]\tTrain Loss: 2117.9082890625\tTime: 0:00:00.211623\n",
            "Epoch: [9/100]\tSamples: [9000/100000]\tTrain Loss: 2112.3096640625\tTime: 0:00:00.218408\n",
            "Epoch: [10/100]\tSamples: [10000/100000]\tTrain Loss: 2105.313875\tTime: 0:00:00.193943\n",
            "Epoch: [11/100]\tSamples: [11000/100000]\tTrain Loss: 2102.304921875\tTime: 0:00:00.211155\n",
            "Epoch: [12/100]\tSamples: [12000/100000]\tTrain Loss: 2098.229484375\tTime: 0:00:00.212109\n",
            "Epoch: [13/100]\tSamples: [13000/100000]\tTrain Loss: 2092.75271875\tTime: 0:00:00.215730\n",
            "Epoch: [14/100]\tSamples: [14000/100000]\tTrain Loss: 2088.6472890625\tTime: 0:00:00.211015\n",
            "Epoch: [15/100]\tSamples: [15000/100000]\tTrain Loss: 2089.3274140625\tTime: 0:00:00.220407\n",
            "Epoch: [16/100]\tSamples: [16000/100000]\tTrain Loss: 2085.379796875\tTime: 0:00:00.205362\n",
            "Epoch: [17/100]\tSamples: [17000/100000]\tTrain Loss: 2081.092078125\tTime: 0:00:00.195752\n",
            "Epoch: [18/100]\tSamples: [18000/100000]\tTrain Loss: 2079.05825\tTime: 0:00:00.211618\n",
            "Epoch: [19/100]\tSamples: [19000/100000]\tTrain Loss: 2076.060375\tTime: 0:00:00.199960\n",
            "Epoch: [20/100]\tSamples: [20000/100000]\tTrain Loss: 2074.2596875\tTime: 0:00:00.187920\n",
            "Epoch: [21/100]\tSamples: [21000/100000]\tTrain Loss: 2072.310109375\tTime: 0:00:00.207161\n",
            "Epoch: [22/100]\tSamples: [22000/100000]\tTrain Loss: 2069.91878125\tTime: 0:00:00.211900\n",
            "Epoch: [23/100]\tSamples: [23000/100000]\tTrain Loss: 2069.5502421875\tTime: 0:00:00.217438\n",
            "Epoch: [24/100]\tSamples: [24000/100000]\tTrain Loss: 2067.9374375\tTime: 0:00:00.213058\n",
            "Epoch: [25/100]\tSamples: [25000/100000]\tTrain Loss: 2065.26446875\tTime: 0:00:00.204922\n",
            "Epoch: [26/100]\tSamples: [26000/100000]\tTrain Loss: 2063.6638125\tTime: 0:00:00.201706\n",
            "Epoch: [27/100]\tSamples: [27000/100000]\tTrain Loss: 2062.5798984375\tTime: 0:00:00.202333\n",
            "Epoch: [28/100]\tSamples: [28000/100000]\tTrain Loss: 2061.31875\tTime: 0:00:00.214859\n",
            "Epoch: [29/100]\tSamples: [29000/100000]\tTrain Loss: 2060.399578125\tTime: 0:00:00.196402\n",
            "Epoch: [30/100]\tSamples: [30000/100000]\tTrain Loss: 2059.1670078125\tTime: 0:00:00.207209\n",
            "Epoch: [31/100]\tSamples: [31000/100000]\tTrain Loss: 2057.991296875\tTime: 0:00:00.206600\n",
            "Epoch: [32/100]\tSamples: [32000/100000]\tTrain Loss: 2056.6087421875\tTime: 0:00:00.205497\n",
            "Epoch: [33/100]\tSamples: [33000/100000]\tTrain Loss: 2055.373234375\tTime: 0:00:00.213646\n",
            "Epoch: [34/100]\tSamples: [34000/100000]\tTrain Loss: 2054.618640625\tTime: 0:00:00.203957\n",
            "Epoch: [35/100]\tSamples: [35000/100000]\tTrain Loss: 2054.1548828125\tTime: 0:00:00.194120\n",
            "Epoch: [36/100]\tSamples: [36000/100000]\tTrain Loss: 2052.4624296875\tTime: 0:00:00.207180\n",
            "Epoch: [37/100]\tSamples: [37000/100000]\tTrain Loss: 2051.6161171875\tTime: 0:00:00.207813\n",
            "Epoch: [38/100]\tSamples: [38000/100000]\tTrain Loss: 2050.8475859375\tTime: 0:00:00.213449\n",
            "Epoch: [39/100]\tSamples: [39000/100000]\tTrain Loss: 2050.8723125\tTime: 0:00:00.199511\n",
            "Epoch: [40/100]\tSamples: [40000/100000]\tTrain Loss: 2049.5630390625\tTime: 0:00:00.203220\n",
            "Epoch: [41/100]\tSamples: [41000/100000]\tTrain Loss: 2048.9882109375\tTime: 0:00:00.220547\n",
            "Epoch: [42/100]\tSamples: [42000/100000]\tTrain Loss: 2049.0730078125\tTime: 0:00:00.206497\n",
            "Epoch: [43/100]\tSamples: [43000/100000]\tTrain Loss: 2048.271046875\tTime: 0:00:00.217085\n",
            "Epoch: [44/100]\tSamples: [44000/100000]\tTrain Loss: 2047.206484375\tTime: 0:00:00.215266\n",
            "Epoch: [45/100]\tSamples: [45000/100000]\tTrain Loss: 2047.4911484375\tTime: 0:00:00.208773\n",
            "Epoch: [46/100]\tSamples: [46000/100000]\tTrain Loss: 2046.73834375\tTime: 0:00:00.202675\n",
            "Epoch: [47/100]\tSamples: [47000/100000]\tTrain Loss: 2046.6750546875\tTime: 0:00:00.201325\n",
            "Epoch: [48/100]\tSamples: [48000/100000]\tTrain Loss: 2045.94034375\tTime: 0:00:00.206467\n",
            "Epoch: [49/100]\tSamples: [49000/100000]\tTrain Loss: 2045.7553046875\tTime: 0:00:00.209920\n",
            "Epoch: [50/100]\tSamples: [50000/100000]\tTrain Loss: 2045.8309765625\tTime: 0:00:00.192048\n",
            "Epoch: [51/100]\tSamples: [51000/100000]\tTrain Loss: 2045.5828359375\tTime: 0:00:00.207745\n",
            "Epoch: [52/100]\tSamples: [52000/100000]\tTrain Loss: 2044.8624296875\tTime: 0:00:00.201385\n",
            "Epoch: [53/100]\tSamples: [53000/100000]\tTrain Loss: 2044.4963671875\tTime: 0:00:00.217532\n",
            "Epoch: [54/100]\tSamples: [54000/100000]\tTrain Loss: 2044.578390625\tTime: 0:00:00.198673\n",
            "Epoch: [55/100]\tSamples: [55000/100000]\tTrain Loss: 2044.6951328125\tTime: 0:00:00.202379\n",
            "Epoch: [56/100]\tSamples: [56000/100000]\tTrain Loss: 2044.25265625\tTime: 0:00:00.201932\n",
            "Epoch: [57/100]\tSamples: [57000/100000]\tTrain Loss: 2044.4807265625\tTime: 0:00:00.189489\n",
            "Epoch: [58/100]\tSamples: [58000/100000]\tTrain Loss: 2044.16071875\tTime: 0:00:00.227523\n",
            "Epoch: [59/100]\tSamples: [59000/100000]\tTrain Loss: 2043.724953125\tTime: 0:00:00.215447\n",
            "Epoch: [60/100]\tSamples: [60000/100000]\tTrain Loss: 2043.9301640625\tTime: 0:00:00.217660\n",
            "Epoch: [61/100]\tSamples: [61000/100000]\tTrain Loss: 2043.51190625\tTime: 0:00:00.222453\n",
            "Epoch: [62/100]\tSamples: [62000/100000]\tTrain Loss: 2043.153015625\tTime: 0:00:00.236272\n",
            "Epoch: [63/100]\tSamples: [63000/100000]\tTrain Loss: 2043.199109375\tTime: 0:00:00.208047\n",
            "Epoch: [64/100]\tSamples: [64000/100000]\tTrain Loss: 2043.3530390625\tTime: 0:00:00.205898\n",
            "Epoch: [65/100]\tSamples: [65000/100000]\tTrain Loss: 2042.9699140625\tTime: 0:00:00.206490\n",
            "Epoch: [66/100]\tSamples: [66000/100000]\tTrain Loss: 2043.2548203125\tTime: 0:00:00.204670\n",
            "Epoch: [67/100]\tSamples: [67000/100000]\tTrain Loss: 2042.7810703125\tTime: 0:00:00.218365\n",
            "Epoch: [68/100]\tSamples: [68000/100000]\tTrain Loss: 2042.7778125\tTime: 0:00:00.204063\n",
            "Epoch: [69/100]\tSamples: [69000/100000]\tTrain Loss: 2042.5929296875\tTime: 0:00:00.201350\n",
            "Epoch: [70/100]\tSamples: [70000/100000]\tTrain Loss: 2041.969640625\tTime: 0:00:00.224907\n",
            "Epoch: [71/100]\tSamples: [71000/100000]\tTrain Loss: 2042.759609375\tTime: 0:00:00.217239\n",
            "Epoch: [72/100]\tSamples: [72000/100000]\tTrain Loss: 2042.0375\tTime: 0:00:00.222201\n",
            "Epoch: [73/100]\tSamples: [73000/100000]\tTrain Loss: 2041.8227265625\tTime: 0:00:00.201254\n",
            "Epoch: [74/100]\tSamples: [74000/100000]\tTrain Loss: 2042.1884375\tTime: 0:00:00.209621\n",
            "Epoch: [75/100]\tSamples: [75000/100000]\tTrain Loss: 2041.667484375\tTime: 0:00:00.207390\n",
            "Epoch: [76/100]\tSamples: [76000/100000]\tTrain Loss: 2041.7975703125\tTime: 0:00:00.203707\n",
            "Epoch: [77/100]\tSamples: [77000/100000]\tTrain Loss: 2041.806828125\tTime: 0:00:00.214590\n",
            "Epoch: [78/100]\tSamples: [78000/100000]\tTrain Loss: 2042.1288515625\tTime: 0:00:00.209034\n",
            "Epoch: [79/100]\tSamples: [79000/100000]\tTrain Loss: 2041.8275\tTime: 0:00:00.212990\n",
            "Epoch: [80/100]\tSamples: [80000/100000]\tTrain Loss: 2041.552828125\tTime: 0:00:00.210612\n",
            "Epoch: [81/100]\tSamples: [81000/100000]\tTrain Loss: 2042.1619140625\tTime: 0:00:00.213300\n",
            "Epoch: [82/100]\tSamples: [82000/100000]\tTrain Loss: 2041.6476796875\tTime: 0:00:00.223822\n",
            "Epoch: [83/100]\tSamples: [83000/100000]\tTrain Loss: 2041.93321875\tTime: 0:00:00.222720\n",
            "Epoch: [84/100]\tSamples: [84000/100000]\tTrain Loss: 2041.6041015625\tTime: 0:00:00.198663\n",
            "Epoch: [85/100]\tSamples: [85000/100000]\tTrain Loss: 2041.729484375\tTime: 0:00:00.205360\n",
            "Epoch: [86/100]\tSamples: [86000/100000]\tTrain Loss: 2041.6020625\tTime: 0:00:00.236265\n",
            "Epoch: [87/100]\tSamples: [87000/100000]\tTrain Loss: 2041.6171328125\tTime: 0:00:00.210812\n",
            "Epoch: [88/100]\tSamples: [88000/100000]\tTrain Loss: 2041.692328125\tTime: 0:00:00.214133\n",
            "Epoch: [89/100]\tSamples: [89000/100000]\tTrain Loss: 2041.2366640625\tTime: 0:00:00.227639\n",
            "Epoch: [90/100]\tSamples: [90000/100000]\tTrain Loss: 2041.6569453125\tTime: 0:00:00.199971\n",
            "Epoch: [91/100]\tSamples: [91000/100000]\tTrain Loss: 2041.599390625\tTime: 0:00:00.207159\n",
            "Epoch: [92/100]\tSamples: [92000/100000]\tTrain Loss: 2041.3824140625\tTime: 0:00:00.199997\n",
            "Epoch: [93/100]\tSamples: [93000/100000]\tTrain Loss: 2041.4052578125\tTime: 0:00:00.220794\n",
            "Epoch: [94/100]\tSamples: [94000/100000]\tTrain Loss: 2041.718265625\tTime: 0:00:00.196662\n",
            "Epoch: [95/100]\tSamples: [95000/100000]\tTrain Loss: 2041.1530390625\tTime: 0:00:00.196889\n",
            "Epoch: [96/100]\tSamples: [96000/100000]\tTrain Loss: 2041.218296875\tTime: 0:00:00.215506\n",
            "Epoch: [97/100]\tSamples: [97000/100000]\tTrain Loss: 2041.2411171875\tTime: 0:00:00.197372\n",
            "Epoch: [98/100]\tSamples: [98000/100000]\tTrain Loss: 2041.43225\tTime: 0:00:00.208922\n",
            "Epoch: [99/100]\tSamples: [99000/100000]\tTrain Loss: 2041.3698828125\tTime: 0:00:00.199171\n",
            "Epoch: [100/100]\tSamples: [100000/100000]\tTrain Loss: 2041.2255\tTime: 0:00:00.203520\n",
            "Settings: \n",
            "               N Components: 10\n",
            "               Topic Prior Mean: 0.0\n",
            "               Topic Prior Variance: 0.9\n",
            "               Model Type: prodLDA\n",
            "               Hidden Sizes: (100, 100)\n",
            "               Activation: softplus\n",
            "               Dropout: 0.2\n",
            "               Learn Priors: True\n",
            "               Learning Rate: 0.002\n",
            "               Momentum: 0.99\n",
            "               Reduce On Plateau: False\n",
            "               Save Dir: None\n",
            "Epoch: [1/100]\tSamples: [1000/100000]\tTrain Loss: 2181.929359375\tTime: 0:00:00.203728\n",
            "Epoch: [2/100]\tSamples: [2000/100000]\tTrain Loss: 2163.77728125\tTime: 0:00:00.198807\n",
            "Epoch: [3/100]\tSamples: [3000/100000]\tTrain Loss: 2156.334609375\tTime: 0:00:00.209515\n",
            "Epoch: [4/100]\tSamples: [4000/100000]\tTrain Loss: 2149.314953125\tTime: 0:00:00.205297\n",
            "Epoch: [5/100]\tSamples: [5000/100000]\tTrain Loss: 2143.7109140625\tTime: 0:00:00.208182\n",
            "Epoch: [6/100]\tSamples: [6000/100000]\tTrain Loss: 2136.492390625\tTime: 0:00:00.205168\n",
            "Epoch: [7/100]\tSamples: [7000/100000]\tTrain Loss: 2133.304921875\tTime: 0:00:00.204705\n",
            "Epoch: [8/100]\tSamples: [8000/100000]\tTrain Loss: 2130.4224140625\tTime: 0:00:00.210810\n",
            "Epoch: [9/100]\tSamples: [9000/100000]\tTrain Loss: 2127.2166015625\tTime: 0:00:00.207073\n",
            "Epoch: [10/100]\tSamples: [10000/100000]\tTrain Loss: 2121.8561640625\tTime: 0:00:00.213916\n",
            "Epoch: [11/100]\tSamples: [11000/100000]\tTrain Loss: 2118.791984375\tTime: 0:00:00.204955\n",
            "Epoch: [12/100]\tSamples: [12000/100000]\tTrain Loss: 2115.0852578125\tTime: 0:00:00.199499\n",
            "Epoch: [13/100]\tSamples: [13000/100000]\tTrain Loss: 2113.070578125\tTime: 0:00:00.195272\n",
            "Epoch: [14/100]\tSamples: [14000/100000]\tTrain Loss: 2110.1480546875\tTime: 0:00:00.202613\n",
            "Epoch: [15/100]\tSamples: [15000/100000]\tTrain Loss: 2108.605625\tTime: 0:00:00.204314\n",
            "Epoch: [16/100]\tSamples: [16000/100000]\tTrain Loss: 2104.3685546875\tTime: 0:00:00.200405\n",
            "Epoch: [17/100]\tSamples: [17000/100000]\tTrain Loss: 2101.56909375\tTime: 0:00:00.217508\n",
            "Epoch: [18/100]\tSamples: [18000/100000]\tTrain Loss: 2098.4700234375\tTime: 0:00:00.210179\n",
            "Epoch: [19/100]\tSamples: [19000/100000]\tTrain Loss: 2096.9729296875\tTime: 0:00:00.221138\n",
            "Epoch: [20/100]\tSamples: [20000/100000]\tTrain Loss: 2094.52965625\tTime: 0:00:00.201132\n",
            "Epoch: [21/100]\tSamples: [21000/100000]\tTrain Loss: 2090.5015703125\tTime: 0:00:00.200552\n",
            "Epoch: [22/100]\tSamples: [22000/100000]\tTrain Loss: 2089.949140625\tTime: 0:00:00.202920\n",
            "Epoch: [23/100]\tSamples: [23000/100000]\tTrain Loss: 2085.474015625\tTime: 0:00:00.211213\n",
            "Epoch: [24/100]\tSamples: [24000/100000]\tTrain Loss: 2082.632796875\tTime: 0:00:00.195628\n",
            "Epoch: [25/100]\tSamples: [25000/100000]\tTrain Loss: 2079.2291328125\tTime: 0:00:00.229438\n",
            "Epoch: [26/100]\tSamples: [26000/100000]\tTrain Loss: 2077.5188515625\tTime: 0:00:00.208356\n",
            "Epoch: [27/100]\tSamples: [27000/100000]\tTrain Loss: 2074.5286171875\tTime: 0:00:00.209219\n",
            "Epoch: [28/100]\tSamples: [28000/100000]\tTrain Loss: 2072.85396875\tTime: 0:00:00.205213\n",
            "Epoch: [29/100]\tSamples: [29000/100000]\tTrain Loss: 2070.7119609375\tTime: 0:00:00.208125\n",
            "Epoch: [30/100]\tSamples: [30000/100000]\tTrain Loss: 2068.0028984375\tTime: 0:00:00.202328\n",
            "Epoch: [31/100]\tSamples: [31000/100000]\tTrain Loss: 2067.2011171875\tTime: 0:00:00.192947\n",
            "Epoch: [32/100]\tSamples: [32000/100000]\tTrain Loss: 2064.98653125\tTime: 0:00:00.191956\n",
            "Epoch: [33/100]\tSamples: [33000/100000]\tTrain Loss: 2063.483890625\tTime: 0:00:00.202486\n",
            "Epoch: [34/100]\tSamples: [34000/100000]\tTrain Loss: 2062.7385859375\tTime: 0:00:00.194686\n",
            "Epoch: [35/100]\tSamples: [35000/100000]\tTrain Loss: 2061.190265625\tTime: 0:00:00.209340\n",
            "Epoch: [36/100]\tSamples: [36000/100000]\tTrain Loss: 2060.1903125\tTime: 0:00:00.212852\n",
            "Epoch: [37/100]\tSamples: [37000/100000]\tTrain Loss: 2059.9033359375\tTime: 0:00:00.188695\n",
            "Epoch: [38/100]\tSamples: [38000/100000]\tTrain Loss: 2058.683515625\tTime: 0:00:00.221179\n",
            "Epoch: [39/100]\tSamples: [39000/100000]\tTrain Loss: 2057.8413125\tTime: 0:00:00.200727\n",
            "Epoch: [40/100]\tSamples: [40000/100000]\tTrain Loss: 2057.0249453125\tTime: 0:00:00.208108\n",
            "Epoch: [41/100]\tSamples: [41000/100000]\tTrain Loss: 2056.5604609375\tTime: 0:00:00.199642\n",
            "Epoch: [42/100]\tSamples: [42000/100000]\tTrain Loss: 2056.1217890625\tTime: 0:00:00.194612\n",
            "Epoch: [43/100]\tSamples: [43000/100000]\tTrain Loss: 2055.783\tTime: 0:00:00.191942\n",
            "Epoch: [44/100]\tSamples: [44000/100000]\tTrain Loss: 2054.91746875\tTime: 0:00:00.204293\n",
            "Epoch: [45/100]\tSamples: [45000/100000]\tTrain Loss: 2054.280203125\tTime: 0:00:00.208205\n",
            "Epoch: [46/100]\tSamples: [46000/100000]\tTrain Loss: 2053.9528203125\tTime: 0:00:00.201957\n",
            "Epoch: [47/100]\tSamples: [47000/100000]\tTrain Loss: 2053.86178125\tTime: 0:00:00.202236\n",
            "Epoch: [48/100]\tSamples: [48000/100000]\tTrain Loss: 2053.4511875\tTime: 0:00:00.211923\n",
            "Epoch: [49/100]\tSamples: [49000/100000]\tTrain Loss: 2053.0445859375\tTime: 0:00:00.208484\n",
            "Epoch: [50/100]\tSamples: [50000/100000]\tTrain Loss: 2052.5762265625\tTime: 0:00:00.197850\n",
            "Epoch: [51/100]\tSamples: [51000/100000]\tTrain Loss: 2052.5513984375\tTime: 0:00:00.198077\n",
            "Epoch: [52/100]\tSamples: [52000/100000]\tTrain Loss: 2052.0794765625\tTime: 0:00:00.187802\n",
            "Epoch: [53/100]\tSamples: [53000/100000]\tTrain Loss: 2051.548203125\tTime: 0:00:00.207466\n",
            "Epoch: [54/100]\tSamples: [54000/100000]\tTrain Loss: 2051.53071875\tTime: 0:00:00.193530\n",
            "Epoch: [55/100]\tSamples: [55000/100000]\tTrain Loss: 2051.6382109375\tTime: 0:00:00.218930\n",
            "Epoch: [56/100]\tSamples: [56000/100000]\tTrain Loss: 2051.0634921875\tTime: 0:00:00.215455\n",
            "Epoch: [57/100]\tSamples: [57000/100000]\tTrain Loss: 2050.5857109375\tTime: 0:00:00.233082\n",
            "Epoch: [58/100]\tSamples: [58000/100000]\tTrain Loss: 2050.6165390625\tTime: 0:00:00.206975\n",
            "Epoch: [59/100]\tSamples: [59000/100000]\tTrain Loss: 2050.5917578125\tTime: 0:00:00.208507\n",
            "Epoch: [60/100]\tSamples: [60000/100000]\tTrain Loss: 2050.3400234375\tTime: 0:00:00.226569\n",
            "Epoch: [61/100]\tSamples: [61000/100000]\tTrain Loss: 2049.882921875\tTime: 0:00:00.209990\n",
            "Epoch: [62/100]\tSamples: [62000/100000]\tTrain Loss: 2049.8202421875\tTime: 0:00:00.195404\n",
            "Epoch: [63/100]\tSamples: [63000/100000]\tTrain Loss: 2050.2846015625\tTime: 0:00:00.215338\n",
            "Epoch: [64/100]\tSamples: [64000/100000]\tTrain Loss: 2049.875703125\tTime: 0:00:00.207026\n",
            "Epoch: [65/100]\tSamples: [65000/100000]\tTrain Loss: 2049.6333828125\tTime: 0:00:00.213401\n",
            "Epoch: [66/100]\tSamples: [66000/100000]\tTrain Loss: 2049.6853203125\tTime: 0:00:00.202981\n",
            "Epoch: [67/100]\tSamples: [67000/100000]\tTrain Loss: 2049.41259375\tTime: 0:00:00.224077\n",
            "Epoch: [68/100]\tSamples: [68000/100000]\tTrain Loss: 2049.5280546875\tTime: 0:00:00.191747\n",
            "Epoch: [69/100]\tSamples: [69000/100000]\tTrain Loss: 2049.01290625\tTime: 0:00:00.191456\n",
            "Epoch: [70/100]\tSamples: [70000/100000]\tTrain Loss: 2049.327046875\tTime: 0:00:00.213491\n",
            "Epoch: [71/100]\tSamples: [71000/100000]\tTrain Loss: 2048.93675\tTime: 0:00:00.197362\n",
            "Epoch: [72/100]\tSamples: [72000/100000]\tTrain Loss: 2049.3267890625\tTime: 0:00:00.215209\n",
            "Epoch: [73/100]\tSamples: [73000/100000]\tTrain Loss: 2049.42659375\tTime: 0:00:00.195527\n",
            "Epoch: [74/100]\tSamples: [74000/100000]\tTrain Loss: 2048.86728125\tTime: 0:00:00.208030\n",
            "Epoch: [75/100]\tSamples: [75000/100000]\tTrain Loss: 2049.023453125\tTime: 0:00:00.236441\n",
            "Epoch: [76/100]\tSamples: [76000/100000]\tTrain Loss: 2048.949015625\tTime: 0:00:00.214782\n",
            "Epoch: [77/100]\tSamples: [77000/100000]\tTrain Loss: 2048.283578125\tTime: 0:00:00.217269\n",
            "Epoch: [78/100]\tSamples: [78000/100000]\tTrain Loss: 2048.5137578125\tTime: 0:00:00.203433\n",
            "Epoch: [79/100]\tSamples: [79000/100000]\tTrain Loss: 2048.4942734375\tTime: 0:00:00.228681\n",
            "Epoch: [80/100]\tSamples: [80000/100000]\tTrain Loss: 2048.2933984375\tTime: 0:00:00.200980\n",
            "Epoch: [81/100]\tSamples: [81000/100000]\tTrain Loss: 2048.360390625\tTime: 0:00:00.191692\n",
            "Epoch: [82/100]\tSamples: [82000/100000]\tTrain Loss: 2048.9035\tTime: 0:00:00.215884\n",
            "Epoch: [83/100]\tSamples: [83000/100000]\tTrain Loss: 2048.0437421875\tTime: 0:00:00.204007\n",
            "Epoch: [84/100]\tSamples: [84000/100000]\tTrain Loss: 2048.34325\tTime: 0:00:00.217714\n",
            "Epoch: [85/100]\tSamples: [85000/100000]\tTrain Loss: 2048.5376640625\tTime: 0:00:00.214252\n",
            "Epoch: [86/100]\tSamples: [86000/100000]\tTrain Loss: 2048.1346640625\tTime: 0:00:00.216138\n",
            "Epoch: [87/100]\tSamples: [87000/100000]\tTrain Loss: 2048.26640625\tTime: 0:00:00.205739\n",
            "Epoch: [88/100]\tSamples: [88000/100000]\tTrain Loss: 2047.7716015625\tTime: 0:00:00.205035\n",
            "Epoch: [89/100]\tSamples: [89000/100000]\tTrain Loss: 2048.24403125\tTime: 0:00:00.213484\n",
            "Epoch: [90/100]\tSamples: [90000/100000]\tTrain Loss: 2047.8014140625\tTime: 0:00:00.206963\n",
            "Epoch: [91/100]\tSamples: [91000/100000]\tTrain Loss: 2047.7878203125\tTime: 0:00:00.206954\n",
            "Epoch: [92/100]\tSamples: [92000/100000]\tTrain Loss: 2048.24\tTime: 0:00:00.203637\n",
            "Epoch: [93/100]\tSamples: [93000/100000]\tTrain Loss: 2047.8873125\tTime: 0:00:00.202306\n",
            "Epoch: [94/100]\tSamples: [94000/100000]\tTrain Loss: 2048.0343671875\tTime: 0:00:00.223223\n",
            "Epoch: [95/100]\tSamples: [95000/100000]\tTrain Loss: 2047.8243359375\tTime: 0:00:00.213231\n",
            "Epoch: [96/100]\tSamples: [96000/100000]\tTrain Loss: 2047.75359375\tTime: 0:00:00.207826\n",
            "Epoch: [97/100]\tSamples: [97000/100000]\tTrain Loss: 2047.4054765625\tTime: 0:00:00.187483\n",
            "Epoch: [98/100]\tSamples: [98000/100000]\tTrain Loss: 2047.690109375\tTime: 0:00:00.201367\n",
            "Epoch: [99/100]\tSamples: [99000/100000]\tTrain Loss: 2047.6820234375\tTime: 0:00:00.209577\n",
            "Epoch: [100/100]\tSamples: [100000/100000]\tTrain Loss: 2047.6184765625\tTime: 0:00:00.200471\n",
            "Settings: \n",
            "               N Components: 10\n",
            "               Topic Prior Mean: 0.0\n",
            "               Topic Prior Variance: 0.9\n",
            "               Model Type: prodLDA\n",
            "               Hidden Sizes: (100, 100)\n",
            "               Activation: softplus\n",
            "               Dropout: 0.2\n",
            "               Learn Priors: True\n",
            "               Learning Rate: 0.002\n",
            "               Momentum: 0.99\n",
            "               Reduce On Plateau: False\n",
            "               Save Dir: None\n",
            "Epoch: [1/100]\tSamples: [1000/100000]\tTrain Loss: 2192.4793125\tTime: 0:00:00.201634\n",
            "Epoch: [2/100]\tSamples: [2000/100000]\tTrain Loss: 2176.907515625\tTime: 0:00:00.202961\n",
            "Epoch: [3/100]\tSamples: [3000/100000]\tTrain Loss: 2166.247328125\tTime: 0:00:00.212613\n",
            "Epoch: [4/100]\tSamples: [4000/100000]\tTrain Loss: 2159.19940625\tTime: 0:00:00.221603\n",
            "Epoch: [5/100]\tSamples: [5000/100000]\tTrain Loss: 2152.871734375\tTime: 0:00:00.199291\n",
            "Epoch: [6/100]\tSamples: [6000/100000]\tTrain Loss: 2150.13334375\tTime: 0:00:00.200811\n",
            "Epoch: [7/100]\tSamples: [7000/100000]\tTrain Loss: 2142.8842578125\tTime: 0:00:00.194977\n",
            "Epoch: [8/100]\tSamples: [8000/100000]\tTrain Loss: 2137.478203125\tTime: 0:00:00.205295\n",
            "Epoch: [9/100]\tSamples: [9000/100000]\tTrain Loss: 2132.859671875\tTime: 0:00:00.195151\n",
            "Epoch: [10/100]\tSamples: [10000/100000]\tTrain Loss: 2128.641359375\tTime: 0:00:00.206525\n",
            "Epoch: [11/100]\tSamples: [11000/100000]\tTrain Loss: 2123.2282578125\tTime: 0:00:00.211455\n",
            "Epoch: [12/100]\tSamples: [12000/100000]\tTrain Loss: 2116.26865625\tTime: 0:00:00.207082\n",
            "Epoch: [13/100]\tSamples: [13000/100000]\tTrain Loss: 2115.0135859375\tTime: 0:00:00.221175\n",
            "Epoch: [14/100]\tSamples: [14000/100000]\tTrain Loss: 2109.7643125\tTime: 0:00:00.210651\n",
            "Epoch: [15/100]\tSamples: [15000/100000]\tTrain Loss: 2105.860328125\tTime: 0:00:00.208914\n",
            "Epoch: [16/100]\tSamples: [16000/100000]\tTrain Loss: 2102.23803125\tTime: 0:00:00.199013\n",
            "Epoch: [17/100]\tSamples: [17000/100000]\tTrain Loss: 2097.859671875\tTime: 0:00:00.198596\n",
            "Epoch: [18/100]\tSamples: [18000/100000]\tTrain Loss: 2094.894609375\tTime: 0:00:00.193409\n",
            "Epoch: [19/100]\tSamples: [19000/100000]\tTrain Loss: 2092.15209375\tTime: 0:00:00.200029\n",
            "Epoch: [20/100]\tSamples: [20000/100000]\tTrain Loss: 2087.9732109375\tTime: 0:00:00.190073\n",
            "Epoch: [21/100]\tSamples: [21000/100000]\tTrain Loss: 2085.5530390625\tTime: 0:00:00.199786\n",
            "Epoch: [22/100]\tSamples: [22000/100000]\tTrain Loss: 2083.659921875\tTime: 0:00:00.208261\n",
            "Epoch: [23/100]\tSamples: [23000/100000]\tTrain Loss: 2080.845046875\tTime: 0:00:00.234424\n",
            "Epoch: [24/100]\tSamples: [24000/100000]\tTrain Loss: 2079.0519140625\tTime: 0:00:00.196292\n",
            "Epoch: [25/100]\tSamples: [25000/100000]\tTrain Loss: 2077.0672578125\tTime: 0:00:00.191357\n",
            "Epoch: [26/100]\tSamples: [26000/100000]\tTrain Loss: 2075.4680234375\tTime: 0:00:00.211205\n",
            "Epoch: [27/100]\tSamples: [27000/100000]\tTrain Loss: 2073.525765625\tTime: 0:00:00.211439\n",
            "Epoch: [28/100]\tSamples: [28000/100000]\tTrain Loss: 2072.7617265625\tTime: 0:00:00.213548\n",
            "Epoch: [29/100]\tSamples: [29000/100000]\tTrain Loss: 2072.43859375\tTime: 0:00:00.203650\n",
            "Epoch: [30/100]\tSamples: [30000/100000]\tTrain Loss: 2070.64771875\tTime: 0:00:00.199705\n",
            "Epoch: [31/100]\tSamples: [31000/100000]\tTrain Loss: 2069.4098125\tTime: 0:00:00.208107\n",
            "Epoch: [32/100]\tSamples: [32000/100000]\tTrain Loss: 2068.97434375\tTime: 0:00:00.213004\n",
            "Epoch: [33/100]\tSamples: [33000/100000]\tTrain Loss: 2068.20765625\tTime: 0:00:00.228522\n",
            "Epoch: [34/100]\tSamples: [34000/100000]\tTrain Loss: 2067.448484375\tTime: 0:00:00.202696\n",
            "Epoch: [35/100]\tSamples: [35000/100000]\tTrain Loss: 2066.3936640625\tTime: 0:00:00.206531\n",
            "Epoch: [36/100]\tSamples: [36000/100000]\tTrain Loss: 2066.4020546875\tTime: 0:00:00.204723\n",
            "Epoch: [37/100]\tSamples: [37000/100000]\tTrain Loss: 2066.3127421875\tTime: 0:00:00.203293\n",
            "Epoch: [38/100]\tSamples: [38000/100000]\tTrain Loss: 2065.525234375\tTime: 0:00:00.217187\n",
            "Epoch: [39/100]\tSamples: [39000/100000]\tTrain Loss: 2065.3331875\tTime: 0:00:00.190024\n",
            "Epoch: [40/100]\tSamples: [40000/100000]\tTrain Loss: 2064.4893671875\tTime: 0:00:00.199690\n",
            "Epoch: [41/100]\tSamples: [41000/100000]\tTrain Loss: 2064.3398203125\tTime: 0:00:00.212274\n",
            "Epoch: [42/100]\tSamples: [42000/100000]\tTrain Loss: 2064.039734375\tTime: 0:00:00.214133\n",
            "Epoch: [43/100]\tSamples: [43000/100000]\tTrain Loss: 2063.8331953125\tTime: 0:00:00.219064\n",
            "Epoch: [44/100]\tSamples: [44000/100000]\tTrain Loss: 2063.312890625\tTime: 0:00:00.208174\n",
            "Epoch: [45/100]\tSamples: [45000/100000]\tTrain Loss: 2063.435453125\tTime: 0:00:00.204605\n",
            "Epoch: [46/100]\tSamples: [46000/100000]\tTrain Loss: 2062.6445625\tTime: 0:00:00.202589\n",
            "Epoch: [47/100]\tSamples: [47000/100000]\tTrain Loss: 2062.4968359375\tTime: 0:00:00.210261\n",
            "Epoch: [48/100]\tSamples: [48000/100000]\tTrain Loss: 2062.828640625\tTime: 0:00:00.224592\n",
            "Epoch: [49/100]\tSamples: [49000/100000]\tTrain Loss: 2062.458265625\tTime: 0:00:00.204851\n",
            "Epoch: [50/100]\tSamples: [50000/100000]\tTrain Loss: 2061.837046875\tTime: 0:00:00.197512\n",
            "Epoch: [51/100]\tSamples: [51000/100000]\tTrain Loss: 2061.5633671875\tTime: 0:00:00.208545\n",
            "Epoch: [52/100]\tSamples: [52000/100000]\tTrain Loss: 2061.4122109375\tTime: 0:00:00.230914\n",
            "Epoch: [53/100]\tSamples: [53000/100000]\tTrain Loss: 2061.2532265625\tTime: 0:00:00.210982\n",
            "Epoch: [54/100]\tSamples: [54000/100000]\tTrain Loss: 2061.0273671875\tTime: 0:00:00.201516\n",
            "Epoch: [55/100]\tSamples: [55000/100000]\tTrain Loss: 2061.0386171875\tTime: 0:00:00.208164\n",
            "Epoch: [56/100]\tSamples: [56000/100000]\tTrain Loss: 2060.83778125\tTime: 0:00:00.198663\n",
            "Epoch: [57/100]\tSamples: [57000/100000]\tTrain Loss: 2060.8371171875\tTime: 0:00:00.212742\n",
            "Epoch: [58/100]\tSamples: [58000/100000]\tTrain Loss: 2060.355546875\tTime: 0:00:00.207604\n",
            "Epoch: [59/100]\tSamples: [59000/100000]\tTrain Loss: 2060.0765546875\tTime: 0:00:00.203338\n",
            "Epoch: [60/100]\tSamples: [60000/100000]\tTrain Loss: 2060.095203125\tTime: 0:00:00.212529\n",
            "Epoch: [61/100]\tSamples: [61000/100000]\tTrain Loss: 2060.5965078125\tTime: 0:00:00.221282\n",
            "Epoch: [62/100]\tSamples: [62000/100000]\tTrain Loss: 2059.8807109375\tTime: 0:00:00.228207\n",
            "Epoch: [63/100]\tSamples: [63000/100000]\tTrain Loss: 2059.502640625\tTime: 0:00:00.205566\n",
            "Epoch: [64/100]\tSamples: [64000/100000]\tTrain Loss: 2059.39221875\tTime: 0:00:00.213942\n",
            "Epoch: [65/100]\tSamples: [65000/100000]\tTrain Loss: 2059.8181953125\tTime: 0:00:00.209195\n",
            "Epoch: [66/100]\tSamples: [66000/100000]\tTrain Loss: 2059.2651953125\tTime: 0:00:00.200907\n",
            "Epoch: [67/100]\tSamples: [67000/100000]\tTrain Loss: 2059.4158125\tTime: 0:00:00.218112\n",
            "Epoch: [68/100]\tSamples: [68000/100000]\tTrain Loss: 2059.546203125\tTime: 0:00:00.212335\n",
            "Epoch: [69/100]\tSamples: [69000/100000]\tTrain Loss: 2058.986125\tTime: 0:00:00.186651\n",
            "Epoch: [70/100]\tSamples: [70000/100000]\tTrain Loss: 2058.8893984375\tTime: 0:00:00.209542\n",
            "Epoch: [71/100]\tSamples: [71000/100000]\tTrain Loss: 2059.25890625\tTime: 0:00:00.219915\n",
            "Epoch: [72/100]\tSamples: [72000/100000]\tTrain Loss: 2059.17496875\tTime: 0:00:00.209266\n",
            "Epoch: [73/100]\tSamples: [73000/100000]\tTrain Loss: 2058.9147265625\tTime: 0:00:00.197845\n",
            "Epoch: [74/100]\tSamples: [74000/100000]\tTrain Loss: 2058.7283046875\tTime: 0:00:00.197849\n",
            "Epoch: [75/100]\tSamples: [75000/100000]\tTrain Loss: 2058.4310703125\tTime: 0:00:00.192239\n",
            "Epoch: [76/100]\tSamples: [76000/100000]\tTrain Loss: 2058.4260390625\tTime: 0:00:00.199850\n",
            "Epoch: [77/100]\tSamples: [77000/100000]\tTrain Loss: 2057.8336484375\tTime: 0:00:00.211256\n",
            "Epoch: [78/100]\tSamples: [78000/100000]\tTrain Loss: 2058.4881328125\tTime: 0:00:00.197053\n",
            "Epoch: [79/100]\tSamples: [79000/100000]\tTrain Loss: 2058.4227578125\tTime: 0:00:00.203498\n",
            "Epoch: [80/100]\tSamples: [80000/100000]\tTrain Loss: 2058.0433359375\tTime: 0:00:00.209844\n",
            "Epoch: [81/100]\tSamples: [81000/100000]\tTrain Loss: 2058.312203125\tTime: 0:00:00.204415\n",
            "Epoch: [82/100]\tSamples: [82000/100000]\tTrain Loss: 2058.589484375\tTime: 0:00:00.222350\n",
            "Epoch: [83/100]\tSamples: [83000/100000]\tTrain Loss: 2057.787859375\tTime: 0:00:00.206544\n",
            "Epoch: [84/100]\tSamples: [84000/100000]\tTrain Loss: 2058.0052109375\tTime: 0:00:00.200532\n",
            "Epoch: [85/100]\tSamples: [85000/100000]\tTrain Loss: 2058.0661328125\tTime: 0:00:00.204895\n",
            "Epoch: [86/100]\tSamples: [86000/100000]\tTrain Loss: 2058.1526328125\tTime: 0:00:00.212844\n",
            "Epoch: [87/100]\tSamples: [87000/100000]\tTrain Loss: 2057.418921875\tTime: 0:00:00.217015\n",
            "Epoch: [88/100]\tSamples: [88000/100000]\tTrain Loss: 2058.2934375\tTime: 0:00:00.203535\n",
            "Epoch: [89/100]\tSamples: [89000/100000]\tTrain Loss: 2057.947625\tTime: 0:00:00.211033\n",
            "Epoch: [90/100]\tSamples: [90000/100000]\tTrain Loss: 2057.6743046875\tTime: 0:00:00.225862\n",
            "Epoch: [91/100]\tSamples: [91000/100000]\tTrain Loss: 2057.638828125\tTime: 0:00:00.209242\n",
            "Epoch: [92/100]\tSamples: [92000/100000]\tTrain Loss: 2057.849859375\tTime: 0:00:00.218845\n",
            "Epoch: [93/100]\tSamples: [93000/100000]\tTrain Loss: 2057.7966328125\tTime: 0:00:00.210599\n",
            "Epoch: [94/100]\tSamples: [94000/100000]\tTrain Loss: 2057.451921875\tTime: 0:00:00.201112\n",
            "Epoch: [95/100]\tSamples: [95000/100000]\tTrain Loss: 2057.6391015625\tTime: 0:00:00.200436\n",
            "Epoch: [96/100]\tSamples: [96000/100000]\tTrain Loss: 2057.019703125\tTime: 0:00:00.204147\n",
            "Epoch: [97/100]\tSamples: [97000/100000]\tTrain Loss: 2057.5320234375\tTime: 0:00:00.217205\n",
            "Epoch: [98/100]\tSamples: [98000/100000]\tTrain Loss: 2057.689703125\tTime: 0:00:00.202123\n",
            "Epoch: [99/100]\tSamples: [99000/100000]\tTrain Loss: 2057.6510390625\tTime: 0:00:00.215582\n",
            "Epoch: [100/100]\tSamples: [100000/100000]\tTrain Loss: 2057.0740625\tTime: 0:00:00.212165\n"
          ]
        }
      ],
      "source": [
        "train_datasets = []\n",
        "avitms = []\n",
        "id2tokens = []\n",
        "for corpus_node in documents_all:\n",
        "  cv = CountVectorizer(input='content', lowercase=True, stop_words='english',\n",
        "                      max_df=0.99, min_df=0.01, binary=False)\n",
        "  \n",
        "  docs = [\" \".join(corpus_node[i]) for i in np.arange(len(corpus_node))]\n",
        "\n",
        "  train_bow = cv.fit_transform(docs)\n",
        "  train_bow = train_bow.toarray()\n",
        "\n",
        "  idx2token = cv.get_feature_names()\n",
        "  input_size = len(idx2token)\n",
        "\n",
        "  id2token = {k: v for k, v in zip(range(0, len(idx2token)), idx2token)}\n",
        "  id2tokens.append(id2token)\n",
        "\n",
        "  train_data = BOWDataset(train_bow, idx2token)\n",
        "\n",
        "  avitm = AVITM(input_size=input_size, n_components=10, model_type='prodLDA',\n",
        "                hidden_sizes=(100, 100), activation='softplus', dropout=0.2,\n",
        "                learn_priors=True, batch_size=64, lr=2e-3, momentum=0.99,\n",
        "                solver='adam', num_epochs=100, reduce_on_plateau=False)\n",
        "\n",
        "  avitm.fit(train_data)\n",
        "  avitms.append(avitm)\n",
        "\n",
        "  train_datasets.append(train_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CD0GfefPvJD"
      },
      "source": [
        "# 2.1 Topics at each node"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-X9g0dNGk0bC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "ab41b094-8eba-41e5-b1e5-c70cc3e996d0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-3b1e9aab-7f39-4e64-b243-82d368c12d2a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>wd728</td>\n",
              "      <td>wd316</td>\n",
              "      <td>wd129</td>\n",
              "      <td>wd567</td>\n",
              "      <td>wd813</td>\n",
              "      <td>wd743</td>\n",
              "      <td>wd467</td>\n",
              "      <td>wd683</td>\n",
              "      <td>wd344</td>\n",
              "      <td>wd220</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>wd591</td>\n",
              "      <td>wd319</td>\n",
              "      <td>wd684</td>\n",
              "      <td>wd162</td>\n",
              "      <td>wd616</td>\n",
              "      <td>wd342</td>\n",
              "      <td>wd389</td>\n",
              "      <td>wd309</td>\n",
              "      <td>wd291</td>\n",
              "      <td>wd678</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>wd605</td>\n",
              "      <td>wd90</td>\n",
              "      <td>wd293</td>\n",
              "      <td>wd222</td>\n",
              "      <td>wd546</td>\n",
              "      <td>wd822</td>\n",
              "      <td>wd469</td>\n",
              "      <td>wd392</td>\n",
              "      <td>wd125</td>\n",
              "      <td>wd984</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>wd654</td>\n",
              "      <td>wd772</td>\n",
              "      <td>wd419</td>\n",
              "      <td>wd130</td>\n",
              "      <td>wd90</td>\n",
              "      <td>wd562</td>\n",
              "      <td>wd605</td>\n",
              "      <td>wd682</td>\n",
              "      <td>wd931</td>\n",
              "      <td>wd879</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>wd358</td>\n",
              "      <td>wd325</td>\n",
              "      <td>wd752</td>\n",
              "      <td>wd10</td>\n",
              "      <td>wd854</td>\n",
              "      <td>wd134</td>\n",
              "      <td>wd183</td>\n",
              "      <td>wd330</td>\n",
              "      <td>wd595</td>\n",
              "      <td>wd317</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>wd743</td>\n",
              "      <td>wd261</td>\n",
              "      <td>wd509</td>\n",
              "      <td>wd697</td>\n",
              "      <td>wd802</td>\n",
              "      <td>wd242</td>\n",
              "      <td>wd899</td>\n",
              "      <td>wd676</td>\n",
              "      <td>wd745</td>\n",
              "      <td>wd911</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>wd818</td>\n",
              "      <td>wd977</td>\n",
              "      <td>wd829</td>\n",
              "      <td>wd863</td>\n",
              "      <td>wd658</td>\n",
              "      <td>wd511</td>\n",
              "      <td>wd750</td>\n",
              "      <td>wd222</td>\n",
              "      <td>wd105</td>\n",
              "      <td>wd534</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>wd603</td>\n",
              "      <td>wd16</td>\n",
              "      <td>wd282</td>\n",
              "      <td>wd392</td>\n",
              "      <td>wd239</td>\n",
              "      <td>wd947</td>\n",
              "      <td>wd668</td>\n",
              "      <td>wd13</td>\n",
              "      <td>wd968</td>\n",
              "      <td>wd688</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>wd743</td>\n",
              "      <td>wd376</td>\n",
              "      <td>wd749</td>\n",
              "      <td>wd761</td>\n",
              "      <td>wd764</td>\n",
              "      <td>wd403</td>\n",
              "      <td>wd715</td>\n",
              "      <td>wd713</td>\n",
              "      <td>wd457</td>\n",
              "      <td>wd361</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>wd66</td>\n",
              "      <td>wd239</td>\n",
              "      <td>wd802</td>\n",
              "      <td>wd360</td>\n",
              "      <td>wd164</td>\n",
              "      <td>wd630</td>\n",
              "      <td>wd16</td>\n",
              "      <td>wd579</td>\n",
              "      <td>wd379</td>\n",
              "      <td>wd783</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3b1e9aab-7f39-4e64-b243-82d368c12d2a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3b1e9aab-7f39-4e64-b243-82d368c12d2a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3b1e9aab-7f39-4e64-b243-82d368c12d2a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "       0      1      2      3      4      5      6      7      8      9\n",
              "0  wd728  wd316  wd129  wd567  wd813  wd743  wd467  wd683  wd344  wd220\n",
              "1  wd591  wd319  wd684  wd162  wd616  wd342  wd389  wd309  wd291  wd678\n",
              "2  wd605   wd90  wd293  wd222  wd546  wd822  wd469  wd392  wd125  wd984\n",
              "3  wd654  wd772  wd419  wd130   wd90  wd562  wd605  wd682  wd931  wd879\n",
              "4  wd358  wd325  wd752   wd10  wd854  wd134  wd183  wd330  wd595  wd317\n",
              "5  wd743  wd261  wd509  wd697  wd802  wd242  wd899  wd676  wd745  wd911\n",
              "6  wd818  wd977  wd829  wd863  wd658  wd511  wd750  wd222  wd105  wd534\n",
              "7  wd603   wd16  wd282  wd392  wd239  wd947  wd668   wd13  wd968  wd688\n",
              "8  wd743  wd376  wd749  wd761  wd764  wd403  wd715  wd713  wd457  wd361\n",
              "9   wd66  wd239  wd802  wd360  wd164  wd630   wd16  wd579  wd379  wd783"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "topics_all = []\n",
        "for node in np.arange(n_nodes):\n",
        "  topics = pd.DataFrame(avitm.get_topics(10)).T\n",
        "  topics_all.append(topics)\n",
        "topics_all[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4kezlbmRaRV"
      },
      "source": [
        "## 2.2 Document-topic distributions at each node"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "orpK--hqxQkG"
      },
      "outputs": [],
      "source": [
        "def get_doc_topic_distribution(avitm, dataset, n_samples=20):\n",
        "    avitm.model.eval()\n",
        "\n",
        "    loader = DataLoader(\n",
        "            avitm.train_data, batch_size=avitm.batch_size, shuffle=True,\n",
        "            num_workers=mp.cpu_count())\n",
        "\n",
        "    pbar = tqdm(n_samples, position=0, leave=True)\n",
        "\n",
        "    final_thetas = []\n",
        "    for sample_index in range(n_samples):\n",
        "        with torch.no_grad():\n",
        "            collect_theta = []\n",
        "\n",
        "            for batch_samples in loader:\n",
        "                X = batch_samples['X']\n",
        "\n",
        "                if avitm.USE_CUDA:\n",
        "                  X = X.cuda()\n",
        "\n",
        "                # forward pass\n",
        "                avitm.model.zero_grad()\n",
        "                \n",
        "                with torch.no_grad():\n",
        "                  posterior_mu, posterior_log_sigma = avitm.model.inf_net(X)\n",
        "\n",
        "                  # Generate samples from theta\n",
        "                  theta = F.softmax(\n",
        "                          avitm.model.reparameterize(posterior_mu, posterior_log_sigma), dim=1)\n",
        "                  theta = avitm.model.drop_theta(theta)\n",
        "\n",
        "                collect_theta.extend(theta.cpu().numpy().tolist())\n",
        "\n",
        "            pbar.update(1)\n",
        "            pbar.set_description(\"Sampling: [{}/{}]\".format(sample_index + 1, n_samples))\n",
        "\n",
        "            final_thetas.append(np.array(collect_theta))\n",
        "    pbar.close()\n",
        "    return np.sum(final_thetas, axis=0) / n_samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PXTjce2LlVRG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef1dc8fd-e132-420f-8199-28dc04e1505b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Sampling: [5/5]: : 5it [00:00,  6.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document-topic distribution node 0 \n",
            "(1000, 10)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Sampling: [5/5]: : 5it [00:00,  6.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document-topic distribution node 1 \n",
            "(1000, 10)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Sampling: [5/5]: : 5it [00:00,  6.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document-topic distribution node 2 \n",
            "(1000, 10)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Sampling: [5/5]: : 5it [00:00,  6.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document-topic distribution node 3 \n",
            "(1000, 10)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Sampling: [5/5]: : 5it [00:00,  6.82it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document-topic distribution node 4 \n",
            "(1000, 10)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "doc_topic_all = []\n",
        "for node in np.arange(n_nodes):\n",
        "  doc_topic = get_doc_topic_distribution(avitms[node], train_datasets[node], n_samples=5) # get all the topic predictions\n",
        "  print(\"Document-topic distribution node\", str(node), \"\")\n",
        "  doc_topic_all.append(doc_topic)\n",
        "  print(np.array(doc_topics).shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aEE_kyedRgYv"
      },
      "source": [
        "## 2.3 Word-topic distributions attained at each node"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "douV1llyTM4b"
      },
      "outputs": [],
      "source": [
        "def get_topic_word_distribution(avtim_model):\n",
        "  topic_word_matrix = avtim_model.model.beta.cpu().detach().numpy()\n",
        "  return softmax(topic_word_matrix, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BdM2jxRJUvk3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96503ed3-5965-4ada-f1fd-ef90e1045e94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['wd1', 'wd2', 'wd3', 'wd4', 'wd5', 'wd6', 'wd7', 'wd8', 'wd9', 'wd10', 'wd11', 'wd12', 'wd13', 'wd14', 'wd15', 'wd16', 'wd17', 'wd18', 'wd19', 'wd20', 'wd21', 'wd22', 'wd23', 'wd24', 'wd25', 'wd26', 'wd27', 'wd28', 'wd29', 'wd30', 'wd31', 'wd32', 'wd33', 'wd34', 'wd35', 'wd36', 'wd37', 'wd38', 'wd39', 'wd40', 'wd41', 'wd42', 'wd43', 'wd44', 'wd45', 'wd46', 'wd47', 'wd48', 'wd49', 'wd50', 'wd51', 'wd52', 'wd53', 'wd54', 'wd55', 'wd56', 'wd57', 'wd58', 'wd59', 'wd60', 'wd61', 'wd62', 'wd63', 'wd64', 'wd65', 'wd66', 'wd67', 'wd68', 'wd69', 'wd70', 'wd71', 'wd72', 'wd73', 'wd74', 'wd75', 'wd76', 'wd77', 'wd78', 'wd79', 'wd80', 'wd81', 'wd82', 'wd83', 'wd84', 'wd85', 'wd86', 'wd87', 'wd88', 'wd89', 'wd90', 'wd91', 'wd92', 'wd93', 'wd94', 'wd95', 'wd96', 'wd97', 'wd98', 'wd99', 'wd100', 'wd101', 'wd102', 'wd103', 'wd104', 'wd105', 'wd106', 'wd107', 'wd108', 'wd109', 'wd110', 'wd111', 'wd112', 'wd113', 'wd114', 'wd115', 'wd116', 'wd117', 'wd118', 'wd119', 'wd120', 'wd121', 'wd122', 'wd123', 'wd124', 'wd125', 'wd126', 'wd127', 'wd128', 'wd129', 'wd130', 'wd131', 'wd132', 'wd133', 'wd134', 'wd135', 'wd136', 'wd137', 'wd138', 'wd139', 'wd140', 'wd141', 'wd142', 'wd143', 'wd144', 'wd145', 'wd146', 'wd147', 'wd148', 'wd149', 'wd150', 'wd151', 'wd152', 'wd153', 'wd154', 'wd155', 'wd156', 'wd157', 'wd158', 'wd159', 'wd160', 'wd161', 'wd162', 'wd163', 'wd164', 'wd165', 'wd166', 'wd167', 'wd168', 'wd169', 'wd170', 'wd171', 'wd172', 'wd173', 'wd174', 'wd175', 'wd176', 'wd177', 'wd178', 'wd179', 'wd180', 'wd181', 'wd182', 'wd183', 'wd184', 'wd185', 'wd186', 'wd187', 'wd188', 'wd189', 'wd190', 'wd191', 'wd192', 'wd193', 'wd194', 'wd195', 'wd196', 'wd197', 'wd198', 'wd199', 'wd200', 'wd201', 'wd202', 'wd203', 'wd204', 'wd205', 'wd206', 'wd207', 'wd208', 'wd209', 'wd210', 'wd211', 'wd212', 'wd213', 'wd214', 'wd215', 'wd216', 'wd217', 'wd218', 'wd219', 'wd220', 'wd221', 'wd222', 'wd223', 'wd224', 'wd225', 'wd226', 'wd227', 'wd228', 'wd229', 'wd230', 'wd231', 'wd232', 'wd233', 'wd234', 'wd235', 'wd236', 'wd237', 'wd238', 'wd239', 'wd240', 'wd241', 'wd242', 'wd243', 'wd244', 'wd245', 'wd246', 'wd247', 'wd248', 'wd249', 'wd250', 'wd251', 'wd252', 'wd253', 'wd254', 'wd255', 'wd256', 'wd257', 'wd258', 'wd259', 'wd260', 'wd261', 'wd262', 'wd263', 'wd264', 'wd265', 'wd266', 'wd267', 'wd268', 'wd269', 'wd270', 'wd271', 'wd272', 'wd273', 'wd274', 'wd275', 'wd276', 'wd277', 'wd278', 'wd279', 'wd280', 'wd281', 'wd282', 'wd283', 'wd284', 'wd285', 'wd286', 'wd287', 'wd288', 'wd289', 'wd290', 'wd291', 'wd292', 'wd293', 'wd294', 'wd295', 'wd296', 'wd297', 'wd298', 'wd299', 'wd300', 'wd301', 'wd302', 'wd303', 'wd304', 'wd305', 'wd306', 'wd307', 'wd308', 'wd309', 'wd310', 'wd311', 'wd312', 'wd313', 'wd314', 'wd315', 'wd316', 'wd317', 'wd318', 'wd319', 'wd320', 'wd321', 'wd322', 'wd323', 'wd324', 'wd325', 'wd326', 'wd327', 'wd328', 'wd329', 'wd330', 'wd331', 'wd332', 'wd333', 'wd334', 'wd335', 'wd336', 'wd337', 'wd338', 'wd339', 'wd340', 'wd341', 'wd342', 'wd343', 'wd344', 'wd345', 'wd346', 'wd347', 'wd348', 'wd349', 'wd350', 'wd351', 'wd352', 'wd353', 'wd354', 'wd355', 'wd356', 'wd357', 'wd358', 'wd359', 'wd360', 'wd361', 'wd362', 'wd363', 'wd364', 'wd365', 'wd366', 'wd367', 'wd368', 'wd369', 'wd370', 'wd371', 'wd372', 'wd373', 'wd374', 'wd375', 'wd376', 'wd377', 'wd378', 'wd379', 'wd380', 'wd381', 'wd382', 'wd383', 'wd384', 'wd385', 'wd386', 'wd387', 'wd388', 'wd389', 'wd390', 'wd391', 'wd392', 'wd393', 'wd394', 'wd395', 'wd396', 'wd397', 'wd398', 'wd399', 'wd400', 'wd401', 'wd402', 'wd403', 'wd404', 'wd405', 'wd406', 'wd407', 'wd408', 'wd409', 'wd410', 'wd411', 'wd412', 'wd413', 'wd414', 'wd415', 'wd416', 'wd417', 'wd418', 'wd419', 'wd420', 'wd421', 'wd422', 'wd423', 'wd424', 'wd425', 'wd426', 'wd427', 'wd428', 'wd429', 'wd430', 'wd431', 'wd432', 'wd433', 'wd434', 'wd435', 'wd436', 'wd437', 'wd438', 'wd439', 'wd440', 'wd441', 'wd442', 'wd443', 'wd444', 'wd445', 'wd446', 'wd447', 'wd448', 'wd449', 'wd450', 'wd451', 'wd452', 'wd453', 'wd454', 'wd455', 'wd456', 'wd457', 'wd458', 'wd459', 'wd460', 'wd461', 'wd462', 'wd463', 'wd464', 'wd465', 'wd466', 'wd467', 'wd468', 'wd469', 'wd470', 'wd471', 'wd472', 'wd473', 'wd474', 'wd475', 'wd476', 'wd477', 'wd478', 'wd479', 'wd480', 'wd481', 'wd482', 'wd483', 'wd484', 'wd485', 'wd486', 'wd487', 'wd488', 'wd489', 'wd490', 'wd491', 'wd492', 'wd493', 'wd494', 'wd495', 'wd496', 'wd497', 'wd498', 'wd499', 'wd500', 'wd501', 'wd502', 'wd503', 'wd504', 'wd505', 'wd506', 'wd507', 'wd508', 'wd509', 'wd510', 'wd511', 'wd512', 'wd513', 'wd514', 'wd515', 'wd516', 'wd517', 'wd518', 'wd519', 'wd520', 'wd521', 'wd522', 'wd523', 'wd524', 'wd525', 'wd526', 'wd527', 'wd528', 'wd529', 'wd530', 'wd531', 'wd532', 'wd533', 'wd534', 'wd535', 'wd536', 'wd537', 'wd538', 'wd539', 'wd540', 'wd541', 'wd542', 'wd543', 'wd544', 'wd545', 'wd546', 'wd547', 'wd548', 'wd549', 'wd550', 'wd551', 'wd552', 'wd553', 'wd554', 'wd555', 'wd556', 'wd557', 'wd558', 'wd559', 'wd560', 'wd561', 'wd562', 'wd563', 'wd564', 'wd565', 'wd566', 'wd567', 'wd568', 'wd569', 'wd570', 'wd571', 'wd572', 'wd573', 'wd574', 'wd575', 'wd576', 'wd577', 'wd578', 'wd579', 'wd580', 'wd581', 'wd582', 'wd583', 'wd584', 'wd585', 'wd586', 'wd587', 'wd588', 'wd589', 'wd590', 'wd591', 'wd592', 'wd593', 'wd594', 'wd595', 'wd596', 'wd597', 'wd598', 'wd599', 'wd600', 'wd601', 'wd602', 'wd603', 'wd604', 'wd605', 'wd606', 'wd607', 'wd608', 'wd609', 'wd610', 'wd611', 'wd612', 'wd613', 'wd614', 'wd615', 'wd616', 'wd617', 'wd618', 'wd619', 'wd620', 'wd621', 'wd622', 'wd623', 'wd624', 'wd625', 'wd626', 'wd627', 'wd628', 'wd629', 'wd630', 'wd631', 'wd632', 'wd633', 'wd634', 'wd635', 'wd636', 'wd637', 'wd638', 'wd639', 'wd640', 'wd641', 'wd642', 'wd643', 'wd644', 'wd645', 'wd646', 'wd647', 'wd648', 'wd649', 'wd650', 'wd651', 'wd652', 'wd653', 'wd654', 'wd655', 'wd656', 'wd657', 'wd658', 'wd659', 'wd660', 'wd661', 'wd662', 'wd663', 'wd664', 'wd665', 'wd666', 'wd667', 'wd668', 'wd669', 'wd670', 'wd671', 'wd672', 'wd673', 'wd674', 'wd675', 'wd676', 'wd677', 'wd678', 'wd679', 'wd680', 'wd681', 'wd682', 'wd683', 'wd684', 'wd685', 'wd686', 'wd687', 'wd688', 'wd689', 'wd690', 'wd691', 'wd692', 'wd693', 'wd694', 'wd695', 'wd696', 'wd697', 'wd698', 'wd699', 'wd700', 'wd701', 'wd702', 'wd703', 'wd704', 'wd705', 'wd706', 'wd707', 'wd708', 'wd709', 'wd710', 'wd711', 'wd712', 'wd713', 'wd714', 'wd715', 'wd716', 'wd717', 'wd718', 'wd719', 'wd720', 'wd721', 'wd722', 'wd723', 'wd724', 'wd725', 'wd726', 'wd727', 'wd728', 'wd729', 'wd730', 'wd731', 'wd732', 'wd733', 'wd734', 'wd735', 'wd736', 'wd737', 'wd738', 'wd739', 'wd740', 'wd741', 'wd742', 'wd743', 'wd744', 'wd745', 'wd746', 'wd747', 'wd748', 'wd749', 'wd750', 'wd751', 'wd752', 'wd753', 'wd754', 'wd755', 'wd756', 'wd757', 'wd758', 'wd759', 'wd760', 'wd761', 'wd762', 'wd763', 'wd764', 'wd765', 'wd766', 'wd767', 'wd768', 'wd769', 'wd770', 'wd771', 'wd772', 'wd773', 'wd774', 'wd775', 'wd776', 'wd777', 'wd778', 'wd779', 'wd780', 'wd781', 'wd782', 'wd783', 'wd784', 'wd785', 'wd786', 'wd787', 'wd788', 'wd789', 'wd790', 'wd791', 'wd792', 'wd793', 'wd794', 'wd795', 'wd796', 'wd797', 'wd798', 'wd799', 'wd800', 'wd801', 'wd802', 'wd803', 'wd804', 'wd805', 'wd806', 'wd807', 'wd808', 'wd809', 'wd810', 'wd811', 'wd812', 'wd813', 'wd814', 'wd815', 'wd816', 'wd817', 'wd818', 'wd819', 'wd820', 'wd821', 'wd822', 'wd823', 'wd824', 'wd825', 'wd826', 'wd827', 'wd828', 'wd829', 'wd830', 'wd831', 'wd832', 'wd833', 'wd834', 'wd835', 'wd836', 'wd837', 'wd838', 'wd839', 'wd840', 'wd841', 'wd842', 'wd843', 'wd844', 'wd845', 'wd846', 'wd847', 'wd848', 'wd849', 'wd850', 'wd851', 'wd852', 'wd853', 'wd854', 'wd855', 'wd856', 'wd857', 'wd858', 'wd859', 'wd860', 'wd861', 'wd862', 'wd863', 'wd864', 'wd865', 'wd866', 'wd867', 'wd868', 'wd869', 'wd870', 'wd871', 'wd872', 'wd873', 'wd874', 'wd875', 'wd876', 'wd877', 'wd878', 'wd879', 'wd880', 'wd881', 'wd882', 'wd883', 'wd884', 'wd885', 'wd886', 'wd887', 'wd888', 'wd889', 'wd890', 'wd891', 'wd892', 'wd893', 'wd894', 'wd895', 'wd896', 'wd897', 'wd898', 'wd899', 'wd900', 'wd901', 'wd902', 'wd903', 'wd904', 'wd905', 'wd906', 'wd907', 'wd908', 'wd909', 'wd910', 'wd911', 'wd912', 'wd913', 'wd914', 'wd915', 'wd916', 'wd917', 'wd918', 'wd919', 'wd920', 'wd921', 'wd922', 'wd923', 'wd924', 'wd925', 'wd926', 'wd927', 'wd928', 'wd929', 'wd930', 'wd931', 'wd932', 'wd933', 'wd934', 'wd935', 'wd936', 'wd937', 'wd938', 'wd939', 'wd940', 'wd941', 'wd942', 'wd943', 'wd944', 'wd945', 'wd946', 'wd947', 'wd948', 'wd949', 'wd950', 'wd951', 'wd952', 'wd953', 'wd954', 'wd955', 'wd956', 'wd957', 'wd958', 'wd959', 'wd960', 'wd961', 'wd962', 'wd963', 'wd964', 'wd965', 'wd966', 'wd967', 'wd968', 'wd969', 'wd970', 'wd971', 'wd972', 'wd973', 'wd974', 'wd975', 'wd976', 'wd977', 'wd978', 'wd979', 'wd980', 'wd981', 'wd982', 'wd983', 'wd984', 'wd985', 'wd986', 'wd987', 'wd988', 'wd989', 'wd990', 'wd991', 'wd992', 'wd993', 'wd994', 'wd995', 'wd996', 'wd997', 'wd998', 'wd999', 'wd1000']\n"
          ]
        }
      ],
      "source": [
        "all_words = []\n",
        "for word in np.arange(vocab_size+1):\n",
        "  if word > 0:\n",
        "    all_words.append('wd'+str(word))\n",
        "print(all_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MJfQe7PdUmkC"
      },
      "outputs": [],
      "source": [
        "topic_word_all = []\n",
        "for node in np.arange(n_nodes):\n",
        "  w_t_distrib = np.zeros((10,vocab_size), dtype=np.float64) \n",
        "  wd = get_topic_word_distribution(avitms[node])\n",
        "  for i in np.arange(10):\n",
        "    for idx, word in id2tokens[node].items():\n",
        "      for j in np.arange(len(all_words)):\n",
        "        if all_words[j] == word:\n",
        "          w_t_distrib[i,j] = wd[i][idx]\n",
        "          break\n",
        "  sum_of_rows = w_t_distrib.sum(axis=1)\n",
        "  normalized_array = w_t_distrib / sum_of_rows[:, np.newaxis]\n",
        "  topic_word_all.append(normalized_array)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QieP5DdU7zY0"
      },
      "source": [
        "# 3. Centralized model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wTQrxfRpRrD1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebc29b66-9fce-4604-fd2f-24f5a1b4d1b0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5000"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "documents_centr = [*documents_all[0], *documents_all[1], *documents_all[2], *documents_all[3], *documents_all[4]]\n",
        "len(documents_centr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kxW_tMtFRyfX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c4e450c-3ccd-447e-bf1a-2d00e5cff0dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Settings: \n",
            "               N Components: 10\n",
            "               Topic Prior Mean: 0.0\n",
            "               Topic Prior Variance: 0.9\n",
            "               Model Type: prodLDA\n",
            "               Hidden Sizes: (100, 100)\n",
            "               Activation: softplus\n",
            "               Dropout: 0.2\n",
            "               Learn Priors: True\n",
            "               Learning Rate: 0.002\n",
            "               Momentum: 0.99\n",
            "               Reduce On Plateau: False\n",
            "               Save Dir: None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [1/100]\tSamples: [5000/500000]\tTrain Loss: 2168.87533046875\tTime: 0:00:00.702300\n",
            "Epoch: [2/100]\tSamples: [10000/500000]\tTrain Loss: 2140.241965234375\tTime: 0:00:00.726461\n",
            "Epoch: [3/100]\tSamples: [15000/500000]\tTrain Loss: 2117.941953515625\tTime: 0:00:00.768611\n",
            "Epoch: [4/100]\tSamples: [20000/500000]\tTrain Loss: 2098.736754296875\tTime: 0:00:00.746288\n",
            "Epoch: [5/100]\tSamples: [25000/500000]\tTrain Loss: 2083.039616015625\tTime: 0:00:00.718176\n",
            "Epoch: [6/100]\tSamples: [30000/500000]\tTrain Loss: 2073.4177876953127\tTime: 0:00:00.754804\n",
            "Epoch: [7/100]\tSamples: [35000/500000]\tTrain Loss: 2068.00695625\tTime: 0:00:00.712224\n",
            "Epoch: [8/100]\tSamples: [40000/500000]\tTrain Loss: 2064.78079453125\tTime: 0:00:00.784470\n",
            "Epoch: [9/100]\tSamples: [45000/500000]\tTrain Loss: 2062.7489203125\tTime: 0:00:00.750257\n",
            "Epoch: [10/100]\tSamples: [50000/500000]\tTrain Loss: 2061.5458974609373\tTime: 0:00:00.729916\n",
            "Epoch: [11/100]\tSamples: [55000/500000]\tTrain Loss: 2060.629187890625\tTime: 0:00:00.738933\n",
            "Epoch: [12/100]\tSamples: [60000/500000]\tTrain Loss: 2060.024664453125\tTime: 0:00:00.729311\n",
            "Epoch: [13/100]\tSamples: [65000/500000]\tTrain Loss: 2059.4968123046874\tTime: 0:00:00.713198\n",
            "Epoch: [14/100]\tSamples: [70000/500000]\tTrain Loss: 2059.083262890625\tTime: 0:00:00.765644\n",
            "Epoch: [15/100]\tSamples: [75000/500000]\tTrain Loss: 2058.9276439453124\tTime: 0:00:00.780158\n",
            "Epoch: [16/100]\tSamples: [80000/500000]\tTrain Loss: 2058.606034765625\tTime: 0:00:00.762561\n",
            "Epoch: [17/100]\tSamples: [85000/500000]\tTrain Loss: 2058.2511046875\tTime: 0:00:00.739340\n",
            "Epoch: [18/100]\tSamples: [90000/500000]\tTrain Loss: 2058.119503125\tTime: 0:00:00.769757\n",
            "Epoch: [19/100]\tSamples: [95000/500000]\tTrain Loss: 2057.9788921875\tTime: 0:00:00.777756\n",
            "Epoch: [20/100]\tSamples: [100000/500000]\tTrain Loss: 2057.62670234375\tTime: 0:00:00.759647\n",
            "Epoch: [21/100]\tSamples: [105000/500000]\tTrain Loss: 2057.6285716796874\tTime: 0:00:00.749358\n",
            "Epoch: [22/100]\tSamples: [110000/500000]\tTrain Loss: 2057.4837052734374\tTime: 0:00:00.782451\n",
            "Epoch: [23/100]\tSamples: [115000/500000]\tTrain Loss: 2057.264124609375\tTime: 0:00:00.769869\n",
            "Epoch: [24/100]\tSamples: [120000/500000]\tTrain Loss: 2057.19800078125\tTime: 0:00:00.807812\n",
            "Epoch: [25/100]\tSamples: [125000/500000]\tTrain Loss: 2057.00296171875\tTime: 0:00:00.749723\n",
            "Epoch: [26/100]\tSamples: [130000/500000]\tTrain Loss: 2057.003465234375\tTime: 0:00:00.725186\n",
            "Epoch: [27/100]\tSamples: [135000/500000]\tTrain Loss: 2056.9188349609376\tTime: 0:00:00.758270\n",
            "Epoch: [28/100]\tSamples: [140000/500000]\tTrain Loss: 2056.615615234375\tTime: 0:00:00.735223\n",
            "Epoch: [29/100]\tSamples: [145000/500000]\tTrain Loss: 2056.736665234375\tTime: 0:00:00.727696\n",
            "Epoch: [30/100]\tSamples: [150000/500000]\tTrain Loss: 2056.5300076171875\tTime: 0:00:00.760175\n",
            "Epoch: [31/100]\tSamples: [155000/500000]\tTrain Loss: 2056.441596484375\tTime: 0:00:00.713191\n",
            "Epoch: [32/100]\tSamples: [160000/500000]\tTrain Loss: 2056.442271875\tTime: 0:00:00.763113\n",
            "Epoch: [33/100]\tSamples: [165000/500000]\tTrain Loss: 2056.4850353515626\tTime: 0:00:00.732281\n",
            "Epoch: [34/100]\tSamples: [170000/500000]\tTrain Loss: 2056.33643515625\tTime: 0:00:00.752574\n",
            "Epoch: [35/100]\tSamples: [175000/500000]\tTrain Loss: 2056.08364375\tTime: 0:00:00.774625\n",
            "Epoch: [36/100]\tSamples: [180000/500000]\tTrain Loss: 2056.02583828125\tTime: 0:00:00.774673\n",
            "Epoch: [37/100]\tSamples: [185000/500000]\tTrain Loss: 2056.27543671875\tTime: 0:00:00.732396\n",
            "Epoch: [38/100]\tSamples: [190000/500000]\tTrain Loss: 2056.19118828125\tTime: 0:00:00.735027\n",
            "Epoch: [39/100]\tSamples: [195000/500000]\tTrain Loss: 2055.820708203125\tTime: 0:00:00.724319\n",
            "Epoch: [40/100]\tSamples: [200000/500000]\tTrain Loss: 2056.026464453125\tTime: 0:00:00.781314\n",
            "Epoch: [41/100]\tSamples: [205000/500000]\tTrain Loss: 2055.9664908203126\tTime: 0:00:00.762933\n",
            "Epoch: [42/100]\tSamples: [210000/500000]\tTrain Loss: 2055.77092890625\tTime: 0:00:00.806583\n",
            "Epoch: [43/100]\tSamples: [215000/500000]\tTrain Loss: 2055.7071865234375\tTime: 0:00:00.791875\n",
            "Epoch: [44/100]\tSamples: [220000/500000]\tTrain Loss: 2055.864649609375\tTime: 0:00:00.767757\n",
            "Epoch: [45/100]\tSamples: [225000/500000]\tTrain Loss: 2055.772051953125\tTime: 0:00:00.770733\n",
            "Epoch: [46/100]\tSamples: [230000/500000]\tTrain Loss: 2055.7134693359376\tTime: 0:00:00.768500\n",
            "Epoch: [47/100]\tSamples: [235000/500000]\tTrain Loss: 2055.86792734375\tTime: 0:00:00.785066\n",
            "Epoch: [48/100]\tSamples: [240000/500000]\tTrain Loss: 2055.7909919921876\tTime: 0:00:00.781534\n",
            "Epoch: [49/100]\tSamples: [245000/500000]\tTrain Loss: 2055.58807890625\tTime: 0:00:00.763535\n",
            "Epoch: [50/100]\tSamples: [250000/500000]\tTrain Loss: 2055.6664\tTime: 0:00:00.786897\n",
            "Epoch: [51/100]\tSamples: [255000/500000]\tTrain Loss: 2055.4474546875\tTime: 0:00:00.773627\n",
            "Epoch: [52/100]\tSamples: [260000/500000]\tTrain Loss: 2055.4058134765623\tTime: 0:00:00.718708\n",
            "Epoch: [53/100]\tSamples: [265000/500000]\tTrain Loss: 2055.505080078125\tTime: 0:00:00.751671\n",
            "Epoch: [54/100]\tSamples: [270000/500000]\tTrain Loss: 2055.493910546875\tTime: 0:00:00.750427\n",
            "Epoch: [55/100]\tSamples: [275000/500000]\tTrain Loss: 2055.62377890625\tTime: 0:00:00.752797\n",
            "Epoch: [56/100]\tSamples: [280000/500000]\tTrain Loss: 2055.43636640625\tTime: 0:00:00.787235\n",
            "Epoch: [57/100]\tSamples: [285000/500000]\tTrain Loss: 2055.47970390625\tTime: 0:00:00.736449\n",
            "Epoch: [58/100]\tSamples: [290000/500000]\tTrain Loss: 2055.4067974609375\tTime: 0:00:00.786107\n",
            "Epoch: [59/100]\tSamples: [295000/500000]\tTrain Loss: 2055.31241484375\tTime: 0:00:00.731871\n",
            "Epoch: [60/100]\tSamples: [300000/500000]\tTrain Loss: 2055.3607009765624\tTime: 0:00:00.724064\n",
            "Epoch: [61/100]\tSamples: [305000/500000]\tTrain Loss: 2055.2720755859373\tTime: 0:00:00.790636\n",
            "Epoch: [62/100]\tSamples: [310000/500000]\tTrain Loss: 2055.4728654296873\tTime: 0:00:00.760680\n",
            "Epoch: [63/100]\tSamples: [315000/500000]\tTrain Loss: 2055.183617578125\tTime: 0:00:00.805224\n",
            "Epoch: [64/100]\tSamples: [320000/500000]\tTrain Loss: 2055.518912109375\tTime: 0:00:00.794697\n",
            "Epoch: [65/100]\tSamples: [325000/500000]\tTrain Loss: 2055.46177109375\tTime: 0:00:00.794972\n",
            "Epoch: [66/100]\tSamples: [330000/500000]\tTrain Loss: 2055.13011796875\tTime: 0:00:00.845438\n",
            "Epoch: [67/100]\tSamples: [335000/500000]\tTrain Loss: 2055.3943546875\tTime: 0:00:00.759292\n",
            "Epoch: [68/100]\tSamples: [340000/500000]\tTrain Loss: 2055.419575\tTime: 0:00:00.753873\n",
            "Epoch: [69/100]\tSamples: [345000/500000]\tTrain Loss: 2055.437737109375\tTime: 0:00:00.778079\n",
            "Epoch: [70/100]\tSamples: [350000/500000]\tTrain Loss: 2055.545371484375\tTime: 0:00:00.771780\n",
            "Epoch: [71/100]\tSamples: [355000/500000]\tTrain Loss: 2055.4451\tTime: 0:00:00.771322\n",
            "Epoch: [72/100]\tSamples: [360000/500000]\tTrain Loss: 2055.46058984375\tTime: 0:00:00.761695\n",
            "Epoch: [73/100]\tSamples: [365000/500000]\tTrain Loss: 2055.2861927734375\tTime: 0:00:00.754449\n",
            "Epoch: [74/100]\tSamples: [370000/500000]\tTrain Loss: 2055.406008203125\tTime: 0:00:00.798098\n",
            "Epoch: [75/100]\tSamples: [375000/500000]\tTrain Loss: 2055.392163671875\tTime: 0:00:00.790099\n",
            "Epoch: [76/100]\tSamples: [380000/500000]\tTrain Loss: 2055.271073828125\tTime: 0:00:00.801382\n",
            "Epoch: [77/100]\tSamples: [385000/500000]\tTrain Loss: 2055.4245693359376\tTime: 0:00:00.804745\n",
            "Epoch: [78/100]\tSamples: [390000/500000]\tTrain Loss: 2055.390400390625\tTime: 0:00:00.801124\n",
            "Epoch: [79/100]\tSamples: [395000/500000]\tTrain Loss: 2055.522378125\tTime: 0:00:00.788936\n",
            "Epoch: [80/100]\tSamples: [400000/500000]\tTrain Loss: 2055.3413078125\tTime: 0:00:00.788664\n",
            "Epoch: [81/100]\tSamples: [405000/500000]\tTrain Loss: 2055.399130078125\tTime: 0:00:00.812895\n",
            "Epoch: [82/100]\tSamples: [410000/500000]\tTrain Loss: 2055.2969556640624\tTime: 0:00:00.822814\n",
            "Epoch: [83/100]\tSamples: [415000/500000]\tTrain Loss: 2055.21807109375\tTime: 0:00:00.817517\n",
            "Epoch: [84/100]\tSamples: [420000/500000]\tTrain Loss: 2055.1398794921874\tTime: 0:00:00.795510\n",
            "Epoch: [85/100]\tSamples: [425000/500000]\tTrain Loss: 2055.196750390625\tTime: 0:00:00.788629\n",
            "Epoch: [86/100]\tSamples: [430000/500000]\tTrain Loss: 2055.184553125\tTime: 0:00:00.801385\n",
            "Epoch: [87/100]\tSamples: [435000/500000]\tTrain Loss: 2055.217703125\tTime: 0:00:00.801862\n",
            "Epoch: [88/100]\tSamples: [440000/500000]\tTrain Loss: 2055.04368359375\tTime: 0:00:00.771893\n",
            "Epoch: [89/100]\tSamples: [445000/500000]\tTrain Loss: 2055.141885546875\tTime: 0:00:00.788107\n",
            "Epoch: [90/100]\tSamples: [450000/500000]\tTrain Loss: 2055.22953984375\tTime: 0:00:00.752726\n",
            "Epoch: [91/100]\tSamples: [455000/500000]\tTrain Loss: 2055.1378650390625\tTime: 0:00:00.798416\n",
            "Epoch: [92/100]\tSamples: [460000/500000]\tTrain Loss: 2055.43469453125\tTime: 0:00:00.829610\n",
            "Epoch: [93/100]\tSamples: [465000/500000]\tTrain Loss: 2055.100680859375\tTime: 0:00:00.816191\n",
            "Epoch: [94/100]\tSamples: [470000/500000]\tTrain Loss: 2055.1879734375\tTime: 0:00:00.798258\n",
            "Epoch: [95/100]\tSamples: [475000/500000]\tTrain Loss: 2055.2536703125\tTime: 0:00:00.750320\n",
            "Epoch: [96/100]\tSamples: [480000/500000]\tTrain Loss: 2054.9882828125\tTime: 0:00:00.776327\n",
            "Epoch: [97/100]\tSamples: [485000/500000]\tTrain Loss: 2055.23828984375\tTime: 0:00:00.822032\n",
            "Epoch: [98/100]\tSamples: [490000/500000]\tTrain Loss: 2055.337430078125\tTime: 0:00:00.744701\n",
            "Epoch: [99/100]\tSamples: [495000/500000]\tTrain Loss: 2055.01328203125\tTime: 0:00:00.768296\n",
            "Epoch: [100/100]\tSamples: [500000/500000]\tTrain Loss: 2055.1139828125\tTime: 0:00:00.782005\n"
          ]
        }
      ],
      "source": [
        "cv = CountVectorizer(input='content', lowercase=True, stop_words='english',\n",
        "                      max_df=0.99, min_df=0.01, binary=False)\n",
        "  \n",
        "docs_centr = [\" \".join(documents_centr[i]) for i in np.arange(len(documents_centr))]\n",
        "\n",
        "train_bow_centr = cv.fit_transform(docs_centr)\n",
        "train_bow_centr = train_bow_centr.toarray()\n",
        "\n",
        "idx2token_centr = cv.get_feature_names()\n",
        "input_size_centr = len(idx2token_centr)\n",
        "\n",
        "id2token_centr = {k: v for k, v in zip(range(0, len(idx2token_centr)), idx2token_centr)}\n",
        "\n",
        "train_data_centr = BOWDataset(train_bow_centr, idx2token_centr)\n",
        "\n",
        "avitm_centr = AVITM(input_size=input_size_centr, n_components=10, model_type='prodLDA',\n",
        "              hidden_sizes=(100, 100), activation='softplus', dropout=0.2,\n",
        "              learn_priors=True, batch_size=64, lr=2e-3, momentum=0.99,\n",
        "              solver='adam', num_epochs=100, reduce_on_plateau=False)\n",
        "\n",
        "avitm_centr.fit(train_data_centr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A3_xM2LUSYAN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "61c179ee-c778-431b-fc4c-ebaaa8d9084d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-ec3ae7df-2809-406d-b82a-567530fe8bdd\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>wd62</td>\n",
              "      <td>wd81</td>\n",
              "      <td>wd388</td>\n",
              "      <td>wd475</td>\n",
              "      <td>wd466</td>\n",
              "      <td>wd940</td>\n",
              "      <td>wd948</td>\n",
              "      <td>wd377</td>\n",
              "      <td>wd727</td>\n",
              "      <td>wd965</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>wd654</td>\n",
              "      <td>wd90</td>\n",
              "      <td>wd605</td>\n",
              "      <td>wd562</td>\n",
              "      <td>wd130</td>\n",
              "      <td>wd879</td>\n",
              "      <td>wd682</td>\n",
              "      <td>wd152</td>\n",
              "      <td>wd419</td>\n",
              "      <td>wd714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>wd408</td>\n",
              "      <td>wd351</td>\n",
              "      <td>wd802</td>\n",
              "      <td>wd630</td>\n",
              "      <td>wd263</td>\n",
              "      <td>wd849</td>\n",
              "      <td>wd198</td>\n",
              "      <td>wd94</td>\n",
              "      <td>wd194</td>\n",
              "      <td>wd499</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>wd142</td>\n",
              "      <td>wd145</td>\n",
              "      <td>wd687</td>\n",
              "      <td>wd718</td>\n",
              "      <td>wd19</td>\n",
              "      <td>wd240</td>\n",
              "      <td>wd338</td>\n",
              "      <td>wd686</td>\n",
              "      <td>wd651</td>\n",
              "      <td>wd191</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>wd376</td>\n",
              "      <td>wd818</td>\n",
              "      <td>wd222</td>\n",
              "      <td>wd548</td>\n",
              "      <td>wd892</td>\n",
              "      <td>wd439</td>\n",
              "      <td>wd863</td>\n",
              "      <td>wd977</td>\n",
              "      <td>wd783</td>\n",
              "      <td>wd511</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>wd826</td>\n",
              "      <td>wd634</td>\n",
              "      <td>wd935</td>\n",
              "      <td>wd467</td>\n",
              "      <td>wd567</td>\n",
              "      <td>wd197</td>\n",
              "      <td>wd356</td>\n",
              "      <td>wd369</td>\n",
              "      <td>wd373</td>\n",
              "      <td>wd836</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>wd543</td>\n",
              "      <td>wd298</td>\n",
              "      <td>wd256</td>\n",
              "      <td>wd319</td>\n",
              "      <td>wd616</td>\n",
              "      <td>wd342</td>\n",
              "      <td>wd549</td>\n",
              "      <td>wd281</td>\n",
              "      <td>wd454</td>\n",
              "      <td>wd114</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>wd220</td>\n",
              "      <td>wd40</td>\n",
              "      <td>wd778</td>\n",
              "      <td>wd234</td>\n",
              "      <td>wd257</td>\n",
              "      <td>wd56</td>\n",
              "      <td>wd798</td>\n",
              "      <td>wd516</td>\n",
              "      <td>wd574</td>\n",
              "      <td>wd379</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>wd239</td>\n",
              "      <td>wd200</td>\n",
              "      <td>wd968</td>\n",
              "      <td>wd392</td>\n",
              "      <td>wd603</td>\n",
              "      <td>wd618</td>\n",
              "      <td>wd595</td>\n",
              "      <td>wd741</td>\n",
              "      <td>wd769</td>\n",
              "      <td>wd370</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>wd436</td>\n",
              "      <td>wd292</td>\n",
              "      <td>wd425</td>\n",
              "      <td>wd906</td>\n",
              "      <td>wd921</td>\n",
              "      <td>wd905</td>\n",
              "      <td>wd315</td>\n",
              "      <td>wd319</td>\n",
              "      <td>wd755</td>\n",
              "      <td>wd526</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ec3ae7df-2809-406d-b82a-567530fe8bdd')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ec3ae7df-2809-406d-b82a-567530fe8bdd button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ec3ae7df-2809-406d-b82a-567530fe8bdd');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "       0      1      2      3      4      5      6      7      8      9\n",
              "0   wd62   wd81  wd388  wd475  wd466  wd940  wd948  wd377  wd727  wd965\n",
              "1  wd654   wd90  wd605  wd562  wd130  wd879  wd682  wd152  wd419  wd714\n",
              "2  wd408  wd351  wd802  wd630  wd263  wd849  wd198   wd94  wd194  wd499\n",
              "3  wd142  wd145  wd687  wd718   wd19  wd240  wd338  wd686  wd651  wd191\n",
              "4  wd376  wd818  wd222  wd548  wd892  wd439  wd863  wd977  wd783  wd511\n",
              "5  wd826  wd634  wd935  wd467  wd567  wd197  wd356  wd369  wd373  wd836\n",
              "6  wd543  wd298  wd256  wd319  wd616  wd342  wd549  wd281  wd454  wd114\n",
              "7  wd220   wd40  wd778  wd234  wd257   wd56  wd798  wd516  wd574  wd379\n",
              "8  wd239  wd200  wd968  wd392  wd603  wd618  wd595  wd741  wd769  wd370\n",
              "9  wd436  wd292  wd425  wd906  wd921  wd905  wd315  wd319  wd755  wd526"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "topics_centr = pd.DataFrame(avitm_centr.get_topics(10)).T\n",
        "topics_centr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z1w7cb9r7QAl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43dcbfff-dbb2-4064-ab98-71a7ce4e2f6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Sampling: [5/5]: : 5it [00:01,  2.58it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5000, 10)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "doc_topic_centr = get_doc_topic_distribution(avitm_centr, train_data_centr, n_samples=5) # get all the topic predictions\n",
        "print(doc_topic_centr.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YHry56Bz7sbV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52b3f65a-d0b7-4796-ea90-fe3d3bf33bae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.00091444 0.0009958  0.00101818 ... 0.00104175 0.00099569 0.        ]\n",
            " [0.00103641 0.001063   0.00094639 ... 0.0009269  0.00109994 0.        ]\n",
            " [0.00108066 0.00099681 0.00103892 ... 0.00107284 0.00103973 0.        ]\n",
            " ...\n",
            " [0.00093105 0.00105715 0.00104899 ... 0.00107849 0.00091129 0.        ]\n",
            " [0.00103928 0.00109128 0.00095818 ... 0.00099077 0.00097624 0.        ]\n",
            " [0.00089263 0.00103244 0.00098212 ... 0.00091502 0.00095436 0.        ]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0000000000000002"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "w_t_distrib_centr = np.zeros((10,vocab_size), dtype=np.float64) # vocab_size = 10000\n",
        "wd = get_topic_word_distribution(avitm_centr)\n",
        "for i in np.arange(10):\n",
        "  for idx, word in id2token_centr.items():\n",
        "    for j in np.arange(len(all_words)):\n",
        "      if all_words[j] == word:\n",
        "        w_t_distrib_centr[i,j] = wd[i][idx]\n",
        "        break\n",
        "sum_of_rows = w_t_distrib_centr.sum(axis=1)\n",
        "w_t_distrib_centr_norm = w_t_distrib_centr / sum_of_rows[:, np.newaxis]\n",
        "print(w_t_distrib_centr_norm)\n",
        "sum(w_t_distrib_centr_norm[8,:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usiERbR-8Roe"
      },
      "source": [
        "# 4. Get similarity through Frobenius"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ng1ykc1A9wkn"
      },
      "outputs": [],
      "source": [
        "doc_topic_centr_all = []\n",
        "doc_topic_centr_all.append(doc_topic_centr[0:1000,:])\n",
        "doc_topic_centr_all.append(doc_topic_centr[1000:2000,:])\n",
        "doc_topic_centr_all.append(doc_topic_centr[2000:3000,:])\n",
        "doc_topic_centr_all.append(doc_topic_centr[3000:4000,:])\n",
        "doc_topic_centr_all.append(doc_topic_centr[4000:5000,:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-NR5xZgQ8S8W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "198af184-ca8e-4ac1-e132-b17abe33bf79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NODE 0\n",
            "GT vs inferred in node: 153.93371261086259\n",
            "GT vs centralized in node 162.4884491174411\n",
            "***************************************************************\n",
            "NODE 1\n",
            "GT vs inferred in node: 157.61478251742997\n",
            "GT vs centralized in node 161.74215271331846\n",
            "***************************************************************\n",
            "NODE 2\n",
            "GT vs inferred in node: 156.59530146464317\n",
            "GT vs centralized in node 163.66871170199173\n",
            "***************************************************************\n",
            "NODE 3\n",
            "GT vs inferred in node: 156.93074450124902\n",
            "GT vs centralized in node 162.0151048700031\n",
            "***************************************************************\n",
            "NODE 4\n",
            "GT vs inferred in node: 160.55517732808175\n",
            "GT vs centralized in node 167.3481261164559\n",
            "***************************************************************\n"
          ]
        }
      ],
      "source": [
        "for node in np.arange(n_nodes):\n",
        "  # Ground truth in node vs inferred in node\n",
        "  doc_topics_avitm_sqrt_node = np.sqrt(doc_topic_all[node])\n",
        "  similarity_avitm_node = doc_topics_avitm_sqrt_node.dot(doc_topics_avitm_sqrt_node.T)\n",
        "\n",
        "  doc_topics_gt_sqrt_node = np.sqrt(doc_topics_all_gt[node])\n",
        "  similarity_gt = doc_topics_gt_sqrt_node.dot(doc_topics_gt_sqrt_node.T)\n",
        "\n",
        "  diff_sims = similarity_avitm_node - similarity_gt\n",
        "  frobenius_diff_sims_node = np.linalg.norm(diff_sims,'fro')\n",
        "\n",
        "  # Ground truth in node vs centralized (for documents of such a node)\n",
        "  doc_topics_avitm_sqrt_centr_node = np.sqrt(doc_topic_centr_all[node])\n",
        "  similarity_avitm_centr = doc_topics_avitm_sqrt_centr_node.dot(doc_topics_avitm_sqrt_centr_node.T)\n",
        "\n",
        "  diff_sims = similarity_avitm_centr - similarity_gt\n",
        "  frobenius_diff_sims_avg = np.linalg.norm(diff_sims,'fro')\n",
        "\n",
        "  print(\"NODE\", str(node))\n",
        "  print(\"GT vs inferred in node:\", frobenius_diff_sims_node)\n",
        "  print(\"GT vs centralized in node\", frobenius_diff_sims_avg)\n",
        "  print(\"***************************************************************\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uQ6YfIbw-72y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8be02abe-9060-4a8e-9783-587ad6cd0f7d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NODE 0\n",
            "Original vs inferred in node sum max row: 8.886891745058382\n",
            "***************************************************************\n",
            "NODE 1\n",
            "Original vs inferred in node sum max row: 8.885832815192643\n",
            "***************************************************************\n",
            "NODE 2\n",
            "Original vs inferred in node sum max row: 8.886111029120721\n",
            "***************************************************************\n",
            "NODE 3\n",
            "Original vs inferred in node sum max row: 8.886157527884988\n",
            "***************************************************************\n",
            "NODE 4\n",
            "Original vs inferred in node sum max row: 8.886518768235145\n",
            "***************************************************************\n",
            "CENTRALIZED\n",
            "Original vs avg of inferred in nodes sum max row 8.885192909774132\n"
          ]
        }
      ],
      "source": [
        "for node in np.arange(n_nodes):\n",
        "  # GT vs inferred in node\n",
        "  topic_words_gt_sqrt = np.sqrt(topic_vectors)\n",
        "  topic_words_avtim_node_sqrt = np.sqrt(topic_word_all[node])\n",
        "  simmat_t_w = topic_words_gt_sqrt.dot(topic_words_avtim_node_sqrt.T)\n",
        "\n",
        "  simmat_t_w_pd = pd.DataFrame(simmat_t_w)\n",
        "  maxValues_rows = simmat_t_w_pd.max(axis = 1)\n",
        "  max_values_rows_sum = maxValues_rows.sum()\n",
        "\n",
        "  print(\"NODE\", str(node))\n",
        "  print(\"Original vs inferred in node sum max row:\", max_values_rows_sum)\n",
        "  print(\"***************************************************************\")\n",
        "\n",
        "# GT vs centralized\n",
        "topic_words_avtim_centr_sqrt = np.sqrt(w_t_distrib_centr_norm)\n",
        "simmat_t_w = topic_words_gt_sqrt.dot(topic_words_avtim_centr_sqrt.T)\n",
        "\n",
        "simmat_t_w_pd = pd.DataFrame(simmat_t_w)\n",
        "maxValues_rows = simmat_t_w_pd.max(axis = 1)\n",
        "max_values_rows_sum_centr = maxValues_rows.sum()\n",
        "\n",
        "print(\"CENTRALIZED\")\n",
        "print(\"Original vs avg of inferred in nodes sum max row\", max_values_rows_sum_centr)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.imshow(simmat_t_w)\n",
        "plt.colorbar()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "f62uaxhLKIOF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "d5dd3612-a31d-4ad7-9bb5-185852adc430"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Centralized-ProdLDA-vocab-1000_beta-1_prior-0.9.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}