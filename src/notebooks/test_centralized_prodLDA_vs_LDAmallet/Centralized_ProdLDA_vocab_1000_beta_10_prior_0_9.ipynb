{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sib1HSks6Xqv"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from scipy.special import softmax\n",
        "import multiprocessing as mp\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "from sklearn.preprocessing import normalize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CObHMSd6LLz"
      },
      "source": [
        "# Installing ProdLDA\n",
        "**Restart notbook after the installation!!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-VDAzxJA6FK5",
        "outputId": "1c1ee6f2-a295-4725-8dee-65984d90620e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'PyTorchAVITM'...\n",
            "remote: Enumerating objects: 19052, done.\u001b[K\n",
            "remote: Total 19052 (delta 0), reused 0 (delta 0), pack-reused 19052\u001b[K\n",
            "Receiving objects: 100% (19052/19052), 132.62 MiB | 23.45 MiB/s, done.\n",
            "Resolving deltas: 100% (89/89), done.\n",
            "Checking out files: 100% (37/37), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/estebandito22/PyTorchAVITM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6kW5jO66UKj"
      },
      "source": [
        "# 1. Creation of synthetic corpus\n",
        "\n",
        "We consider a scenario with n parties, each of them as an associated corpus.\n",
        "To generate the corpus associated with each of the parties, we consider a common beta distribution (word-topic distribution), but we freeze different topics/ assign different asymmetric Dirichlet priors favoring different topics at the time of generating the document that composes each party's corpus."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twSZ3G0p6d1z"
      },
      "source": [
        "## 1.1. Function for permuting the Dirichlet prior at each node"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tXdkpdrh6Thn"
      },
      "outputs": [],
      "source": [
        "def rotateArray(arr, n, d):\n",
        "    temp = []\n",
        "    i = 0\n",
        "    while (i < d):\n",
        "        temp.append(arr[i])\n",
        "        i = i + 1\n",
        "    i = 0\n",
        "    while (d < n):\n",
        "        arr[i] = arr[d]\n",
        "        i = i + 1\n",
        "        d = d + 1\n",
        "    arr[:] = arr[: i] + temp\n",
        "    return arr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AyFA9eGH6hGH"
      },
      "source": [
        "## 1.2. Topic modeling and node settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5DmfSiuR6iI0",
        "outputId": "9f265f5b-0e24-4be5-da87-43b0d21bb67a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.1, 0.1, 0.1, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9]\n"
          ]
        }
      ],
      "source": [
        "# Topic modeling settings\n",
        "vocab_size = 1000\n",
        "n_topics = 10\n",
        "beta = 10\n",
        "alpha = 5/n_topics\n",
        "n_docs = 1000\n",
        "nwords = (150, 450) #Min and max lengths of the documents\n",
        "\n",
        "# Nodes settings\n",
        "n_nodes = 5\n",
        "frozen_topics = 3\n",
        "dirichlet_symmetric = False\n",
        "prior = (n_topics)*[0.9]\n",
        "prior[0] = prior[1] = prior[2] = 0.1\n",
        "print(prior)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ylo9Vsu6zpX"
      },
      "source": [
        "## 1.3. Topics generation (common for all nodes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uy3AuOSx6qc1",
        "outputId": "f6d6b8e0-b766-499e-836e-730372f3018d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ordered probabilities for the first topic vector:\n",
            "[0.00216189 0.00209201 0.00204932 0.00202167 0.00197597 0.00192813\n",
            " 0.00192633 0.00192144 0.00191688 0.00191116 0.00188827 0.00188267\n",
            " 0.00186135 0.00182845 0.00181846 0.00181357 0.00181267 0.00177286\n",
            " 0.00176281 0.00175662 0.00174947 0.00174723 0.00174637 0.00174494\n",
            " 0.00173748 0.00173353 0.00173063 0.00172758 0.00172637 0.0017228\n",
            " 0.00171088 0.00170778 0.00169424 0.00167567 0.00167456 0.00166255\n",
            " 0.0016607  0.00165041 0.00164919 0.00163921 0.00163234 0.00162723\n",
            " 0.00162349 0.00161775 0.00161035 0.0016056  0.00159488 0.00159442\n",
            " 0.00159346 0.00159024 0.00158832 0.00158095 0.00158079 0.00157692\n",
            " 0.00157684 0.00157102 0.00156955 0.00156502 0.00155915 0.00155645\n",
            " 0.00155644 0.00155469 0.00155346 0.00155066 0.0015496  0.0015492\n",
            " 0.00154728 0.00154612 0.00154494 0.00154494 0.00154313 0.00154211\n",
            " 0.00153775 0.00153735 0.00152927 0.00152825 0.00152511 0.00152151\n",
            " 0.00151956 0.00151335 0.00151249 0.00151209 0.00150786 0.00149348\n",
            " 0.00148925 0.00148792 0.00148417 0.00148414 0.00148402 0.00148373\n",
            " 0.00148036 0.00147326 0.00147217 0.00145437 0.00145382 0.00144972\n",
            " 0.00144933 0.00144923 0.00144791 0.00144767 0.00144705 0.0014467\n",
            " 0.00144261 0.00143961 0.00143851 0.00143638 0.0014345  0.00143261\n",
            " 0.00142626 0.00141925 0.00141481 0.00140696 0.00140629 0.00140302\n",
            " 0.00140155 0.00140138 0.00139468 0.00139374 0.00138816 0.00138662\n",
            " 0.00138306 0.00138294 0.00138246 0.00138245 0.00138107 0.00137753\n",
            " 0.00137591 0.00137386 0.00137241 0.00136859 0.00136305 0.00136021\n",
            " 0.00135811 0.00135808 0.00135417 0.00135406 0.00135203 0.00135098\n",
            " 0.0013488  0.00134651 0.00134239 0.00134196 0.00133993 0.00133874\n",
            " 0.00133562 0.00133489 0.00133429 0.00132502 0.00132146 0.00131972\n",
            " 0.00131773 0.00131653 0.0013149  0.00130788 0.00130664 0.00130587\n",
            " 0.00130484 0.00130458 0.00130381 0.00130322 0.00130192 0.00130002\n",
            " 0.00129986 0.00129873 0.00129534 0.00129377 0.00129343 0.00129337\n",
            " 0.00129251 0.00129119 0.00128782 0.00128607 0.00128574 0.00128533\n",
            " 0.0012844  0.0012828  0.00128068 0.00128004 0.00127751 0.00127201\n",
            " 0.00127176 0.00126974 0.00126902 0.00126897 0.00126775 0.00126686\n",
            " 0.00126666 0.00126276 0.0012625  0.00126183 0.00126119 0.00125954\n",
            " 0.00125935 0.00125792 0.00125722 0.00125666 0.00125617 0.00125132\n",
            " 0.00124958 0.00124659 0.00124542 0.00124247 0.00124207 0.00124108\n",
            " 0.00124033 0.00123783 0.00123749 0.00123677 0.00123554 0.00123481\n",
            " 0.00123317 0.00123275 0.00123272 0.00123165 0.00123084 0.00123022\n",
            " 0.00122991 0.0012255  0.00122415 0.00122361 0.00122235 0.00122125\n",
            " 0.00121994 0.00121652 0.00121594 0.00121387 0.00121297 0.00121252\n",
            " 0.00121118 0.00121049 0.00120994 0.00120916 0.00120828 0.00120624\n",
            " 0.00120542 0.00120491 0.00120256 0.0012023  0.00120183 0.00120151\n",
            " 0.00120133 0.00120056 0.00119872 0.00119394 0.00119363 0.00119216\n",
            " 0.00119061 0.00118683 0.00118552 0.00118432 0.00118316 0.00117919\n",
            " 0.00117865 0.00117812 0.00117695 0.00117599 0.00117555 0.00117266\n",
            " 0.00117172 0.00117015 0.00116967 0.00116948 0.00116883 0.00116713\n",
            " 0.00116652 0.00116548 0.00116431 0.00116255 0.00116109 0.0011569\n",
            " 0.0011565  0.00115602 0.00115539 0.00115463 0.00115298 0.00115183\n",
            " 0.00114935 0.00114665 0.0011464  0.00114455 0.00114342 0.00114245\n",
            " 0.00114104 0.00114087 0.00113847 0.00113682 0.00113207 0.00113164\n",
            " 0.00113068 0.0011305  0.00113029 0.00113029 0.00112977 0.00112971\n",
            " 0.00112876 0.00112788 0.00112762 0.00112734 0.00112703 0.00112588\n",
            " 0.00112491 0.00112479 0.00112413 0.0011213  0.00112018 0.00111943\n",
            " 0.00111803 0.00111745 0.00111743 0.00111605 0.00111545 0.00111504\n",
            " 0.00111313 0.00111303 0.00111208 0.00111204 0.00111036 0.00110978\n",
            " 0.00110928 0.00110904 0.00110686 0.00110588 0.00110467 0.0011035\n",
            " 0.00110289 0.001102   0.00110157 0.00110131 0.00110126 0.00110092\n",
            " 0.00110075 0.00110022 0.00110006 0.00109961 0.00109434 0.00109429\n",
            " 0.00109423 0.00109305 0.00109262 0.00109138 0.00108899 0.00108883\n",
            " 0.00108859 0.00108507 0.00108458 0.00108405 0.00108394 0.00108351\n",
            " 0.00108244 0.0010824  0.00108151 0.00108117 0.00107952 0.00107945\n",
            " 0.00107914 0.00107833 0.00107773 0.00107641 0.00107493 0.0010744\n",
            " 0.00107331 0.00107153 0.00107125 0.00107079 0.00107005 0.00106971\n",
            " 0.00106951 0.00106885 0.00106728 0.00106713 0.00106677 0.00106507\n",
            " 0.00106295 0.0010612  0.00106059 0.00106045 0.00105994 0.00105716\n",
            " 0.00105596 0.00105495 0.001054   0.00105397 0.00105387 0.00105361\n",
            " 0.00105253 0.00105227 0.00105179 0.00105113 0.0010509  0.00105067\n",
            " 0.00105057 0.00104944 0.00104936 0.00104775 0.00104755 0.00104728\n",
            " 0.00104707 0.00104649 0.00104477 0.00104311 0.00104303 0.00104219\n",
            " 0.00104206 0.00104107 0.00104085 0.00103891 0.00103859 0.00103738\n",
            " 0.00103713 0.0010355  0.00103534 0.00103467 0.00103234 0.00103092\n",
            " 0.00103089 0.00102695 0.0010268  0.00102657 0.00102606 0.00102597\n",
            " 0.00102537 0.00102485 0.00102439 0.00102401 0.00102306 0.00102217\n",
            " 0.00102149 0.00102146 0.00102114 0.00102008 0.00101846 0.00101806\n",
            " 0.00101682 0.00101626 0.0010155  0.00101537 0.00101448 0.00101244\n",
            " 0.00101167 0.00101065 0.00100757 0.00100585 0.00100388 0.00100336\n",
            " 0.00100217 0.00100164 0.00100072 0.00099968 0.00099787 0.00099749\n",
            " 0.00099739 0.00099738 0.00099713 0.00099671 0.0009966  0.00099612\n",
            " 0.00099483 0.00099348 0.00099324 0.00099312 0.00099181 0.00099158\n",
            " 0.00098994 0.00098986 0.00098948 0.00098942 0.00098866 0.00098844\n",
            " 0.00098805 0.00098711 0.00098532 0.00098295 0.00098263 0.00098227\n",
            " 0.00098182 0.0009818  0.00097895 0.00097889 0.00097809 0.00097704\n",
            " 0.00097698 0.0009767  0.00097631 0.00097283 0.0009706  0.00096732\n",
            " 0.00096697 0.00096611 0.00096422 0.00096373 0.00096243 0.00096168\n",
            " 0.00096074 0.00096034 0.00095754 0.0009541  0.00095378 0.00095374\n",
            " 0.00095354 0.00095336 0.00095071 0.00095051 0.0009505  0.00095027\n",
            " 0.00094974 0.00094944 0.00094937 0.00094868 0.00094799 0.00094787\n",
            " 0.00094677 0.00094502 0.00094441 0.00094156 0.0009414  0.00093916\n",
            " 0.00093844 0.00093832 0.0009378  0.00093767 0.00093685 0.00093683\n",
            " 0.00093529 0.00093433 0.00093413 0.00093159 0.00093022 0.00092984\n",
            " 0.0009293  0.00092749 0.00092682 0.00092662 0.00092619 0.00092388\n",
            " 0.00092242 0.00092224 0.00092155 0.00092099 0.00091956 0.00091889\n",
            " 0.00091828 0.00091796 0.00091718 0.00091528 0.00091496 0.00091453\n",
            " 0.0009131  0.000913   0.00091265 0.0009123  0.00091102 0.00091093\n",
            " 0.0009097  0.00090965 0.00090911 0.00090845 0.00090822 0.0009082\n",
            " 0.00090713 0.00090681 0.00090671 0.00090627 0.00090517 0.00090487\n",
            " 0.00090456 0.00090351 0.00090308 0.00090203 0.00090122 0.00090083\n",
            " 0.00090046 0.00090026 0.00089872 0.00089756 0.00089698 0.00089695\n",
            " 0.00089635 0.0008961  0.00089609 0.00089582 0.00089519 0.00089495\n",
            " 0.00089372 0.00089342 0.00089229 0.00089152 0.00089099 0.00089064\n",
            " 0.00088947 0.00088852 0.00088722 0.00088652 0.00088647 0.00088577\n",
            " 0.00088562 0.00088502 0.00088421 0.00088293 0.00088275 0.00088046\n",
            " 0.0008804  0.00087992 0.00087874 0.00087862 0.00087811 0.000878\n",
            " 0.00087774 0.00087772 0.0008772  0.00087625 0.00087599 0.00087579\n",
            " 0.00087474 0.00087351 0.00087137 0.00087095 0.00086994 0.00086988\n",
            " 0.00086964 0.00086806 0.00086767 0.00086668 0.00086661 0.00086643\n",
            " 0.00086492 0.00086464 0.0008624  0.00086194 0.00086094 0.00086083\n",
            " 0.00086047 0.00086041 0.00086004 0.00085966 0.00085911 0.00085871\n",
            " 0.00085849 0.00085829 0.0008571  0.00085679 0.0008564  0.00085486\n",
            " 0.00085308 0.000853   0.0008521  0.00085195 0.0008516  0.0008498\n",
            " 0.00084955 0.00084744 0.00084529 0.00084469 0.00084215 0.00084089\n",
            " 0.00084014 0.0008391  0.00083897 0.00083804 0.00083722 0.00083511\n",
            " 0.00083428 0.00083363 0.00083281 0.00083242 0.0008316  0.00083138\n",
            " 0.0008311  0.00083077 0.00082955 0.00082922 0.00082832 0.00082785\n",
            " 0.00082746 0.00082641 0.00082635 0.00082626 0.0008262  0.00082504\n",
            " 0.00082502 0.00082385 0.00082374 0.00082345 0.0008233  0.00082308\n",
            " 0.00082285 0.00082284 0.0008228  0.00082172 0.00082134 0.00082126\n",
            " 0.00082075 0.0008203  0.00081993 0.00081931 0.0008184  0.00081794\n",
            " 0.00081781 0.00081731 0.00081713 0.00081619 0.00081254 0.0008117\n",
            " 0.00081094 0.00080995 0.00080947 0.00080924 0.00080848 0.0008082\n",
            " 0.00080473 0.00080444 0.00080349 0.00080231 0.00080222 0.00080109\n",
            " 0.00080095 0.0008007  0.00080019 0.00079996 0.00079933 0.0007991\n",
            " 0.00079898 0.00079784 0.00079597 0.00079566 0.0007947  0.00079439\n",
            " 0.00079439 0.00079185 0.00079174 0.00079072 0.00079036 0.00078986\n",
            " 0.00078725 0.00078694 0.00078498 0.00078476 0.00078392 0.00078199\n",
            " 0.00078199 0.00078158 0.00078108 0.00078079 0.00077867 0.00077723\n",
            " 0.0007771  0.00077704 0.00077692 0.00077663 0.00077586 0.00077535\n",
            " 0.0007738  0.00077171 0.00077048 0.00077036 0.00076937 0.00076812\n",
            " 0.00076761 0.00076732 0.00076646 0.00076618 0.00076501 0.0007649\n",
            " 0.0007647  0.0007645  0.00076379 0.0007614  0.00076136 0.00076096\n",
            " 0.00076001 0.00075983 0.0007595  0.00075942 0.00075871 0.00075865\n",
            " 0.00075741 0.00075665 0.00075587 0.00075502 0.00075501 0.00075444\n",
            " 0.00075368 0.00075305 0.00075228 0.00074948 0.00074937 0.00074901\n",
            " 0.0007486  0.00074839 0.000748   0.00074795 0.00074794 0.0007469\n",
            " 0.00074678 0.00074627 0.00074607 0.00074592 0.00074584 0.00074522\n",
            " 0.00074516 0.00074459 0.00074454 0.00074411 0.00074225 0.00073987\n",
            " 0.0007394  0.00073928 0.00073869 0.00073747 0.00073667 0.00073651\n",
            " 0.00073561 0.00073521 0.00073397 0.00073331 0.00073263 0.0007311\n",
            " 0.00072977 0.00072953 0.00072945 0.00072939 0.0007291  0.00072683\n",
            " 0.00072625 0.00072513 0.00072439 0.0007228  0.00072163 0.00072015\n",
            " 0.0007181  0.00071695 0.0007162  0.00071177 0.00071049 0.00071027\n",
            " 0.00070995 0.00070826 0.00070811 0.00070709 0.00070677 0.00070666\n",
            " 0.00070489 0.0007045  0.00070264 0.00070259 0.00070154 0.00070006\n",
            " 0.00069686 0.00069597 0.00069541 0.00069443 0.00069363 0.00069311\n",
            " 0.0006901  0.00068869 0.0006876  0.00068131 0.00067892 0.00067524\n",
            " 0.0006751  0.00067236 0.00067088 0.00067064 0.00066785 0.00066562\n",
            " 0.00066541 0.0006651  0.00066373 0.00066303 0.0006628  0.00066279\n",
            " 0.00065886 0.00065877 0.00065743 0.00065699 0.00065692 0.00065431\n",
            " 0.0006541  0.00065368 0.00065084 0.00064954 0.00064947 0.00064909\n",
            " 0.0006477  0.00064472 0.00064261 0.00064008 0.00063884 0.00063868\n",
            " 0.00063776 0.0006377  0.00063626 0.00063607 0.00063458 0.00063166\n",
            " 0.00063102 0.00063023 0.00062923 0.00062885 0.00062831 0.00062714\n",
            " 0.00062583 0.00062567 0.00062549 0.00062513 0.00062378 0.00062214\n",
            " 0.00062174 0.00062086 0.0006202  0.00061863 0.00061588 0.00061435\n",
            " 0.00061406 0.00061333 0.00061043 0.00060718 0.00060706 0.00060641\n",
            " 0.00060629 0.00060555 0.00060516 0.00060496 0.00060044 0.00060023\n",
            " 0.00060006 0.00059904 0.00059722 0.00059674 0.0005966  0.00059628\n",
            " 0.00059626 0.00059358 0.00059193 0.00059121 0.00059048 0.00058892\n",
            " 0.00058643 0.00058597 0.0005844  0.00058039 0.00058001 0.00057601\n",
            " 0.00057377 0.00057154 0.000571   0.00056922 0.00056857 0.0005671\n",
            " 0.0005627  0.00056053 0.00056038 0.00055605 0.00055589 0.00055404\n",
            " 0.00055267 0.00055024 0.00054828 0.00054322 0.00054244 0.00054163\n",
            " 0.00053803 0.00053431 0.00053404 0.00053227 0.00053124 0.00053054\n",
            " 0.00052141 0.00051493 0.00050894 0.00050833 0.00050551 0.00049868\n",
            " 0.00049721 0.00049664 0.00049451 0.00049253 0.00049175 0.00048349\n",
            " 0.00047966 0.00047797 0.00047475 0.0004683  0.00046815 0.00046811\n",
            " 0.00046118 0.00046088 0.00045941 0.00043303 0.00043058 0.00042517\n",
            " 0.00041334 0.00039424 0.00036772 0.00036508 0.00036456 0.00032341\n",
            " 0.00030042 0.00024645 0.0002447  0.00023931]\n",
            "(10, 1000)\n"
          ]
        }
      ],
      "source": [
        "topic_vectors = np.random.dirichlet(vocab_size*[beta], n_topics)\n",
        "print('Ordered probabilities for the first topic vector:')\n",
        "print(np.sort(topic_vectors[0])[::-1])\n",
        "print(topic_vectors.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BaQEe-yD6vl_"
      },
      "source": [
        "## 1.4. Generation of document topic proportions and documents for each node\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N-BziCW56vFL",
        "outputId": "14bbbcb9-2d45-4b67-dd79-3e4c753c1333",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.1, 0.1, 0.1]\n",
            "Ordered probabilities for the first document - node 0 :\n",
            "[4.63289165e-01 1.86301245e-01 1.15191386e-01 1.02139123e-01\n",
            " 7.10482506e-02 3.34788042e-02 2.30668751e-02 3.32753731e-03\n",
            " 2.15394623e-03 3.66883020e-06]\n",
            "Documents of node 0 generated.\n",
            "[0.9, 0.9, 0.9, 0.9, 0.1, 0.1, 0.1, 0.9, 0.9, 0.9]\n",
            "Ordered probabilities for the first document - node 1 :\n",
            "[2.87208720e-01 2.56446226e-01 2.46110799e-01 9.30152947e-02\n",
            " 7.87984547e-02 2.00520730e-02 1.83357838e-02 3.26083535e-05\n",
            " 2.47902792e-08 1.67972926e-08]\n",
            "Documents of node 1 generated.\n",
            "[0.9, 0.1, 0.1, 0.1, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9]\n",
            "Ordered probabilities for the first document - node 2 :\n",
            "[2.78356097e-01 2.59269241e-01 2.05918280e-01 1.83367558e-01\n",
            " 3.07875527e-02 2.89320117e-02 7.75748134e-03 5.52707790e-03\n",
            " 8.46370007e-05 6.28150618e-08]\n",
            "Documents of node 2 generated.\n",
            "[0.1, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.1, 0.1]\n",
            "Ordered probabilities for the first document - node 3 :\n",
            "[3.86126814e-01 2.09424915e-01 1.94697541e-01 9.17539221e-02\n",
            " 6.47600048e-02 3.47091542e-02 1.46316442e-02 3.81726696e-03\n",
            " 7.87379172e-05 1.63942218e-19]\n",
            "Documents of node 3 generated.\n",
            "[0.9, 0.9, 0.9, 0.9, 0.9, 0.1, 0.1, 0.1, 0.9, 0.9]\n",
            "Ordered probabilities for the first document - node 4 :\n",
            "[4.41922734e-01 2.54995880e-01 8.91182201e-02 7.97036716e-02\n",
            " 5.06404402e-02 3.40036153e-02 3.12140944e-02 1.83947255e-02\n",
            " 3.63545612e-06 2.98256566e-06]\n",
            "Documents of node 4 generated.\n"
          ]
        }
      ],
      "source": [
        "doc_topics_all_gt = []\n",
        "documents_all = []\n",
        "z_all = []\n",
        "for i in np.arange(n_nodes):\n",
        "  # Step 2 - generation of document topic proportions for each node\n",
        "  if dirichlet_symmetric:\n",
        "    doc_topics = np.random.dirichlet((n_topics)*[alpha], n_docs)\n",
        "  else:\n",
        "    doc_topics = np.random.dirichlet(prior, n_docs)\n",
        "    prior = rotateArray(prior, len(prior), 3)\n",
        "    print(prior)\n",
        "  print('Ordered probabilities for the first document - node', str(i), ':')\n",
        "  print(np.sort(doc_topics[0])[::-1])\n",
        "  doc_topics_all_gt.append(doc_topics)\n",
        "  # Step 3 - Document generation\n",
        "  documents = [] # Document words\n",
        "  z = [] # Assignments\n",
        "  for docid in np.arange(n_docs):\n",
        "      doc_len = np.random.randint(low=nwords[0], high=nwords[1])\n",
        "      this_doc_words = []\n",
        "      this_doc_assigns = []\n",
        "      for wd_idx in np.arange(doc_len):\n",
        "          tpc = np.nonzero(np.random.multinomial(1, doc_topics[docid]))[0][0]\n",
        "          this_doc_assigns.append(tpc)\n",
        "          word = np.nonzero(np.random.multinomial(1, topic_vectors[tpc]))[0][0]\n",
        "          this_doc_words.append('wd'+str(word))\n",
        "      z.append(this_doc_assigns)\n",
        "      documents.append(this_doc_words)\n",
        "  print(\"Documents of node\", str(i), \"generated.\")\n",
        "  documents_all.append(documents)\n",
        "  z_all.append(z)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HuJUlOIJ69iw"
      },
      "source": [
        "# 2. Preprocessing, generation of training dataset and training of a ProdLDA model at each node"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1XwiP814FZ5U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "707e7803-46c3-4c7b-d305-49a64034773e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "call_signature.png  LICENSE.md  \u001b[0m\u001b[01;34mpytorchavitm\u001b[0m/  README.md  train_abs.py\n",
            "\u001b[01;34mdata\u001b[0m/               \u001b[01;34moutputs\u001b[0m/    \u001b[01;34mPyTorchAVITM\u001b[0m/  setup.py   train.py\n"
          ]
        }
      ],
      "source": [
        "ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "14tnh0ndFpb0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa2a5338-9a62-43aa-80ec-6fa47e4b3fcb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/PyTorchAVITM/pytorchavitm/datasets\n"
          ]
        }
      ],
      "source": [
        "cd /content/PyTorchAVITM/pytorchavitm/datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Is7SF6iQcqA"
      },
      "outputs": [],
      "source": [
        "from bow import BOWDataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k4okSYycQaOK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6cacbedc-eaff-4e1a-d8e5-e4d6c30c3ff1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/PyTorchAVITM\n"
          ]
        }
      ],
      "source": [
        "cd /content/PyTorchAVITM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JrcVglDoQhU0"
      },
      "outputs": [],
      "source": [
        "from pytorchavitm import AVITM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wchhyQ5bDIhK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20ecb59d-f8af-40be-eeab-17bb9b7cc91f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Settings: \n",
            "               N Components: 10\n",
            "               Topic Prior Mean: 0.0\n",
            "               Topic Prior Variance: 0.9\n",
            "               Model Type: prodLDA\n",
            "               Hidden Sizes: (100, 100)\n",
            "               Activation: softplus\n",
            "               Dropout: 0.2\n",
            "               Learn Priors: True\n",
            "               Learning Rate: 0.002\n",
            "               Momentum: 0.99\n",
            "               Reduce On Plateau: False\n",
            "               Save Dir: None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [1/100]\tSamples: [1000/100000]\tTrain Loss: 2232.401390625\tTime: 0:00:00.203384\n",
            "Epoch: [2/100]\tSamples: [2000/100000]\tTrain Loss: 2214.83128125\tTime: 0:00:00.211200\n",
            "Epoch: [3/100]\tSamples: [3000/100000]\tTrain Loss: 2206.3879609375\tTime: 0:00:00.249124\n",
            "Epoch: [4/100]\tSamples: [4000/100000]\tTrain Loss: 2198.043171875\tTime: 0:00:00.217579\n",
            "Epoch: [5/100]\tSamples: [5000/100000]\tTrain Loss: 2193.7402265625\tTime: 0:00:00.226681\n",
            "Epoch: [6/100]\tSamples: [6000/100000]\tTrain Loss: 2188.174109375\tTime: 0:00:00.219552\n",
            "Epoch: [7/100]\tSamples: [7000/100000]\tTrain Loss: 2182.423015625\tTime: 0:00:00.198638\n",
            "Epoch: [8/100]\tSamples: [8000/100000]\tTrain Loss: 2177.3878828125\tTime: 0:00:00.203180\n",
            "Epoch: [9/100]\tSamples: [9000/100000]\tTrain Loss: 2171.9964921875\tTime: 0:00:00.207316\n",
            "Epoch: [10/100]\tSamples: [10000/100000]\tTrain Loss: 2166.787359375\tTime: 0:00:00.218218\n",
            "Epoch: [11/100]\tSamples: [11000/100000]\tTrain Loss: 2161.0753671875\tTime: 0:00:00.213121\n",
            "Epoch: [12/100]\tSamples: [12000/100000]\tTrain Loss: 2159.432078125\tTime: 0:00:00.201885\n",
            "Epoch: [13/100]\tSamples: [13000/100000]\tTrain Loss: 2151.284578125\tTime: 0:00:00.219251\n",
            "Epoch: [14/100]\tSamples: [14000/100000]\tTrain Loss: 2149.391375\tTime: 0:00:00.229949\n",
            "Epoch: [15/100]\tSamples: [15000/100000]\tTrain Loss: 2143.7073515625\tTime: 0:00:00.205333\n",
            "Epoch: [16/100]\tSamples: [16000/100000]\tTrain Loss: 2139.591796875\tTime: 0:00:00.210888\n",
            "Epoch: [17/100]\tSamples: [17000/100000]\tTrain Loss: 2136.8740859375\tTime: 0:00:00.219890\n",
            "Epoch: [18/100]\tSamples: [18000/100000]\tTrain Loss: 2134.305859375\tTime: 0:00:00.205480\n",
            "Epoch: [19/100]\tSamples: [19000/100000]\tTrain Loss: 2129.183765625\tTime: 0:00:00.216068\n",
            "Epoch: [20/100]\tSamples: [20000/100000]\tTrain Loss: 2124.90940625\tTime: 0:00:00.217228\n",
            "Epoch: [21/100]\tSamples: [21000/100000]\tTrain Loss: 2123.0797734375\tTime: 0:00:00.209082\n",
            "Epoch: [22/100]\tSamples: [22000/100000]\tTrain Loss: 2121.4530625\tTime: 0:00:00.212274\n",
            "Epoch: [23/100]\tSamples: [23000/100000]\tTrain Loss: 2119.3061171875\tTime: 0:00:00.215111\n",
            "Epoch: [24/100]\tSamples: [24000/100000]\tTrain Loss: 2116.028453125\tTime: 0:00:00.289492\n",
            "Epoch: [25/100]\tSamples: [25000/100000]\tTrain Loss: 2113.8428515625\tTime: 0:00:00.230882\n",
            "Epoch: [26/100]\tSamples: [26000/100000]\tTrain Loss: 2113.425796875\tTime: 0:00:00.213787\n",
            "Epoch: [27/100]\tSamples: [27000/100000]\tTrain Loss: 2112.7280546875\tTime: 0:00:00.207248\n",
            "Epoch: [28/100]\tSamples: [28000/100000]\tTrain Loss: 2110.7447265625\tTime: 0:00:00.219160\n",
            "Epoch: [29/100]\tSamples: [29000/100000]\tTrain Loss: 2109.3009765625\tTime: 0:00:00.208256\n",
            "Epoch: [30/100]\tSamples: [30000/100000]\tTrain Loss: 2108.6545859375\tTime: 0:00:00.204992\n",
            "Epoch: [31/100]\tSamples: [31000/100000]\tTrain Loss: 2108.126375\tTime: 0:00:00.223210\n",
            "Epoch: [32/100]\tSamples: [32000/100000]\tTrain Loss: 2106.911796875\tTime: 0:00:00.224247\n",
            "Epoch: [33/100]\tSamples: [33000/100000]\tTrain Loss: 2106.6556171875\tTime: 0:00:00.222556\n",
            "Epoch: [34/100]\tSamples: [34000/100000]\tTrain Loss: 2105.7716875\tTime: 0:00:00.210435\n",
            "Epoch: [35/100]\tSamples: [35000/100000]\tTrain Loss: 2105.5675703125\tTime: 0:00:00.213948\n",
            "Epoch: [36/100]\tSamples: [36000/100000]\tTrain Loss: 2104.6429453125\tTime: 0:00:00.219159\n",
            "Epoch: [37/100]\tSamples: [37000/100000]\tTrain Loss: 2104.1845\tTime: 0:00:00.216771\n",
            "Epoch: [38/100]\tSamples: [38000/100000]\tTrain Loss: 2103.8401171875\tTime: 0:00:00.218460\n",
            "Epoch: [39/100]\tSamples: [39000/100000]\tTrain Loss: 2103.26084375\tTime: 0:00:00.220923\n",
            "Epoch: [40/100]\tSamples: [40000/100000]\tTrain Loss: 2103.10784375\tTime: 0:00:00.203998\n",
            "Epoch: [41/100]\tSamples: [41000/100000]\tTrain Loss: 2102.89703125\tTime: 0:00:00.221452\n",
            "Epoch: [42/100]\tSamples: [42000/100000]\tTrain Loss: 2102.5101875\tTime: 0:00:00.219491\n",
            "Epoch: [43/100]\tSamples: [43000/100000]\tTrain Loss: 2102.4427890625\tTime: 0:00:00.211448\n",
            "Epoch: [44/100]\tSamples: [44000/100000]\tTrain Loss: 2102.0939375\tTime: 0:00:00.221595\n",
            "Epoch: [45/100]\tSamples: [45000/100000]\tTrain Loss: 2101.9201953125\tTime: 0:00:00.219509\n",
            "Epoch: [46/100]\tSamples: [46000/100000]\tTrain Loss: 2102.0891328125\tTime: 0:00:00.221338\n",
            "Epoch: [47/100]\tSamples: [47000/100000]\tTrain Loss: 2101.58015625\tTime: 0:00:00.206197\n",
            "Epoch: [48/100]\tSamples: [48000/100000]\tTrain Loss: 2101.4884453125\tTime: 0:00:00.216198\n",
            "Epoch: [49/100]\tSamples: [49000/100000]\tTrain Loss: 2101.4658125\tTime: 0:00:00.217416\n",
            "Epoch: [50/100]\tSamples: [50000/100000]\tTrain Loss: 2101.0925\tTime: 0:00:00.221086\n",
            "Epoch: [51/100]\tSamples: [51000/100000]\tTrain Loss: 2101.34784375\tTime: 0:00:00.227144\n",
            "Epoch: [52/100]\tSamples: [52000/100000]\tTrain Loss: 2101.3046015625\tTime: 0:00:00.224702\n",
            "Epoch: [53/100]\tSamples: [53000/100000]\tTrain Loss: 2100.782015625\tTime: 0:00:00.226441\n",
            "Epoch: [54/100]\tSamples: [54000/100000]\tTrain Loss: 2100.82384375\tTime: 0:00:00.214315\n",
            "Epoch: [55/100]\tSamples: [55000/100000]\tTrain Loss: 2100.75578125\tTime: 0:00:00.214471\n",
            "Epoch: [56/100]\tSamples: [56000/100000]\tTrain Loss: 2100.506625\tTime: 0:00:00.205404\n",
            "Epoch: [57/100]\tSamples: [57000/100000]\tTrain Loss: 2100.5038125\tTime: 0:00:00.211321\n",
            "Epoch: [58/100]\tSamples: [58000/100000]\tTrain Loss: 2100.3731328125\tTime: 0:00:00.203901\n",
            "Epoch: [59/100]\tSamples: [59000/100000]\tTrain Loss: 2100.4645234375\tTime: 0:00:00.217715\n",
            "Epoch: [60/100]\tSamples: [60000/100000]\tTrain Loss: 2100.2667109375\tTime: 0:00:00.227749\n",
            "Epoch: [61/100]\tSamples: [61000/100000]\tTrain Loss: 2099.9835546875\tTime: 0:00:00.206213\n",
            "Epoch: [62/100]\tSamples: [62000/100000]\tTrain Loss: 2100.271765625\tTime: 0:00:00.204935\n",
            "Epoch: [63/100]\tSamples: [63000/100000]\tTrain Loss: 2099.7908046875\tTime: 0:00:00.225596\n",
            "Epoch: [64/100]\tSamples: [64000/100000]\tTrain Loss: 2099.9043125\tTime: 0:00:00.206378\n",
            "Epoch: [65/100]\tSamples: [65000/100000]\tTrain Loss: 2099.762328125\tTime: 0:00:00.236184\n",
            "Epoch: [66/100]\tSamples: [66000/100000]\tTrain Loss: 2099.7650703125\tTime: 0:00:00.208691\n",
            "Epoch: [67/100]\tSamples: [67000/100000]\tTrain Loss: 2099.4686953125\tTime: 0:00:00.223145\n",
            "Epoch: [68/100]\tSamples: [68000/100000]\tTrain Loss: 2099.643609375\tTime: 0:00:00.207674\n",
            "Epoch: [69/100]\tSamples: [69000/100000]\tTrain Loss: 2099.471671875\tTime: 0:00:00.230315\n",
            "Epoch: [70/100]\tSamples: [70000/100000]\tTrain Loss: 2099.5718203125\tTime: 0:00:00.210856\n",
            "Epoch: [71/100]\tSamples: [71000/100000]\tTrain Loss: 2099.7539921875\tTime: 0:00:00.217208\n",
            "Epoch: [72/100]\tSamples: [72000/100000]\tTrain Loss: 2099.3934921875\tTime: 0:00:00.220882\n",
            "Epoch: [73/100]\tSamples: [73000/100000]\tTrain Loss: 2099.389125\tTime: 0:00:00.228359\n",
            "Epoch: [74/100]\tSamples: [74000/100000]\tTrain Loss: 2099.3465078125\tTime: 0:00:00.218483\n",
            "Epoch: [75/100]\tSamples: [75000/100000]\tTrain Loss: 2099.4125703125\tTime: 0:00:00.204693\n",
            "Epoch: [76/100]\tSamples: [76000/100000]\tTrain Loss: 2099.0505234375\tTime: 0:00:00.207128\n",
            "Epoch: [77/100]\tSamples: [77000/100000]\tTrain Loss: 2099.227265625\tTime: 0:00:00.219404\n",
            "Epoch: [78/100]\tSamples: [78000/100000]\tTrain Loss: 2099.2412265625\tTime: 0:00:00.215181\n",
            "Epoch: [79/100]\tSamples: [79000/100000]\tTrain Loss: 2099.0934140625\tTime: 0:00:00.232045\n",
            "Epoch: [80/100]\tSamples: [80000/100000]\tTrain Loss: 2098.7940703125\tTime: 0:00:00.220635\n",
            "Epoch: [81/100]\tSamples: [81000/100000]\tTrain Loss: 2098.6992578125\tTime: 0:00:00.230654\n",
            "Epoch: [82/100]\tSamples: [82000/100000]\tTrain Loss: 2098.8467265625\tTime: 0:00:00.209643\n",
            "Epoch: [83/100]\tSamples: [83000/100000]\tTrain Loss: 2098.9470625\tTime: 0:00:00.231399\n",
            "Epoch: [84/100]\tSamples: [84000/100000]\tTrain Loss: 2098.927765625\tTime: 0:00:00.218217\n",
            "Epoch: [85/100]\tSamples: [85000/100000]\tTrain Loss: 2098.8312265625\tTime: 0:00:00.210542\n",
            "Epoch: [86/100]\tSamples: [86000/100000]\tTrain Loss: 2098.5470625\tTime: 0:00:00.199013\n",
            "Epoch: [87/100]\tSamples: [87000/100000]\tTrain Loss: 2098.6408515625\tTime: 0:00:00.228381\n",
            "Epoch: [88/100]\tSamples: [88000/100000]\tTrain Loss: 2098.78815625\tTime: 0:00:00.224282\n",
            "Epoch: [89/100]\tSamples: [89000/100000]\tTrain Loss: 2098.6370625\tTime: 0:00:00.208392\n",
            "Epoch: [90/100]\tSamples: [90000/100000]\tTrain Loss: 2098.602890625\tTime: 0:00:00.226919\n",
            "Epoch: [91/100]\tSamples: [91000/100000]\tTrain Loss: 2098.7153125\tTime: 0:00:00.220516\n",
            "Epoch: [92/100]\tSamples: [92000/100000]\tTrain Loss: 2098.67475\tTime: 0:00:00.208172\n",
            "Epoch: [93/100]\tSamples: [93000/100000]\tTrain Loss: 2098.644171875\tTime: 0:00:00.215702\n",
            "Epoch: [94/100]\tSamples: [94000/100000]\tTrain Loss: 2098.605203125\tTime: 0:00:00.206255\n",
            "Epoch: [95/100]\tSamples: [95000/100000]\tTrain Loss: 2098.301375\tTime: 0:00:00.222722\n",
            "Epoch: [96/100]\tSamples: [96000/100000]\tTrain Loss: 2098.2824765625\tTime: 0:00:00.203825\n",
            "Epoch: [97/100]\tSamples: [97000/100000]\tTrain Loss: 2098.709484375\tTime: 0:00:00.215568\n",
            "Epoch: [98/100]\tSamples: [98000/100000]\tTrain Loss: 2098.44165625\tTime: 0:00:00.227487\n",
            "Epoch: [99/100]\tSamples: [99000/100000]\tTrain Loss: 2098.495078125\tTime: 0:00:00.215602\n",
            "Epoch: [100/100]\tSamples: [100000/100000]\tTrain Loss: 2098.55303125\tTime: 0:00:00.218848\n",
            "Settings: \n",
            "               N Components: 10\n",
            "               Topic Prior Mean: 0.0\n",
            "               Topic Prior Variance: 0.9\n",
            "               Model Type: prodLDA\n",
            "               Hidden Sizes: (100, 100)\n",
            "               Activation: softplus\n",
            "               Dropout: 0.2\n",
            "               Learn Priors: True\n",
            "               Learning Rate: 0.002\n",
            "               Momentum: 0.99\n",
            "               Reduce On Plateau: False\n",
            "               Save Dir: None\n",
            "Epoch: [1/100]\tSamples: [1000/100000]\tTrain Loss: 2182.002765625\tTime: 0:00:00.222426\n",
            "Epoch: [2/100]\tSamples: [2000/100000]\tTrain Loss: 2167.0623984375\tTime: 0:00:00.214743\n",
            "Epoch: [3/100]\tSamples: [3000/100000]\tTrain Loss: 2158.227171875\tTime: 0:00:00.197160\n",
            "Epoch: [4/100]\tSamples: [4000/100000]\tTrain Loss: 2153.3345859375\tTime: 0:00:00.205260\n",
            "Epoch: [5/100]\tSamples: [5000/100000]\tTrain Loss: 2147.52884375\tTime: 0:00:00.220484\n",
            "Epoch: [6/100]\tSamples: [6000/100000]\tTrain Loss: 2141.3252734375\tTime: 0:00:00.227195\n",
            "Epoch: [7/100]\tSamples: [7000/100000]\tTrain Loss: 2138.04446875\tTime: 0:00:00.206980\n",
            "Epoch: [8/100]\tSamples: [8000/100000]\tTrain Loss: 2132.7932734375\tTime: 0:00:00.222405\n",
            "Epoch: [9/100]\tSamples: [9000/100000]\tTrain Loss: 2128.26071875\tTime: 0:00:00.212679\n",
            "Epoch: [10/100]\tSamples: [10000/100000]\tTrain Loss: 2124.2399140625\tTime: 0:00:00.200118\n",
            "Epoch: [11/100]\tSamples: [11000/100000]\tTrain Loss: 2122.159046875\tTime: 0:00:00.217080\n",
            "Epoch: [12/100]\tSamples: [12000/100000]\tTrain Loss: 2114.6641640625\tTime: 0:00:00.217472\n",
            "Epoch: [13/100]\tSamples: [13000/100000]\tTrain Loss: 2112.6156796875\tTime: 0:00:00.206732\n",
            "Epoch: [14/100]\tSamples: [14000/100000]\tTrain Loss: 2107.9452421875\tTime: 0:00:00.218973\n",
            "Epoch: [15/100]\tSamples: [15000/100000]\tTrain Loss: 2104.86109375\tTime: 0:00:00.210312\n",
            "Epoch: [16/100]\tSamples: [16000/100000]\tTrain Loss: 2099.99903125\tTime: 0:00:00.209134\n",
            "Epoch: [17/100]\tSamples: [17000/100000]\tTrain Loss: 2099.1219296875\tTime: 0:00:00.210443\n",
            "Epoch: [18/100]\tSamples: [18000/100000]\tTrain Loss: 2095.909765625\tTime: 0:00:00.212262\n",
            "Epoch: [19/100]\tSamples: [19000/100000]\tTrain Loss: 2092.3830234375\tTime: 0:00:00.217552\n",
            "Epoch: [20/100]\tSamples: [20000/100000]\tTrain Loss: 2088.3732109375\tTime: 0:00:00.211704\n",
            "Epoch: [21/100]\tSamples: [21000/100000]\tTrain Loss: 2086.6507734375\tTime: 0:00:00.211422\n",
            "Epoch: [22/100]\tSamples: [22000/100000]\tTrain Loss: 2083.25496875\tTime: 0:00:00.210451\n",
            "Epoch: [23/100]\tSamples: [23000/100000]\tTrain Loss: 2080.585\tTime: 0:00:00.197842\n",
            "Epoch: [24/100]\tSamples: [24000/100000]\tTrain Loss: 2080.17321875\tTime: 0:00:00.224845\n",
            "Epoch: [25/100]\tSamples: [25000/100000]\tTrain Loss: 2076.4079375\tTime: 0:00:00.224323\n",
            "Epoch: [26/100]\tSamples: [26000/100000]\tTrain Loss: 2075.46234375\tTime: 0:00:00.204819\n",
            "Epoch: [27/100]\tSamples: [27000/100000]\tTrain Loss: 2075.2438828125\tTime: 0:00:00.211225\n",
            "Epoch: [28/100]\tSamples: [28000/100000]\tTrain Loss: 2072.7796015625\tTime: 0:00:00.215631\n",
            "Epoch: [29/100]\tSamples: [29000/100000]\tTrain Loss: 2072.1003046875\tTime: 0:00:00.216164\n",
            "Epoch: [30/100]\tSamples: [30000/100000]\tTrain Loss: 2070.7764453125\tTime: 0:00:00.219078\n",
            "Epoch: [31/100]\tSamples: [31000/100000]\tTrain Loss: 2069.354171875\tTime: 0:00:00.213642\n",
            "Epoch: [32/100]\tSamples: [32000/100000]\tTrain Loss: 2067.54725\tTime: 0:00:00.214016\n",
            "Epoch: [33/100]\tSamples: [33000/100000]\tTrain Loss: 2066.214984375\tTime: 0:00:00.216876\n",
            "Epoch: [34/100]\tSamples: [34000/100000]\tTrain Loss: 2066.4854453125\tTime: 0:00:00.215535\n",
            "Epoch: [35/100]\tSamples: [35000/100000]\tTrain Loss: 2065.289953125\tTime: 0:00:00.234049\n",
            "Epoch: [36/100]\tSamples: [36000/100000]\tTrain Loss: 2063.9837890625\tTime: 0:00:00.202414\n",
            "Epoch: [37/100]\tSamples: [37000/100000]\tTrain Loss: 2063.4347109375\tTime: 0:00:00.213225\n",
            "Epoch: [38/100]\tSamples: [38000/100000]\tTrain Loss: 2063.4210078125\tTime: 0:00:00.196434\n",
            "Epoch: [39/100]\tSamples: [39000/100000]\tTrain Loss: 2062.738890625\tTime: 0:00:00.215502\n",
            "Epoch: [40/100]\tSamples: [40000/100000]\tTrain Loss: 2061.9465390625\tTime: 0:00:00.206279\n",
            "Epoch: [41/100]\tSamples: [41000/100000]\tTrain Loss: 2061.2039453125\tTime: 0:00:00.210045\n",
            "Epoch: [42/100]\tSamples: [42000/100000]\tTrain Loss: 2060.5054921875\tTime: 0:00:00.221116\n",
            "Epoch: [43/100]\tSamples: [43000/100000]\tTrain Loss: 2059.829125\tTime: 0:00:00.210959\n",
            "Epoch: [44/100]\tSamples: [44000/100000]\tTrain Loss: 2059.49009375\tTime: 0:00:00.214999\n",
            "Epoch: [45/100]\tSamples: [45000/100000]\tTrain Loss: 2058.752515625\tTime: 0:00:00.233888\n",
            "Epoch: [46/100]\tSamples: [46000/100000]\tTrain Loss: 2058.0686875\tTime: 0:00:00.204863\n",
            "Epoch: [47/100]\tSamples: [47000/100000]\tTrain Loss: 2057.8823125\tTime: 0:00:00.220063\n",
            "Epoch: [48/100]\tSamples: [48000/100000]\tTrain Loss: 2057.3388515625\tTime: 0:00:00.213271\n",
            "Epoch: [49/100]\tSamples: [49000/100000]\tTrain Loss: 2057.0549296875\tTime: 0:00:00.221358\n",
            "Epoch: [50/100]\tSamples: [50000/100000]\tTrain Loss: 2056.425078125\tTime: 0:00:00.207224\n",
            "Epoch: [51/100]\tSamples: [51000/100000]\tTrain Loss: 2056.5647265625\tTime: 0:00:00.214444\n",
            "Epoch: [52/100]\tSamples: [52000/100000]\tTrain Loss: 2056.02453125\tTime: 0:00:00.207969\n",
            "Epoch: [53/100]\tSamples: [53000/100000]\tTrain Loss: 2055.96534375\tTime: 0:00:00.207838\n",
            "Epoch: [54/100]\tSamples: [54000/100000]\tTrain Loss: 2055.4635234375\tTime: 0:00:00.217489\n",
            "Epoch: [55/100]\tSamples: [55000/100000]\tTrain Loss: 2055.5124921875\tTime: 0:00:00.216847\n",
            "Epoch: [56/100]\tSamples: [56000/100000]\tTrain Loss: 2055.0443984375\tTime: 0:00:00.218108\n",
            "Epoch: [57/100]\tSamples: [57000/100000]\tTrain Loss: 2054.8519921875\tTime: 0:00:00.215225\n",
            "Epoch: [58/100]\tSamples: [58000/100000]\tTrain Loss: 2054.741015625\tTime: 0:00:00.223034\n",
            "Epoch: [59/100]\tSamples: [59000/100000]\tTrain Loss: 2054.499109375\tTime: 0:00:00.226310\n",
            "Epoch: [60/100]\tSamples: [60000/100000]\tTrain Loss: 2054.2779375\tTime: 0:00:00.203526\n",
            "Epoch: [61/100]\tSamples: [61000/100000]\tTrain Loss: 2054.0464375\tTime: 0:00:00.225125\n",
            "Epoch: [62/100]\tSamples: [62000/100000]\tTrain Loss: 2054.0601796875\tTime: 0:00:00.207216\n",
            "Epoch: [63/100]\tSamples: [63000/100000]\tTrain Loss: 2053.8561171875\tTime: 0:00:00.223796\n",
            "Epoch: [64/100]\tSamples: [64000/100000]\tTrain Loss: 2053.8954140625\tTime: 0:00:00.214565\n",
            "Epoch: [65/100]\tSamples: [65000/100000]\tTrain Loss: 2053.6768203125\tTime: 0:00:00.207376\n",
            "Epoch: [66/100]\tSamples: [66000/100000]\tTrain Loss: 2053.634546875\tTime: 0:00:00.222620\n",
            "Epoch: [67/100]\tSamples: [67000/100000]\tTrain Loss: 2053.1899765625\tTime: 0:00:00.196498\n",
            "Epoch: [68/100]\tSamples: [68000/100000]\tTrain Loss: 2053.5885546875\tTime: 0:00:00.224064\n",
            "Epoch: [69/100]\tSamples: [69000/100000]\tTrain Loss: 2053.495203125\tTime: 0:00:00.226242\n",
            "Epoch: [70/100]\tSamples: [70000/100000]\tTrain Loss: 2053.3153046875\tTime: 0:00:00.220778\n",
            "Epoch: [71/100]\tSamples: [71000/100000]\tTrain Loss: 2052.9599765625\tTime: 0:00:00.205289\n",
            "Epoch: [72/100]\tSamples: [72000/100000]\tTrain Loss: 2053.229625\tTime: 0:00:00.224489\n",
            "Epoch: [73/100]\tSamples: [73000/100000]\tTrain Loss: 2053.1602421875\tTime: 0:00:00.234568\n",
            "Epoch: [74/100]\tSamples: [74000/100000]\tTrain Loss: 2053.2889140625\tTime: 0:00:00.212108\n",
            "Epoch: [75/100]\tSamples: [75000/100000]\tTrain Loss: 2053.193234375\tTime: 0:00:00.215549\n",
            "Epoch: [76/100]\tSamples: [76000/100000]\tTrain Loss: 2052.9446953125\tTime: 0:00:00.206774\n",
            "Epoch: [77/100]\tSamples: [77000/100000]\tTrain Loss: 2052.84546875\tTime: 0:00:00.210038\n",
            "Epoch: [78/100]\tSamples: [78000/100000]\tTrain Loss: 2052.8099140625\tTime: 0:00:00.232243\n",
            "Epoch: [79/100]\tSamples: [79000/100000]\tTrain Loss: 2052.659859375\tTime: 0:00:00.220301\n",
            "Epoch: [80/100]\tSamples: [80000/100000]\tTrain Loss: 2052.5990859375\tTime: 0:00:00.209841\n",
            "Epoch: [81/100]\tSamples: [81000/100000]\tTrain Loss: 2052.9263125\tTime: 0:00:00.220076\n",
            "Epoch: [82/100]\tSamples: [82000/100000]\tTrain Loss: 2052.3470703125\tTime: 0:00:00.230689\n",
            "Epoch: [83/100]\tSamples: [83000/100000]\tTrain Loss: 2052.756421875\tTime: 0:00:00.216068\n",
            "Epoch: [84/100]\tSamples: [84000/100000]\tTrain Loss: 2052.808578125\tTime: 0:00:00.210302\n",
            "Epoch: [85/100]\tSamples: [85000/100000]\tTrain Loss: 2052.5634765625\tTime: 0:00:00.208035\n",
            "Epoch: [86/100]\tSamples: [86000/100000]\tTrain Loss: 2052.483203125\tTime: 0:00:00.221760\n",
            "Epoch: [87/100]\tSamples: [87000/100000]\tTrain Loss: 2052.6068203125\tTime: 0:00:00.228451\n",
            "Epoch: [88/100]\tSamples: [88000/100000]\tTrain Loss: 2052.7768203125\tTime: 0:00:00.214138\n",
            "Epoch: [89/100]\tSamples: [89000/100000]\tTrain Loss: 2052.724703125\tTime: 0:00:00.214492\n",
            "Epoch: [90/100]\tSamples: [90000/100000]\tTrain Loss: 2052.441796875\tTime: 0:00:00.211166\n",
            "Epoch: [91/100]\tSamples: [91000/100000]\tTrain Loss: 2052.7013515625\tTime: 0:00:00.210936\n",
            "Epoch: [92/100]\tSamples: [92000/100000]\tTrain Loss: 2052.351828125\tTime: 0:00:00.217301\n",
            "Epoch: [93/100]\tSamples: [93000/100000]\tTrain Loss: 2052.2765\tTime: 0:00:00.215206\n",
            "Epoch: [94/100]\tSamples: [94000/100000]\tTrain Loss: 2052.193390625\tTime: 0:00:00.204130\n",
            "Epoch: [95/100]\tSamples: [95000/100000]\tTrain Loss: 2052.2649296875\tTime: 0:00:00.207036\n",
            "Epoch: [96/100]\tSamples: [96000/100000]\tTrain Loss: 2052.2995625\tTime: 0:00:00.212966\n",
            "Epoch: [97/100]\tSamples: [97000/100000]\tTrain Loss: 2052.4320390625\tTime: 0:00:00.213077\n",
            "Epoch: [98/100]\tSamples: [98000/100000]\tTrain Loss: 2052.59509375\tTime: 0:00:00.222577\n",
            "Epoch: [99/100]\tSamples: [99000/100000]\tTrain Loss: 2052.4564296875\tTime: 0:00:00.215259\n",
            "Epoch: [100/100]\tSamples: [100000/100000]\tTrain Loss: 2052.40209375\tTime: 0:00:00.217542\n",
            "Settings: \n",
            "               N Components: 10\n",
            "               Topic Prior Mean: 0.0\n",
            "               Topic Prior Variance: 0.9\n",
            "               Model Type: prodLDA\n",
            "               Hidden Sizes: (100, 100)\n",
            "               Activation: softplus\n",
            "               Dropout: 0.2\n",
            "               Learn Priors: True\n",
            "               Learning Rate: 0.002\n",
            "               Momentum: 0.99\n",
            "               Reduce On Plateau: False\n",
            "               Save Dir: None\n",
            "Epoch: [1/100]\tSamples: [1000/100000]\tTrain Loss: 2178.9084609375\tTime: 0:00:00.210467\n",
            "Epoch: [2/100]\tSamples: [2000/100000]\tTrain Loss: 2164.3579921875\tTime: 0:00:00.212043\n",
            "Epoch: [3/100]\tSamples: [3000/100000]\tTrain Loss: 2154.9765859375\tTime: 0:00:00.209153\n",
            "Epoch: [4/100]\tSamples: [4000/100000]\tTrain Loss: 2151.7139140625\tTime: 0:00:00.203308\n",
            "Epoch: [5/100]\tSamples: [5000/100000]\tTrain Loss: 2147.2697109375\tTime: 0:00:00.223328\n",
            "Epoch: [6/100]\tSamples: [6000/100000]\tTrain Loss: 2138.7003359375\tTime: 0:00:00.223916\n",
            "Epoch: [7/100]\tSamples: [7000/100000]\tTrain Loss: 2136.12946875\tTime: 0:00:00.204538\n",
            "Epoch: [8/100]\tSamples: [8000/100000]\tTrain Loss: 2132.5068828125\tTime: 0:00:00.209179\n",
            "Epoch: [9/100]\tSamples: [9000/100000]\tTrain Loss: 2127.4901875\tTime: 0:00:00.214492\n",
            "Epoch: [10/100]\tSamples: [10000/100000]\tTrain Loss: 2124.3789375\tTime: 0:00:00.224911\n",
            "Epoch: [11/100]\tSamples: [11000/100000]\tTrain Loss: 2121.5147265625\tTime: 0:00:00.201501\n",
            "Epoch: [12/100]\tSamples: [12000/100000]\tTrain Loss: 2118.2078984375\tTime: 0:00:00.226262\n",
            "Epoch: [13/100]\tSamples: [13000/100000]\tTrain Loss: 2115.3204375\tTime: 0:00:00.234954\n",
            "Epoch: [14/100]\tSamples: [14000/100000]\tTrain Loss: 2111.4148671875\tTime: 0:00:00.222667\n",
            "Epoch: [15/100]\tSamples: [15000/100000]\tTrain Loss: 2108.7115625\tTime: 0:00:00.217577\n",
            "Epoch: [16/100]\tSamples: [16000/100000]\tTrain Loss: 2105.2800078125\tTime: 0:00:00.207940\n",
            "Epoch: [17/100]\tSamples: [17000/100000]\tTrain Loss: 2103.9257421875\tTime: 0:00:00.226792\n",
            "Epoch: [18/100]\tSamples: [18000/100000]\tTrain Loss: 2099.668125\tTime: 0:00:00.205180\n",
            "Epoch: [19/100]\tSamples: [19000/100000]\tTrain Loss: 2098.3030234375\tTime: 0:00:00.235597\n",
            "Epoch: [20/100]\tSamples: [20000/100000]\tTrain Loss: 2094.2533828125\tTime: 0:00:00.213440\n",
            "Epoch: [21/100]\tSamples: [21000/100000]\tTrain Loss: 2090.9226796875\tTime: 0:00:00.220352\n",
            "Epoch: [22/100]\tSamples: [22000/100000]\tTrain Loss: 2088.2450625\tTime: 0:00:00.212208\n",
            "Epoch: [23/100]\tSamples: [23000/100000]\tTrain Loss: 2084.7709140625\tTime: 0:00:00.227507\n",
            "Epoch: [24/100]\tSamples: [24000/100000]\tTrain Loss: 2081.8736640625\tTime: 0:00:00.230803\n",
            "Epoch: [25/100]\tSamples: [25000/100000]\tTrain Loss: 2078.141109375\tTime: 0:00:00.202861\n",
            "Epoch: [26/100]\tSamples: [26000/100000]\tTrain Loss: 2076.882234375\tTime: 0:00:00.217012\n",
            "Epoch: [27/100]\tSamples: [27000/100000]\tTrain Loss: 2072.6922578125\tTime: 0:00:00.222252\n",
            "Epoch: [28/100]\tSamples: [28000/100000]\tTrain Loss: 2070.0778671875\tTime: 0:00:00.225419\n",
            "Epoch: [29/100]\tSamples: [29000/100000]\tTrain Loss: 2067.529390625\tTime: 0:00:00.210899\n",
            "Epoch: [30/100]\tSamples: [30000/100000]\tTrain Loss: 2066.2594921875\tTime: 0:00:00.210273\n",
            "Epoch: [31/100]\tSamples: [31000/100000]\tTrain Loss: 2064.6032890625\tTime: 0:00:00.216031\n",
            "Epoch: [32/100]\tSamples: [32000/100000]\tTrain Loss: 2061.9304609375\tTime: 0:00:00.207008\n",
            "Epoch: [33/100]\tSamples: [33000/100000]\tTrain Loss: 2061.062875\tTime: 0:00:00.226169\n",
            "Epoch: [34/100]\tSamples: [34000/100000]\tTrain Loss: 2059.890375\tTime: 0:00:00.222023\n",
            "Epoch: [35/100]\tSamples: [35000/100000]\tTrain Loss: 2058.3301015625\tTime: 0:00:00.217507\n",
            "Epoch: [36/100]\tSamples: [36000/100000]\tTrain Loss: 2057.555625\tTime: 0:00:00.209043\n",
            "Epoch: [37/100]\tSamples: [37000/100000]\tTrain Loss: 2056.8244453125\tTime: 0:00:00.214462\n",
            "Epoch: [38/100]\tSamples: [38000/100000]\tTrain Loss: 2056.2361484375\tTime: 0:00:00.225069\n",
            "Epoch: [39/100]\tSamples: [39000/100000]\tTrain Loss: 2056.0002109375\tTime: 0:00:00.206157\n",
            "Epoch: [40/100]\tSamples: [40000/100000]\tTrain Loss: 2055.5360703125\tTime: 0:00:00.208404\n",
            "Epoch: [41/100]\tSamples: [41000/100000]\tTrain Loss: 2054.7944453125\tTime: 0:00:00.217123\n",
            "Epoch: [42/100]\tSamples: [42000/100000]\tTrain Loss: 2054.548015625\tTime: 0:00:00.217630\n",
            "Epoch: [43/100]\tSamples: [43000/100000]\tTrain Loss: 2054.133625\tTime: 0:00:00.243360\n",
            "Epoch: [44/100]\tSamples: [44000/100000]\tTrain Loss: 2054.2234453125\tTime: 0:00:00.221297\n",
            "Epoch: [45/100]\tSamples: [45000/100000]\tTrain Loss: 2053.88046875\tTime: 0:00:00.221186\n",
            "Epoch: [46/100]\tSamples: [46000/100000]\tTrain Loss: 2053.6366171875\tTime: 0:00:00.220376\n",
            "Epoch: [47/100]\tSamples: [47000/100000]\tTrain Loss: 2053.46328125\tTime: 0:00:00.232185\n",
            "Epoch: [48/100]\tSamples: [48000/100000]\tTrain Loss: 2053.155359375\tTime: 0:00:00.213969\n",
            "Epoch: [49/100]\tSamples: [49000/100000]\tTrain Loss: 2053.0215703125\tTime: 0:00:00.234925\n",
            "Epoch: [50/100]\tSamples: [50000/100000]\tTrain Loss: 2052.9466328125\tTime: 0:00:00.217099\n",
            "Epoch: [51/100]\tSamples: [51000/100000]\tTrain Loss: 2052.9305\tTime: 0:00:00.214967\n",
            "Epoch: [52/100]\tSamples: [52000/100000]\tTrain Loss: 2052.970265625\tTime: 0:00:00.227839\n",
            "Epoch: [53/100]\tSamples: [53000/100000]\tTrain Loss: 2052.4990234375\tTime: 0:00:00.219232\n",
            "Epoch: [54/100]\tSamples: [54000/100000]\tTrain Loss: 2052.6263359375\tTime: 0:00:00.208930\n",
            "Epoch: [55/100]\tSamples: [55000/100000]\tTrain Loss: 2052.271125\tTime: 0:00:00.210436\n",
            "Epoch: [56/100]\tSamples: [56000/100000]\tTrain Loss: 2052.4550546875\tTime: 0:00:00.209238\n",
            "Epoch: [57/100]\tSamples: [57000/100000]\tTrain Loss: 2052.351296875\tTime: 0:00:00.219951\n",
            "Epoch: [58/100]\tSamples: [58000/100000]\tTrain Loss: 2051.783375\tTime: 0:00:00.214935\n",
            "Epoch: [59/100]\tSamples: [59000/100000]\tTrain Loss: 2051.884359375\tTime: 0:00:00.218173\n",
            "Epoch: [60/100]\tSamples: [60000/100000]\tTrain Loss: 2052.09578125\tTime: 0:00:00.236953\n",
            "Epoch: [61/100]\tSamples: [61000/100000]\tTrain Loss: 2051.833328125\tTime: 0:00:00.231091\n",
            "Epoch: [62/100]\tSamples: [62000/100000]\tTrain Loss: 2051.7398046875\tTime: 0:00:00.205654\n",
            "Epoch: [63/100]\tSamples: [63000/100000]\tTrain Loss: 2051.6391640625\tTime: 0:00:00.229137\n",
            "Epoch: [64/100]\tSamples: [64000/100000]\tTrain Loss: 2051.5753671875\tTime: 0:00:00.220968\n",
            "Epoch: [65/100]\tSamples: [65000/100000]\tTrain Loss: 2051.7745078125\tTime: 0:00:00.214011\n",
            "Epoch: [66/100]\tSamples: [66000/100000]\tTrain Loss: 2051.626953125\tTime: 0:00:00.220341\n",
            "Epoch: [67/100]\tSamples: [67000/100000]\tTrain Loss: 2051.4075234375\tTime: 0:00:00.205341\n",
            "Epoch: [68/100]\tSamples: [68000/100000]\tTrain Loss: 2051.71028125\tTime: 0:00:00.198998\n",
            "Epoch: [69/100]\tSamples: [69000/100000]\tTrain Loss: 2051.5360078125\tTime: 0:00:00.214727\n",
            "Epoch: [70/100]\tSamples: [70000/100000]\tTrain Loss: 2051.33065625\tTime: 0:00:00.234217\n",
            "Epoch: [71/100]\tSamples: [71000/100000]\tTrain Loss: 2051.3726953125\tTime: 0:00:00.230765\n",
            "Epoch: [72/100]\tSamples: [72000/100000]\tTrain Loss: 2051.25915625\tTime: 0:00:00.220136\n",
            "Epoch: [73/100]\tSamples: [73000/100000]\tTrain Loss: 2051.2411953125\tTime: 0:00:00.230631\n",
            "Epoch: [74/100]\tSamples: [74000/100000]\tTrain Loss: 2051.0643359375\tTime: 0:00:00.226684\n",
            "Epoch: [75/100]\tSamples: [75000/100000]\tTrain Loss: 2051.3423984375\tTime: 0:00:00.223500\n",
            "Epoch: [76/100]\tSamples: [76000/100000]\tTrain Loss: 2051.000109375\tTime: 0:00:00.211078\n",
            "Epoch: [77/100]\tSamples: [77000/100000]\tTrain Loss: 2050.95234375\tTime: 0:00:00.224157\n",
            "Epoch: [78/100]\tSamples: [78000/100000]\tTrain Loss: 2050.914171875\tTime: 0:00:00.216495\n",
            "Epoch: [79/100]\tSamples: [79000/100000]\tTrain Loss: 2051.017640625\tTime: 0:00:00.224850\n",
            "Epoch: [80/100]\tSamples: [80000/100000]\tTrain Loss: 2050.917421875\tTime: 0:00:00.225033\n",
            "Epoch: [81/100]\tSamples: [81000/100000]\tTrain Loss: 2050.943640625\tTime: 0:00:00.224165\n",
            "Epoch: [82/100]\tSamples: [82000/100000]\tTrain Loss: 2050.9959375\tTime: 0:00:00.203469\n",
            "Epoch: [83/100]\tSamples: [83000/100000]\tTrain Loss: 2050.9207890625\tTime: 0:00:00.221782\n",
            "Epoch: [84/100]\tSamples: [84000/100000]\tTrain Loss: 2050.97653125\tTime: 0:00:00.210046\n",
            "Epoch: [85/100]\tSamples: [85000/100000]\tTrain Loss: 2050.856984375\tTime: 0:00:00.220376\n",
            "Epoch: [86/100]\tSamples: [86000/100000]\tTrain Loss: 2050.915125\tTime: 0:00:00.219863\n",
            "Epoch: [87/100]\tSamples: [87000/100000]\tTrain Loss: 2050.7213125\tTime: 0:00:00.235162\n",
            "Epoch: [88/100]\tSamples: [88000/100000]\tTrain Loss: 2050.94621875\tTime: 0:00:00.232740\n",
            "Epoch: [89/100]\tSamples: [89000/100000]\tTrain Loss: 2050.76028125\tTime: 0:00:00.235259\n",
            "Epoch: [90/100]\tSamples: [90000/100000]\tTrain Loss: 2050.9083125\tTime: 0:00:00.216215\n",
            "Epoch: [91/100]\tSamples: [91000/100000]\tTrain Loss: 2050.881484375\tTime: 0:00:00.233416\n",
            "Epoch: [92/100]\tSamples: [92000/100000]\tTrain Loss: 2050.7906875\tTime: 0:00:00.222397\n",
            "Epoch: [93/100]\tSamples: [93000/100000]\tTrain Loss: 2050.4128671875\tTime: 0:00:00.198315\n",
            "Epoch: [94/100]\tSamples: [94000/100000]\tTrain Loss: 2050.957765625\tTime: 0:00:00.235269\n",
            "Epoch: [95/100]\tSamples: [95000/100000]\tTrain Loss: 2050.684078125\tTime: 0:00:00.217282\n",
            "Epoch: [96/100]\tSamples: [96000/100000]\tTrain Loss: 2050.7398046875\tTime: 0:00:00.212097\n",
            "Epoch: [97/100]\tSamples: [97000/100000]\tTrain Loss: 2050.7261171875\tTime: 0:00:00.212202\n",
            "Epoch: [98/100]\tSamples: [98000/100000]\tTrain Loss: 2050.534453125\tTime: 0:00:00.217836\n",
            "Epoch: [99/100]\tSamples: [99000/100000]\tTrain Loss: 2050.676984375\tTime: 0:00:00.212191\n",
            "Epoch: [100/100]\tSamples: [100000/100000]\tTrain Loss: 2050.938\tTime: 0:00:00.222490\n",
            "Settings: \n",
            "               N Components: 10\n",
            "               Topic Prior Mean: 0.0\n",
            "               Topic Prior Variance: 0.9\n",
            "               Model Type: prodLDA\n",
            "               Hidden Sizes: (100, 100)\n",
            "               Activation: softplus\n",
            "               Dropout: 0.2\n",
            "               Learn Priors: True\n",
            "               Learning Rate: 0.002\n",
            "               Momentum: 0.99\n",
            "               Reduce On Plateau: False\n",
            "               Save Dir: None\n",
            "Epoch: [1/100]\tSamples: [1000/100000]\tTrain Loss: 2210.2225625\tTime: 0:00:00.204084\n",
            "Epoch: [2/100]\tSamples: [2000/100000]\tTrain Loss: 2196.624953125\tTime: 0:00:00.217881\n",
            "Epoch: [3/100]\tSamples: [3000/100000]\tTrain Loss: 2188.5369765625\tTime: 0:00:00.206366\n",
            "Epoch: [4/100]\tSamples: [4000/100000]\tTrain Loss: 2182.637484375\tTime: 0:00:00.214669\n",
            "Epoch: [5/100]\tSamples: [5000/100000]\tTrain Loss: 2175.8951484375\tTime: 0:00:00.239273\n",
            "Epoch: [6/100]\tSamples: [6000/100000]\tTrain Loss: 2171.2160234375\tTime: 0:00:00.204842\n",
            "Epoch: [7/100]\tSamples: [7000/100000]\tTrain Loss: 2166.85040625\tTime: 0:00:00.209354\n",
            "Epoch: [8/100]\tSamples: [8000/100000]\tTrain Loss: 2163.2416171875\tTime: 0:00:00.226930\n",
            "Epoch: [9/100]\tSamples: [9000/100000]\tTrain Loss: 2158.57428125\tTime: 0:00:00.219610\n",
            "Epoch: [10/100]\tSamples: [10000/100000]\tTrain Loss: 2156.103875\tTime: 0:00:00.203999\n",
            "Epoch: [11/100]\tSamples: [11000/100000]\tTrain Loss: 2152.1323046875\tTime: 0:00:00.210578\n",
            "Epoch: [12/100]\tSamples: [12000/100000]\tTrain Loss: 2149.248640625\tTime: 0:00:00.222081\n",
            "Epoch: [13/100]\tSamples: [13000/100000]\tTrain Loss: 2146.2538125\tTime: 0:00:00.222668\n",
            "Epoch: [14/100]\tSamples: [14000/100000]\tTrain Loss: 2142.83453125\tTime: 0:00:00.219590\n",
            "Epoch: [15/100]\tSamples: [15000/100000]\tTrain Loss: 2139.2670234375\tTime: 0:00:00.232881\n",
            "Epoch: [16/100]\tSamples: [16000/100000]\tTrain Loss: 2136.28809375\tTime: 0:00:00.244939\n",
            "Epoch: [17/100]\tSamples: [17000/100000]\tTrain Loss: 2133.3539765625\tTime: 0:00:00.222836\n",
            "Epoch: [18/100]\tSamples: [18000/100000]\tTrain Loss: 2129.3225234375\tTime: 0:00:00.216024\n",
            "Epoch: [19/100]\tSamples: [19000/100000]\tTrain Loss: 2124.1607578125\tTime: 0:00:00.212428\n",
            "Epoch: [20/100]\tSamples: [20000/100000]\tTrain Loss: 2120.92609375\tTime: 0:00:00.208072\n",
            "Epoch: [21/100]\tSamples: [21000/100000]\tTrain Loss: 2119.430796875\tTime: 0:00:00.217963\n",
            "Epoch: [22/100]\tSamples: [22000/100000]\tTrain Loss: 2115.882265625\tTime: 0:00:00.214400\n",
            "Epoch: [23/100]\tSamples: [23000/100000]\tTrain Loss: 2111.1778046875\tTime: 0:00:00.219081\n",
            "Epoch: [24/100]\tSamples: [24000/100000]\tTrain Loss: 2108.279625\tTime: 0:00:00.217717\n",
            "Epoch: [25/100]\tSamples: [25000/100000]\tTrain Loss: 2105.5849296875\tTime: 0:00:00.214386\n",
            "Epoch: [26/100]\tSamples: [26000/100000]\tTrain Loss: 2103.761359375\tTime: 0:00:00.242569\n",
            "Epoch: [27/100]\tSamples: [27000/100000]\tTrain Loss: 2101.643328125\tTime: 0:00:00.223708\n",
            "Epoch: [28/100]\tSamples: [28000/100000]\tTrain Loss: 2100.05515625\tTime: 0:00:00.207598\n",
            "Epoch: [29/100]\tSamples: [29000/100000]\tTrain Loss: 2097.8371484375\tTime: 0:00:00.209394\n",
            "Epoch: [30/100]\tSamples: [30000/100000]\tTrain Loss: 2096.2905078125\tTime: 0:00:00.233044\n",
            "Epoch: [31/100]\tSamples: [31000/100000]\tTrain Loss: 2095.1369921875\tTime: 0:00:00.213467\n",
            "Epoch: [32/100]\tSamples: [32000/100000]\tTrain Loss: 2093.713\tTime: 0:00:00.215691\n",
            "Epoch: [33/100]\tSamples: [33000/100000]\tTrain Loss: 2093.0230703125\tTime: 0:00:00.225131\n",
            "Epoch: [34/100]\tSamples: [34000/100000]\tTrain Loss: 2092.4100703125\tTime: 0:00:00.222900\n",
            "Epoch: [35/100]\tSamples: [35000/100000]\tTrain Loss: 2090.624375\tTime: 0:00:00.221335\n",
            "Epoch: [36/100]\tSamples: [36000/100000]\tTrain Loss: 2089.8988125\tTime: 0:00:00.232947\n",
            "Epoch: [37/100]\tSamples: [37000/100000]\tTrain Loss: 2089.506765625\tTime: 0:00:00.213458\n",
            "Epoch: [38/100]\tSamples: [38000/100000]\tTrain Loss: 2088.6545625\tTime: 0:00:00.204397\n",
            "Epoch: [39/100]\tSamples: [39000/100000]\tTrain Loss: 2088.4941328125\tTime: 0:00:00.212148\n",
            "Epoch: [40/100]\tSamples: [40000/100000]\tTrain Loss: 2087.89740625\tTime: 0:00:00.222565\n",
            "Epoch: [41/100]\tSamples: [41000/100000]\tTrain Loss: 2087.2893125\tTime: 0:00:00.221329\n",
            "Epoch: [42/100]\tSamples: [42000/100000]\tTrain Loss: 2087.2710859375\tTime: 0:00:00.204266\n",
            "Epoch: [43/100]\tSamples: [43000/100000]\tTrain Loss: 2086.831046875\tTime: 0:00:00.207167\n",
            "Epoch: [44/100]\tSamples: [44000/100000]\tTrain Loss: 2086.313140625\tTime: 0:00:00.228041\n",
            "Epoch: [45/100]\tSamples: [45000/100000]\tTrain Loss: 2086.6936484375\tTime: 0:00:00.211698\n",
            "Epoch: [46/100]\tSamples: [46000/100000]\tTrain Loss: 2085.48221875\tTime: 0:00:00.211632\n",
            "Epoch: [47/100]\tSamples: [47000/100000]\tTrain Loss: 2085.6193828125\tTime: 0:00:00.198457\n",
            "Epoch: [48/100]\tSamples: [48000/100000]\tTrain Loss: 2085.09046875\tTime: 0:00:00.214509\n",
            "Epoch: [49/100]\tSamples: [49000/100000]\tTrain Loss: 2085.37328125\tTime: 0:00:00.215563\n",
            "Epoch: [50/100]\tSamples: [50000/100000]\tTrain Loss: 2084.75803125\tTime: 0:00:00.223302\n",
            "Epoch: [51/100]\tSamples: [51000/100000]\tTrain Loss: 2084.421796875\tTime: 0:00:00.212775\n",
            "Epoch: [52/100]\tSamples: [52000/100000]\tTrain Loss: 2084.30715625\tTime: 0:00:00.231780\n",
            "Epoch: [53/100]\tSamples: [53000/100000]\tTrain Loss: 2084.0936328125\tTime: 0:00:00.212093\n",
            "Epoch: [54/100]\tSamples: [54000/100000]\tTrain Loss: 2083.9618671875\tTime: 0:00:00.231673\n",
            "Epoch: [55/100]\tSamples: [55000/100000]\tTrain Loss: 2083.6513046875\tTime: 0:00:00.222412\n",
            "Epoch: [56/100]\tSamples: [56000/100000]\tTrain Loss: 2083.48975\tTime: 0:00:00.221969\n",
            "Epoch: [57/100]\tSamples: [57000/100000]\tTrain Loss: 2082.9143515625\tTime: 0:00:00.221259\n",
            "Epoch: [58/100]\tSamples: [58000/100000]\tTrain Loss: 2082.4163984375\tTime: 0:00:00.231851\n",
            "Epoch: [59/100]\tSamples: [59000/100000]\tTrain Loss: 2082.5968671875\tTime: 0:00:00.221149\n",
            "Epoch: [60/100]\tSamples: [60000/100000]\tTrain Loss: 2082.5029609375\tTime: 0:00:00.213816\n",
            "Epoch: [61/100]\tSamples: [61000/100000]\tTrain Loss: 2082.1961953125\tTime: 0:00:00.215617\n",
            "Epoch: [62/100]\tSamples: [62000/100000]\tTrain Loss: 2081.9183046875\tTime: 0:00:00.233078\n",
            "Epoch: [63/100]\tSamples: [63000/100000]\tTrain Loss: 2082.076953125\tTime: 0:00:00.225711\n",
            "Epoch: [64/100]\tSamples: [64000/100000]\tTrain Loss: 2082.1329609375\tTime: 0:00:00.217240\n",
            "Epoch: [65/100]\tSamples: [65000/100000]\tTrain Loss: 2081.83546875\tTime: 0:00:00.220629\n",
            "Epoch: [66/100]\tSamples: [66000/100000]\tTrain Loss: 2082.4221015625\tTime: 0:00:00.222445\n",
            "Epoch: [67/100]\tSamples: [67000/100000]\tTrain Loss: 2081.4827421875\tTime: 0:00:00.223140\n",
            "Epoch: [68/100]\tSamples: [68000/100000]\tTrain Loss: 2081.43690625\tTime: 0:00:00.242169\n",
            "Epoch: [69/100]\tSamples: [69000/100000]\tTrain Loss: 2081.36746875\tTime: 0:00:00.213135\n",
            "Epoch: [70/100]\tSamples: [70000/100000]\tTrain Loss: 2081.6115625\tTime: 0:00:00.209941\n",
            "Epoch: [71/100]\tSamples: [71000/100000]\tTrain Loss: 2081.581515625\tTime: 0:00:00.216656\n",
            "Epoch: [72/100]\tSamples: [72000/100000]\tTrain Loss: 2081.5660859375\tTime: 0:00:00.231808\n",
            "Epoch: [73/100]\tSamples: [73000/100000]\tTrain Loss: 2081.1221328125\tTime: 0:00:00.211218\n",
            "Epoch: [74/100]\tSamples: [74000/100000]\tTrain Loss: 2081.265328125\tTime: 0:00:00.207941\n",
            "Epoch: [75/100]\tSamples: [75000/100000]\tTrain Loss: 2081.0570234375\tTime: 0:00:00.212097\n",
            "Epoch: [76/100]\tSamples: [76000/100000]\tTrain Loss: 2081.1581875\tTime: 0:00:00.223473\n",
            "Epoch: [77/100]\tSamples: [77000/100000]\tTrain Loss: 2080.90528125\tTime: 0:00:00.229157\n",
            "Epoch: [78/100]\tSamples: [78000/100000]\tTrain Loss: 2081.035796875\tTime: 0:00:00.212192\n",
            "Epoch: [79/100]\tSamples: [79000/100000]\tTrain Loss: 2081.243015625\tTime: 0:00:00.230332\n",
            "Epoch: [80/100]\tSamples: [80000/100000]\tTrain Loss: 2081.22440625\tTime: 0:00:00.231575\n",
            "Epoch: [81/100]\tSamples: [81000/100000]\tTrain Loss: 2080.896640625\tTime: 0:00:00.216849\n",
            "Epoch: [82/100]\tSamples: [82000/100000]\tTrain Loss: 2080.7881328125\tTime: 0:00:00.219889\n",
            "Epoch: [83/100]\tSamples: [83000/100000]\tTrain Loss: 2081.04209375\tTime: 0:00:00.214124\n",
            "Epoch: [84/100]\tSamples: [84000/100000]\tTrain Loss: 2080.729671875\tTime: 0:00:00.217088\n",
            "Epoch: [85/100]\tSamples: [85000/100000]\tTrain Loss: 2080.82015625\tTime: 0:00:00.213502\n",
            "Epoch: [86/100]\tSamples: [86000/100000]\tTrain Loss: 2081.1164921875\tTime: 0:00:00.240244\n",
            "Epoch: [87/100]\tSamples: [87000/100000]\tTrain Loss: 2080.9784609375\tTime: 0:00:00.219083\n",
            "Epoch: [88/100]\tSamples: [88000/100000]\tTrain Loss: 2080.958890625\tTime: 0:00:00.220673\n",
            "Epoch: [89/100]\tSamples: [89000/100000]\tTrain Loss: 2080.7603125\tTime: 0:00:00.217970\n",
            "Epoch: [90/100]\tSamples: [90000/100000]\tTrain Loss: 2080.57140625\tTime: 0:00:00.219812\n",
            "Epoch: [91/100]\tSamples: [91000/100000]\tTrain Loss: 2080.811796875\tTime: 0:00:00.241872\n",
            "Epoch: [92/100]\tSamples: [92000/100000]\tTrain Loss: 2080.708140625\tTime: 0:00:00.215792\n",
            "Epoch: [93/100]\tSamples: [93000/100000]\tTrain Loss: 2080.627453125\tTime: 0:00:00.223802\n",
            "Epoch: [94/100]\tSamples: [94000/100000]\tTrain Loss: 2080.4698125\tTime: 0:00:00.217250\n",
            "Epoch: [95/100]\tSamples: [95000/100000]\tTrain Loss: 2080.546984375\tTime: 0:00:00.211102\n",
            "Epoch: [96/100]\tSamples: [96000/100000]\tTrain Loss: 2080.6232578125\tTime: 0:00:00.225413\n",
            "Epoch: [97/100]\tSamples: [97000/100000]\tTrain Loss: 2080.95075\tTime: 0:00:00.211489\n",
            "Epoch: [98/100]\tSamples: [98000/100000]\tTrain Loss: 2080.642484375\tTime: 0:00:00.236215\n",
            "Epoch: [99/100]\tSamples: [99000/100000]\tTrain Loss: 2080.6190703125\tTime: 0:00:00.223349\n",
            "Epoch: [100/100]\tSamples: [100000/100000]\tTrain Loss: 2080.6584296875\tTime: 0:00:00.222890\n",
            "Settings: \n",
            "               N Components: 10\n",
            "               Topic Prior Mean: 0.0\n",
            "               Topic Prior Variance: 0.9\n",
            "               Model Type: prodLDA\n",
            "               Hidden Sizes: (100, 100)\n",
            "               Activation: softplus\n",
            "               Dropout: 0.2\n",
            "               Learn Priors: True\n",
            "               Learning Rate: 0.002\n",
            "               Momentum: 0.99\n",
            "               Reduce On Plateau: False\n",
            "               Save Dir: None\n",
            "Epoch: [1/100]\tSamples: [1000/100000]\tTrain Loss: 2215.89896875\tTime: 0:00:00.204924\n",
            "Epoch: [2/100]\tSamples: [2000/100000]\tTrain Loss: 2198.158546875\tTime: 0:00:00.225774\n",
            "Epoch: [3/100]\tSamples: [3000/100000]\tTrain Loss: 2190.571296875\tTime: 0:00:00.220839\n",
            "Epoch: [4/100]\tSamples: [4000/100000]\tTrain Loss: 2185.035109375\tTime: 0:00:00.238790\n",
            "Epoch: [5/100]\tSamples: [5000/100000]\tTrain Loss: 2175.866390625\tTime: 0:00:00.213634\n",
            "Epoch: [6/100]\tSamples: [6000/100000]\tTrain Loss: 2171.999125\tTime: 0:00:00.227485\n",
            "Epoch: [7/100]\tSamples: [7000/100000]\tTrain Loss: 2166.9467109375\tTime: 0:00:00.210483\n",
            "Epoch: [8/100]\tSamples: [8000/100000]\tTrain Loss: 2160.015609375\tTime: 0:00:00.236936\n",
            "Epoch: [9/100]\tSamples: [9000/100000]\tTrain Loss: 2153.1685859375\tTime: 0:00:00.212549\n",
            "Epoch: [10/100]\tSamples: [10000/100000]\tTrain Loss: 2148.948203125\tTime: 0:00:00.207919\n",
            "Epoch: [11/100]\tSamples: [11000/100000]\tTrain Loss: 2142.75834375\tTime: 0:00:00.224285\n",
            "Epoch: [12/100]\tSamples: [12000/100000]\tTrain Loss: 2139.1229375\tTime: 0:00:00.216108\n",
            "Epoch: [13/100]\tSamples: [13000/100000]\tTrain Loss: 2134.4305703125\tTime: 0:00:00.219309\n",
            "Epoch: [14/100]\tSamples: [14000/100000]\tTrain Loss: 2128.116046875\tTime: 0:00:00.219942\n",
            "Epoch: [15/100]\tSamples: [15000/100000]\tTrain Loss: 2124.488625\tTime: 0:00:00.230047\n",
            "Epoch: [16/100]\tSamples: [16000/100000]\tTrain Loss: 2121.225828125\tTime: 0:00:00.221785\n",
            "Epoch: [17/100]\tSamples: [17000/100000]\tTrain Loss: 2117.362125\tTime: 0:00:00.210837\n",
            "Epoch: [18/100]\tSamples: [18000/100000]\tTrain Loss: 2113.883125\tTime: 0:00:00.223351\n",
            "Epoch: [19/100]\tSamples: [19000/100000]\tTrain Loss: 2110.8313828125\tTime: 0:00:00.218585\n",
            "Epoch: [20/100]\tSamples: [20000/100000]\tTrain Loss: 2107.959203125\tTime: 0:00:00.219137\n",
            "Epoch: [21/100]\tSamples: [21000/100000]\tTrain Loss: 2106.2223984375\tTime: 0:00:00.205639\n",
            "Epoch: [22/100]\tSamples: [22000/100000]\tTrain Loss: 2103.3475\tTime: 0:00:00.222120\n",
            "Epoch: [23/100]\tSamples: [23000/100000]\tTrain Loss: 2101.0084296875\tTime: 0:00:00.223749\n",
            "Epoch: [24/100]\tSamples: [24000/100000]\tTrain Loss: 2100.3647734375\tTime: 0:00:00.211948\n",
            "Epoch: [25/100]\tSamples: [25000/100000]\tTrain Loss: 2097.883015625\tTime: 0:00:00.227659\n",
            "Epoch: [26/100]\tSamples: [26000/100000]\tTrain Loss: 2097.3944375\tTime: 0:00:00.222614\n",
            "Epoch: [27/100]\tSamples: [27000/100000]\tTrain Loss: 2095.4788671875\tTime: 0:00:00.224629\n",
            "Epoch: [28/100]\tSamples: [28000/100000]\tTrain Loss: 2094.606203125\tTime: 0:00:00.209083\n",
            "Epoch: [29/100]\tSamples: [29000/100000]\tTrain Loss: 2093.7853125\tTime: 0:00:00.221073\n",
            "Epoch: [30/100]\tSamples: [30000/100000]\tTrain Loss: 2093.015984375\tTime: 0:00:00.224779\n",
            "Epoch: [31/100]\tSamples: [31000/100000]\tTrain Loss: 2092.3448828125\tTime: 0:00:00.213585\n",
            "Epoch: [32/100]\tSamples: [32000/100000]\tTrain Loss: 2091.6858984375\tTime: 0:00:00.229365\n",
            "Epoch: [33/100]\tSamples: [33000/100000]\tTrain Loss: 2090.802859375\tTime: 0:00:00.214744\n",
            "Epoch: [34/100]\tSamples: [34000/100000]\tTrain Loss: 2090.2481640625\tTime: 0:00:00.229945\n",
            "Epoch: [35/100]\tSamples: [35000/100000]\tTrain Loss: 2089.622921875\tTime: 0:00:00.217119\n",
            "Epoch: [36/100]\tSamples: [36000/100000]\tTrain Loss: 2089.1275703125\tTime: 0:00:00.229622\n",
            "Epoch: [37/100]\tSamples: [37000/100000]\tTrain Loss: 2089.1061640625\tTime: 0:00:00.206990\n",
            "Epoch: [38/100]\tSamples: [38000/100000]\tTrain Loss: 2088.7409296875\tTime: 0:00:00.222462\n",
            "Epoch: [39/100]\tSamples: [39000/100000]\tTrain Loss: 2088.2412109375\tTime: 0:00:00.217246\n",
            "Epoch: [40/100]\tSamples: [40000/100000]\tTrain Loss: 2087.9214765625\tTime: 0:00:00.208765\n",
            "Epoch: [41/100]\tSamples: [41000/100000]\tTrain Loss: 2087.81384375\tTime: 0:00:00.234803\n",
            "Epoch: [42/100]\tSamples: [42000/100000]\tTrain Loss: 2087.2905234375\tTime: 0:00:00.215930\n",
            "Epoch: [43/100]\tSamples: [43000/100000]\tTrain Loss: 2087.381640625\tTime: 0:00:00.222511\n",
            "Epoch: [44/100]\tSamples: [44000/100000]\tTrain Loss: 2087.24225\tTime: 0:00:00.221097\n",
            "Epoch: [45/100]\tSamples: [45000/100000]\tTrain Loss: 2087.0626640625\tTime: 0:00:00.225816\n",
            "Epoch: [46/100]\tSamples: [46000/100000]\tTrain Loss: 2086.822921875\tTime: 0:00:00.212582\n",
            "Epoch: [47/100]\tSamples: [47000/100000]\tTrain Loss: 2086.911875\tTime: 0:00:00.214960\n",
            "Epoch: [48/100]\tSamples: [48000/100000]\tTrain Loss: 2086.5165546875\tTime: 0:00:00.215607\n",
            "Epoch: [49/100]\tSamples: [49000/100000]\tTrain Loss: 2086.410609375\tTime: 0:00:00.209733\n",
            "Epoch: [50/100]\tSamples: [50000/100000]\tTrain Loss: 2086.4319453125\tTime: 0:00:00.232011\n",
            "Epoch: [51/100]\tSamples: [51000/100000]\tTrain Loss: 2086.4727265625\tTime: 0:00:00.222941\n",
            "Epoch: [52/100]\tSamples: [52000/100000]\tTrain Loss: 2086.1761640625\tTime: 0:00:00.231435\n",
            "Epoch: [53/100]\tSamples: [53000/100000]\tTrain Loss: 2086.11171875\tTime: 0:00:00.215582\n",
            "Epoch: [54/100]\tSamples: [54000/100000]\tTrain Loss: 2086.1143515625\tTime: 0:00:00.210479\n",
            "Epoch: [55/100]\tSamples: [55000/100000]\tTrain Loss: 2086.0104921875\tTime: 0:00:00.223678\n",
            "Epoch: [56/100]\tSamples: [56000/100000]\tTrain Loss: 2085.8451875\tTime: 0:00:00.221676\n",
            "Epoch: [57/100]\tSamples: [57000/100000]\tTrain Loss: 2085.804\tTime: 0:00:00.209863\n",
            "Epoch: [58/100]\tSamples: [58000/100000]\tTrain Loss: 2085.7988984375\tTime: 0:00:00.232828\n",
            "Epoch: [59/100]\tSamples: [59000/100000]\tTrain Loss: 2085.574921875\tTime: 0:00:00.222083\n",
            "Epoch: [60/100]\tSamples: [60000/100000]\tTrain Loss: 2085.683890625\tTime: 0:00:00.216076\n",
            "Epoch: [61/100]\tSamples: [61000/100000]\tTrain Loss: 2085.599578125\tTime: 0:00:00.226365\n",
            "Epoch: [62/100]\tSamples: [62000/100000]\tTrain Loss: 2085.3793046875\tTime: 0:00:00.225013\n",
            "Epoch: [63/100]\tSamples: [63000/100000]\tTrain Loss: 2085.235\tTime: 0:00:00.216818\n",
            "Epoch: [64/100]\tSamples: [64000/100000]\tTrain Loss: 2085.3205703125\tTime: 0:00:00.216682\n",
            "Epoch: [65/100]\tSamples: [65000/100000]\tTrain Loss: 2085.0372578125\tTime: 0:00:00.227665\n",
            "Epoch: [66/100]\tSamples: [66000/100000]\tTrain Loss: 2085.224484375\tTime: 0:00:00.226085\n",
            "Epoch: [67/100]\tSamples: [67000/100000]\tTrain Loss: 2085.1089140625\tTime: 0:00:00.218557\n",
            "Epoch: [68/100]\tSamples: [68000/100000]\tTrain Loss: 2085.162296875\tTime: 0:00:00.229587\n",
            "Epoch: [69/100]\tSamples: [69000/100000]\tTrain Loss: 2085.1543828125\tTime: 0:00:00.221396\n",
            "Epoch: [70/100]\tSamples: [70000/100000]\tTrain Loss: 2084.8917421875\tTime: 0:00:00.231969\n",
            "Epoch: [71/100]\tSamples: [71000/100000]\tTrain Loss: 2085.0374453125\tTime: 0:00:00.222636\n",
            "Epoch: [72/100]\tSamples: [72000/100000]\tTrain Loss: 2084.8807109375\tTime: 0:00:00.213275\n",
            "Epoch: [73/100]\tSamples: [73000/100000]\tTrain Loss: 2085.20778125\tTime: 0:00:00.225314\n",
            "Epoch: [74/100]\tSamples: [74000/100000]\tTrain Loss: 2085.020375\tTime: 0:00:00.226954\n",
            "Epoch: [75/100]\tSamples: [75000/100000]\tTrain Loss: 2084.412125\tTime: 0:00:00.218253\n",
            "Epoch: [76/100]\tSamples: [76000/100000]\tTrain Loss: 2084.5280703125\tTime: 0:00:00.225285\n",
            "Epoch: [77/100]\tSamples: [77000/100000]\tTrain Loss: 2084.583046875\tTime: 0:00:00.221981\n",
            "Epoch: [78/100]\tSamples: [78000/100000]\tTrain Loss: 2084.556453125\tTime: 0:00:00.238849\n",
            "Epoch: [79/100]\tSamples: [79000/100000]\tTrain Loss: 2084.5931484375\tTime: 0:00:00.216807\n",
            "Epoch: [80/100]\tSamples: [80000/100000]\tTrain Loss: 2084.5312421875\tTime: 0:00:00.218023\n",
            "Epoch: [81/100]\tSamples: [81000/100000]\tTrain Loss: 2084.266046875\tTime: 0:00:00.211617\n",
            "Epoch: [82/100]\tSamples: [82000/100000]\tTrain Loss: 2084.28715625\tTime: 0:00:00.210210\n",
            "Epoch: [83/100]\tSamples: [83000/100000]\tTrain Loss: 2084.1484375\tTime: 0:00:00.231797\n",
            "Epoch: [84/100]\tSamples: [84000/100000]\tTrain Loss: 2084.3679140625\tTime: 0:00:00.226492\n",
            "Epoch: [85/100]\tSamples: [85000/100000]\tTrain Loss: 2084.309484375\tTime: 0:00:00.219102\n",
            "Epoch: [86/100]\tSamples: [86000/100000]\tTrain Loss: 2084.3468671875\tTime: 0:00:00.233043\n",
            "Epoch: [87/100]\tSamples: [87000/100000]\tTrain Loss: 2084.2466640625\tTime: 0:00:00.227555\n",
            "Epoch: [88/100]\tSamples: [88000/100000]\tTrain Loss: 2083.9905859375\tTime: 0:00:00.221253\n",
            "Epoch: [89/100]\tSamples: [89000/100000]\tTrain Loss: 2084.3551875\tTime: 0:00:00.225743\n",
            "Epoch: [90/100]\tSamples: [90000/100000]\tTrain Loss: 2084.212921875\tTime: 0:00:00.206903\n",
            "Epoch: [91/100]\tSamples: [91000/100000]\tTrain Loss: 2084.2064375\tTime: 0:00:00.230508\n",
            "Epoch: [92/100]\tSamples: [92000/100000]\tTrain Loss: 2084.342171875\tTime: 0:00:00.243071\n",
            "Epoch: [93/100]\tSamples: [93000/100000]\tTrain Loss: 2084.2021015625\tTime: 0:00:00.215852\n",
            "Epoch: [94/100]\tSamples: [94000/100000]\tTrain Loss: 2084.156421875\tTime: 0:00:00.211243\n",
            "Epoch: [95/100]\tSamples: [95000/100000]\tTrain Loss: 2084.2501953125\tTime: 0:00:00.224896\n",
            "Epoch: [96/100]\tSamples: [96000/100000]\tTrain Loss: 2084.1409765625\tTime: 0:00:00.237658\n",
            "Epoch: [97/100]\tSamples: [97000/100000]\tTrain Loss: 2083.867984375\tTime: 0:00:00.207206\n",
            "Epoch: [98/100]\tSamples: [98000/100000]\tTrain Loss: 2083.931421875\tTime: 0:00:00.226091\n",
            "Epoch: [99/100]\tSamples: [99000/100000]\tTrain Loss: 2083.894703125\tTime: 0:00:00.218787\n",
            "Epoch: [100/100]\tSamples: [100000/100000]\tTrain Loss: 2083.711515625\tTime: 0:00:00.229228\n"
          ]
        }
      ],
      "source": [
        "train_datasets = []\n",
        "avitms = []\n",
        "id2tokens = []\n",
        "for corpus_node in documents_all:\n",
        "  cv = CountVectorizer(input='content', lowercase=True, stop_words='english',\n",
        "                      max_df=0.99, min_df=0.01, binary=False)\n",
        "  \n",
        "  docs = [\" \".join(corpus_node[i]) for i in np.arange(len(corpus_node))]\n",
        "\n",
        "  train_bow = cv.fit_transform(docs)\n",
        "  train_bow = train_bow.toarray()\n",
        "\n",
        "  idx2token = cv.get_feature_names()\n",
        "  input_size = len(idx2token)\n",
        "\n",
        "  id2token = {k: v for k, v in zip(range(0, len(idx2token)), idx2token)}\n",
        "  id2tokens.append(id2token)\n",
        "\n",
        "  train_data = BOWDataset(train_bow, idx2token)\n",
        "\n",
        "  avitm = AVITM(input_size=input_size, n_components=10, model_type='prodLDA',\n",
        "                hidden_sizes=(100, 100), activation='softplus', dropout=0.2,\n",
        "                learn_priors=True, batch_size=64, lr=2e-3, momentum=0.99,\n",
        "                solver='adam', num_epochs=100, reduce_on_plateau=False)\n",
        "\n",
        "  avitm.fit(train_data)\n",
        "  avitms.append(avitm)\n",
        "\n",
        "  train_datasets.append(train_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CD0GfefPvJD"
      },
      "source": [
        "# 2.1 Topics at each node"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-X9g0dNGk0bC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "49e1cfc4-f452-4298-9a9a-5c958bd09d74"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-7a4ac75f-8514-43cd-80fb-f46fb34376a3\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>wd594</td>\n",
              "      <td>wd72</td>\n",
              "      <td>wd881</td>\n",
              "      <td>wd321</td>\n",
              "      <td>wd817</td>\n",
              "      <td>wd221</td>\n",
              "      <td>wd666</td>\n",
              "      <td>wd993</td>\n",
              "      <td>wd86</td>\n",
              "      <td>wd756</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>wd321</td>\n",
              "      <td>wd50</td>\n",
              "      <td>wd895</td>\n",
              "      <td>wd993</td>\n",
              "      <td>wd888</td>\n",
              "      <td>wd767</td>\n",
              "      <td>wd0</td>\n",
              "      <td>wd397</td>\n",
              "      <td>wd931</td>\n",
              "      <td>wd234</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>wd355</td>\n",
              "      <td>wd705</td>\n",
              "      <td>wd330</td>\n",
              "      <td>wd368</td>\n",
              "      <td>wd952</td>\n",
              "      <td>wd666</td>\n",
              "      <td>wd474</td>\n",
              "      <td>wd698</td>\n",
              "      <td>wd635</td>\n",
              "      <td>wd797</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>wd446</td>\n",
              "      <td>wd720</td>\n",
              "      <td>wd374</td>\n",
              "      <td>wd518</td>\n",
              "      <td>wd103</td>\n",
              "      <td>wd881</td>\n",
              "      <td>wd386</td>\n",
              "      <td>wd871</td>\n",
              "      <td>wd467</td>\n",
              "      <td>wd195</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>wd163</td>\n",
              "      <td>wd986</td>\n",
              "      <td>wd787</td>\n",
              "      <td>wd123</td>\n",
              "      <td>wd406</td>\n",
              "      <td>wd588</td>\n",
              "      <td>wd579</td>\n",
              "      <td>wd247</td>\n",
              "      <td>wd145</td>\n",
              "      <td>wd521</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>wd986</td>\n",
              "      <td>wd48</td>\n",
              "      <td>wd766</td>\n",
              "      <td>wd652</td>\n",
              "      <td>wd787</td>\n",
              "      <td>wd380</td>\n",
              "      <td>wd37</td>\n",
              "      <td>wd549</td>\n",
              "      <td>wd937</td>\n",
              "      <td>wd203</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>wd813</td>\n",
              "      <td>wd66</td>\n",
              "      <td>wd174</td>\n",
              "      <td>wd588</td>\n",
              "      <td>wd163</td>\n",
              "      <td>wd570</td>\n",
              "      <td>wd787</td>\n",
              "      <td>wd722</td>\n",
              "      <td>wd759</td>\n",
              "      <td>wd91</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>wd684</td>\n",
              "      <td>wd270</td>\n",
              "      <td>wd63</td>\n",
              "      <td>wd394</td>\n",
              "      <td>wd934</td>\n",
              "      <td>wd596</td>\n",
              "      <td>wd227</td>\n",
              "      <td>wd636</td>\n",
              "      <td>wd590</td>\n",
              "      <td>wd458</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>wd369</td>\n",
              "      <td>wd311</td>\n",
              "      <td>wd696</td>\n",
              "      <td>wd546</td>\n",
              "      <td>wd892</td>\n",
              "      <td>wd248</td>\n",
              "      <td>wd606</td>\n",
              "      <td>wd542</td>\n",
              "      <td>wd127</td>\n",
              "      <td>wd372</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>wd621</td>\n",
              "      <td>wd174</td>\n",
              "      <td>wd588</td>\n",
              "      <td>wd890</td>\n",
              "      <td>wd907</td>\n",
              "      <td>wd10</td>\n",
              "      <td>wd787</td>\n",
              "      <td>wd48</td>\n",
              "      <td>wd660</td>\n",
              "      <td>wd163</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7a4ac75f-8514-43cd-80fb-f46fb34376a3')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-7a4ac75f-8514-43cd-80fb-f46fb34376a3 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-7a4ac75f-8514-43cd-80fb-f46fb34376a3');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "       0      1      2      3      4      5      6      7      8      9\n",
              "0  wd594   wd72  wd881  wd321  wd817  wd221  wd666  wd993   wd86  wd756\n",
              "1  wd321   wd50  wd895  wd993  wd888  wd767    wd0  wd397  wd931  wd234\n",
              "2  wd355  wd705  wd330  wd368  wd952  wd666  wd474  wd698  wd635  wd797\n",
              "3  wd446  wd720  wd374  wd518  wd103  wd881  wd386  wd871  wd467  wd195\n",
              "4  wd163  wd986  wd787  wd123  wd406  wd588  wd579  wd247  wd145  wd521\n",
              "5  wd986   wd48  wd766  wd652  wd787  wd380   wd37  wd549  wd937  wd203\n",
              "6  wd813   wd66  wd174  wd588  wd163  wd570  wd787  wd722  wd759   wd91\n",
              "7  wd684  wd270   wd63  wd394  wd934  wd596  wd227  wd636  wd590  wd458\n",
              "8  wd369  wd311  wd696  wd546  wd892  wd248  wd606  wd542  wd127  wd372\n",
              "9  wd621  wd174  wd588  wd890  wd907   wd10  wd787   wd48  wd660  wd163"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "topics_all = []\n",
        "for node in np.arange(n_nodes):\n",
        "  topics = pd.DataFrame(avitm.get_topics(10)).T\n",
        "  topics_all.append(topics)\n",
        "topics_all[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4kezlbmRaRV"
      },
      "source": [
        "## 2.2 Document-topic distributions at each node"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "orpK--hqxQkG"
      },
      "outputs": [],
      "source": [
        "def get_doc_topic_distribution(avitm, dataset, n_samples=20):\n",
        "    avitm.model.eval()\n",
        "\n",
        "    loader = DataLoader(\n",
        "            avitm.train_data, batch_size=avitm.batch_size, shuffle=True,\n",
        "            num_workers=mp.cpu_count())\n",
        "\n",
        "    pbar = tqdm(n_samples, position=0, leave=True)\n",
        "\n",
        "    final_thetas = []\n",
        "    for sample_index in range(n_samples):\n",
        "        with torch.no_grad():\n",
        "            collect_theta = []\n",
        "\n",
        "            for batch_samples in loader:\n",
        "                X = batch_samples['X']\n",
        "\n",
        "                if avitm.USE_CUDA:\n",
        "                  X = X.cuda()\n",
        "\n",
        "                # forward pass\n",
        "                avitm.model.zero_grad()\n",
        "                \n",
        "                with torch.no_grad():\n",
        "                  posterior_mu, posterior_log_sigma = avitm.model.inf_net(X)\n",
        "\n",
        "                  # Generate samples from theta\n",
        "                  theta = F.softmax(\n",
        "                          avitm.model.reparameterize(posterior_mu, posterior_log_sigma), dim=1)\n",
        "                  theta = avitm.model.drop_theta(theta)\n",
        "\n",
        "                collect_theta.extend(theta.cpu().numpy().tolist())\n",
        "\n",
        "            pbar.update(1)\n",
        "            pbar.set_description(\"Sampling: [{}/{}]\".format(sample_index + 1, n_samples))\n",
        "\n",
        "            final_thetas.append(np.array(collect_theta))\n",
        "    pbar.close()\n",
        "    return np.sum(final_thetas, axis=0) / n_samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PXTjce2LlVRG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f26346bf-4719-467e-983f-904769955fd6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Sampling: [5/5]: : 5it [00:00,  5.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document-topic distribution node 0 \n",
            "(1000, 10)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Sampling: [5/5]: : 5it [00:00,  6.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document-topic distribution node 1 \n",
            "(1000, 10)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Sampling: [5/5]: : 5it [00:00,  6.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document-topic distribution node 2 \n",
            "(1000, 10)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Sampling: [5/5]: : 5it [00:00,  6.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document-topic distribution node 3 \n",
            "(1000, 10)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Sampling: [5/5]: : 5it [00:00,  5.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document-topic distribution node 4 \n",
            "(1000, 10)\n"
          ]
        }
      ],
      "source": [
        "doc_topic_all = []\n",
        "for node in np.arange(n_nodes):\n",
        "  doc_topic = get_doc_topic_distribution(avitms[node], train_datasets[node], n_samples=5) # get all the topic predictions\n",
        "  print(\"Document-topic distribution node\", str(node), \"\")\n",
        "  doc_topic_all.append(doc_topic)\n",
        "  print(np.array(doc_topics).shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aEE_kyedRgYv"
      },
      "source": [
        "## 2.3 Word-topic distributions attained at each node"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "douV1llyTM4b"
      },
      "outputs": [],
      "source": [
        "def get_topic_word_distribution(avtim_model):\n",
        "  topic_word_matrix = avtim_model.model.beta.cpu().detach().numpy()\n",
        "  return softmax(topic_word_matrix, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BdM2jxRJUvk3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47170384-6c98-49cf-c9da-fa1a9b715f24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['wd1', 'wd2', 'wd3', 'wd4', 'wd5', 'wd6', 'wd7', 'wd8', 'wd9', 'wd10', 'wd11', 'wd12', 'wd13', 'wd14', 'wd15', 'wd16', 'wd17', 'wd18', 'wd19', 'wd20', 'wd21', 'wd22', 'wd23', 'wd24', 'wd25', 'wd26', 'wd27', 'wd28', 'wd29', 'wd30', 'wd31', 'wd32', 'wd33', 'wd34', 'wd35', 'wd36', 'wd37', 'wd38', 'wd39', 'wd40', 'wd41', 'wd42', 'wd43', 'wd44', 'wd45', 'wd46', 'wd47', 'wd48', 'wd49', 'wd50', 'wd51', 'wd52', 'wd53', 'wd54', 'wd55', 'wd56', 'wd57', 'wd58', 'wd59', 'wd60', 'wd61', 'wd62', 'wd63', 'wd64', 'wd65', 'wd66', 'wd67', 'wd68', 'wd69', 'wd70', 'wd71', 'wd72', 'wd73', 'wd74', 'wd75', 'wd76', 'wd77', 'wd78', 'wd79', 'wd80', 'wd81', 'wd82', 'wd83', 'wd84', 'wd85', 'wd86', 'wd87', 'wd88', 'wd89', 'wd90', 'wd91', 'wd92', 'wd93', 'wd94', 'wd95', 'wd96', 'wd97', 'wd98', 'wd99', 'wd100', 'wd101', 'wd102', 'wd103', 'wd104', 'wd105', 'wd106', 'wd107', 'wd108', 'wd109', 'wd110', 'wd111', 'wd112', 'wd113', 'wd114', 'wd115', 'wd116', 'wd117', 'wd118', 'wd119', 'wd120', 'wd121', 'wd122', 'wd123', 'wd124', 'wd125', 'wd126', 'wd127', 'wd128', 'wd129', 'wd130', 'wd131', 'wd132', 'wd133', 'wd134', 'wd135', 'wd136', 'wd137', 'wd138', 'wd139', 'wd140', 'wd141', 'wd142', 'wd143', 'wd144', 'wd145', 'wd146', 'wd147', 'wd148', 'wd149', 'wd150', 'wd151', 'wd152', 'wd153', 'wd154', 'wd155', 'wd156', 'wd157', 'wd158', 'wd159', 'wd160', 'wd161', 'wd162', 'wd163', 'wd164', 'wd165', 'wd166', 'wd167', 'wd168', 'wd169', 'wd170', 'wd171', 'wd172', 'wd173', 'wd174', 'wd175', 'wd176', 'wd177', 'wd178', 'wd179', 'wd180', 'wd181', 'wd182', 'wd183', 'wd184', 'wd185', 'wd186', 'wd187', 'wd188', 'wd189', 'wd190', 'wd191', 'wd192', 'wd193', 'wd194', 'wd195', 'wd196', 'wd197', 'wd198', 'wd199', 'wd200', 'wd201', 'wd202', 'wd203', 'wd204', 'wd205', 'wd206', 'wd207', 'wd208', 'wd209', 'wd210', 'wd211', 'wd212', 'wd213', 'wd214', 'wd215', 'wd216', 'wd217', 'wd218', 'wd219', 'wd220', 'wd221', 'wd222', 'wd223', 'wd224', 'wd225', 'wd226', 'wd227', 'wd228', 'wd229', 'wd230', 'wd231', 'wd232', 'wd233', 'wd234', 'wd235', 'wd236', 'wd237', 'wd238', 'wd239', 'wd240', 'wd241', 'wd242', 'wd243', 'wd244', 'wd245', 'wd246', 'wd247', 'wd248', 'wd249', 'wd250', 'wd251', 'wd252', 'wd253', 'wd254', 'wd255', 'wd256', 'wd257', 'wd258', 'wd259', 'wd260', 'wd261', 'wd262', 'wd263', 'wd264', 'wd265', 'wd266', 'wd267', 'wd268', 'wd269', 'wd270', 'wd271', 'wd272', 'wd273', 'wd274', 'wd275', 'wd276', 'wd277', 'wd278', 'wd279', 'wd280', 'wd281', 'wd282', 'wd283', 'wd284', 'wd285', 'wd286', 'wd287', 'wd288', 'wd289', 'wd290', 'wd291', 'wd292', 'wd293', 'wd294', 'wd295', 'wd296', 'wd297', 'wd298', 'wd299', 'wd300', 'wd301', 'wd302', 'wd303', 'wd304', 'wd305', 'wd306', 'wd307', 'wd308', 'wd309', 'wd310', 'wd311', 'wd312', 'wd313', 'wd314', 'wd315', 'wd316', 'wd317', 'wd318', 'wd319', 'wd320', 'wd321', 'wd322', 'wd323', 'wd324', 'wd325', 'wd326', 'wd327', 'wd328', 'wd329', 'wd330', 'wd331', 'wd332', 'wd333', 'wd334', 'wd335', 'wd336', 'wd337', 'wd338', 'wd339', 'wd340', 'wd341', 'wd342', 'wd343', 'wd344', 'wd345', 'wd346', 'wd347', 'wd348', 'wd349', 'wd350', 'wd351', 'wd352', 'wd353', 'wd354', 'wd355', 'wd356', 'wd357', 'wd358', 'wd359', 'wd360', 'wd361', 'wd362', 'wd363', 'wd364', 'wd365', 'wd366', 'wd367', 'wd368', 'wd369', 'wd370', 'wd371', 'wd372', 'wd373', 'wd374', 'wd375', 'wd376', 'wd377', 'wd378', 'wd379', 'wd380', 'wd381', 'wd382', 'wd383', 'wd384', 'wd385', 'wd386', 'wd387', 'wd388', 'wd389', 'wd390', 'wd391', 'wd392', 'wd393', 'wd394', 'wd395', 'wd396', 'wd397', 'wd398', 'wd399', 'wd400', 'wd401', 'wd402', 'wd403', 'wd404', 'wd405', 'wd406', 'wd407', 'wd408', 'wd409', 'wd410', 'wd411', 'wd412', 'wd413', 'wd414', 'wd415', 'wd416', 'wd417', 'wd418', 'wd419', 'wd420', 'wd421', 'wd422', 'wd423', 'wd424', 'wd425', 'wd426', 'wd427', 'wd428', 'wd429', 'wd430', 'wd431', 'wd432', 'wd433', 'wd434', 'wd435', 'wd436', 'wd437', 'wd438', 'wd439', 'wd440', 'wd441', 'wd442', 'wd443', 'wd444', 'wd445', 'wd446', 'wd447', 'wd448', 'wd449', 'wd450', 'wd451', 'wd452', 'wd453', 'wd454', 'wd455', 'wd456', 'wd457', 'wd458', 'wd459', 'wd460', 'wd461', 'wd462', 'wd463', 'wd464', 'wd465', 'wd466', 'wd467', 'wd468', 'wd469', 'wd470', 'wd471', 'wd472', 'wd473', 'wd474', 'wd475', 'wd476', 'wd477', 'wd478', 'wd479', 'wd480', 'wd481', 'wd482', 'wd483', 'wd484', 'wd485', 'wd486', 'wd487', 'wd488', 'wd489', 'wd490', 'wd491', 'wd492', 'wd493', 'wd494', 'wd495', 'wd496', 'wd497', 'wd498', 'wd499', 'wd500', 'wd501', 'wd502', 'wd503', 'wd504', 'wd505', 'wd506', 'wd507', 'wd508', 'wd509', 'wd510', 'wd511', 'wd512', 'wd513', 'wd514', 'wd515', 'wd516', 'wd517', 'wd518', 'wd519', 'wd520', 'wd521', 'wd522', 'wd523', 'wd524', 'wd525', 'wd526', 'wd527', 'wd528', 'wd529', 'wd530', 'wd531', 'wd532', 'wd533', 'wd534', 'wd535', 'wd536', 'wd537', 'wd538', 'wd539', 'wd540', 'wd541', 'wd542', 'wd543', 'wd544', 'wd545', 'wd546', 'wd547', 'wd548', 'wd549', 'wd550', 'wd551', 'wd552', 'wd553', 'wd554', 'wd555', 'wd556', 'wd557', 'wd558', 'wd559', 'wd560', 'wd561', 'wd562', 'wd563', 'wd564', 'wd565', 'wd566', 'wd567', 'wd568', 'wd569', 'wd570', 'wd571', 'wd572', 'wd573', 'wd574', 'wd575', 'wd576', 'wd577', 'wd578', 'wd579', 'wd580', 'wd581', 'wd582', 'wd583', 'wd584', 'wd585', 'wd586', 'wd587', 'wd588', 'wd589', 'wd590', 'wd591', 'wd592', 'wd593', 'wd594', 'wd595', 'wd596', 'wd597', 'wd598', 'wd599', 'wd600', 'wd601', 'wd602', 'wd603', 'wd604', 'wd605', 'wd606', 'wd607', 'wd608', 'wd609', 'wd610', 'wd611', 'wd612', 'wd613', 'wd614', 'wd615', 'wd616', 'wd617', 'wd618', 'wd619', 'wd620', 'wd621', 'wd622', 'wd623', 'wd624', 'wd625', 'wd626', 'wd627', 'wd628', 'wd629', 'wd630', 'wd631', 'wd632', 'wd633', 'wd634', 'wd635', 'wd636', 'wd637', 'wd638', 'wd639', 'wd640', 'wd641', 'wd642', 'wd643', 'wd644', 'wd645', 'wd646', 'wd647', 'wd648', 'wd649', 'wd650', 'wd651', 'wd652', 'wd653', 'wd654', 'wd655', 'wd656', 'wd657', 'wd658', 'wd659', 'wd660', 'wd661', 'wd662', 'wd663', 'wd664', 'wd665', 'wd666', 'wd667', 'wd668', 'wd669', 'wd670', 'wd671', 'wd672', 'wd673', 'wd674', 'wd675', 'wd676', 'wd677', 'wd678', 'wd679', 'wd680', 'wd681', 'wd682', 'wd683', 'wd684', 'wd685', 'wd686', 'wd687', 'wd688', 'wd689', 'wd690', 'wd691', 'wd692', 'wd693', 'wd694', 'wd695', 'wd696', 'wd697', 'wd698', 'wd699', 'wd700', 'wd701', 'wd702', 'wd703', 'wd704', 'wd705', 'wd706', 'wd707', 'wd708', 'wd709', 'wd710', 'wd711', 'wd712', 'wd713', 'wd714', 'wd715', 'wd716', 'wd717', 'wd718', 'wd719', 'wd720', 'wd721', 'wd722', 'wd723', 'wd724', 'wd725', 'wd726', 'wd727', 'wd728', 'wd729', 'wd730', 'wd731', 'wd732', 'wd733', 'wd734', 'wd735', 'wd736', 'wd737', 'wd738', 'wd739', 'wd740', 'wd741', 'wd742', 'wd743', 'wd744', 'wd745', 'wd746', 'wd747', 'wd748', 'wd749', 'wd750', 'wd751', 'wd752', 'wd753', 'wd754', 'wd755', 'wd756', 'wd757', 'wd758', 'wd759', 'wd760', 'wd761', 'wd762', 'wd763', 'wd764', 'wd765', 'wd766', 'wd767', 'wd768', 'wd769', 'wd770', 'wd771', 'wd772', 'wd773', 'wd774', 'wd775', 'wd776', 'wd777', 'wd778', 'wd779', 'wd780', 'wd781', 'wd782', 'wd783', 'wd784', 'wd785', 'wd786', 'wd787', 'wd788', 'wd789', 'wd790', 'wd791', 'wd792', 'wd793', 'wd794', 'wd795', 'wd796', 'wd797', 'wd798', 'wd799', 'wd800', 'wd801', 'wd802', 'wd803', 'wd804', 'wd805', 'wd806', 'wd807', 'wd808', 'wd809', 'wd810', 'wd811', 'wd812', 'wd813', 'wd814', 'wd815', 'wd816', 'wd817', 'wd818', 'wd819', 'wd820', 'wd821', 'wd822', 'wd823', 'wd824', 'wd825', 'wd826', 'wd827', 'wd828', 'wd829', 'wd830', 'wd831', 'wd832', 'wd833', 'wd834', 'wd835', 'wd836', 'wd837', 'wd838', 'wd839', 'wd840', 'wd841', 'wd842', 'wd843', 'wd844', 'wd845', 'wd846', 'wd847', 'wd848', 'wd849', 'wd850', 'wd851', 'wd852', 'wd853', 'wd854', 'wd855', 'wd856', 'wd857', 'wd858', 'wd859', 'wd860', 'wd861', 'wd862', 'wd863', 'wd864', 'wd865', 'wd866', 'wd867', 'wd868', 'wd869', 'wd870', 'wd871', 'wd872', 'wd873', 'wd874', 'wd875', 'wd876', 'wd877', 'wd878', 'wd879', 'wd880', 'wd881', 'wd882', 'wd883', 'wd884', 'wd885', 'wd886', 'wd887', 'wd888', 'wd889', 'wd890', 'wd891', 'wd892', 'wd893', 'wd894', 'wd895', 'wd896', 'wd897', 'wd898', 'wd899', 'wd900', 'wd901', 'wd902', 'wd903', 'wd904', 'wd905', 'wd906', 'wd907', 'wd908', 'wd909', 'wd910', 'wd911', 'wd912', 'wd913', 'wd914', 'wd915', 'wd916', 'wd917', 'wd918', 'wd919', 'wd920', 'wd921', 'wd922', 'wd923', 'wd924', 'wd925', 'wd926', 'wd927', 'wd928', 'wd929', 'wd930', 'wd931', 'wd932', 'wd933', 'wd934', 'wd935', 'wd936', 'wd937', 'wd938', 'wd939', 'wd940', 'wd941', 'wd942', 'wd943', 'wd944', 'wd945', 'wd946', 'wd947', 'wd948', 'wd949', 'wd950', 'wd951', 'wd952', 'wd953', 'wd954', 'wd955', 'wd956', 'wd957', 'wd958', 'wd959', 'wd960', 'wd961', 'wd962', 'wd963', 'wd964', 'wd965', 'wd966', 'wd967', 'wd968', 'wd969', 'wd970', 'wd971', 'wd972', 'wd973', 'wd974', 'wd975', 'wd976', 'wd977', 'wd978', 'wd979', 'wd980', 'wd981', 'wd982', 'wd983', 'wd984', 'wd985', 'wd986', 'wd987', 'wd988', 'wd989', 'wd990', 'wd991', 'wd992', 'wd993', 'wd994', 'wd995', 'wd996', 'wd997', 'wd998', 'wd999', 'wd1000']\n"
          ]
        }
      ],
      "source": [
        "all_words = []\n",
        "for word in np.arange(vocab_size+1):\n",
        "  if word > 0:\n",
        "    all_words.append('wd'+str(word))\n",
        "print(all_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MJfQe7PdUmkC"
      },
      "outputs": [],
      "source": [
        "topic_word_all = []\n",
        "for node in np.arange(n_nodes):\n",
        "  w_t_distrib = np.zeros((10,vocab_size), dtype=np.float64) \n",
        "  wd = get_topic_word_distribution(avitms[node])\n",
        "  for i in np.arange(10):\n",
        "    for idx, word in id2tokens[node].items():\n",
        "      for j in np.arange(len(all_words)):\n",
        "        if all_words[j] == word:\n",
        "          w_t_distrib[i,j] = wd[i][idx]\n",
        "          break\n",
        "  sum_of_rows = w_t_distrib.sum(axis=1)\n",
        "  normalized_array = w_t_distrib / sum_of_rows[:, np.newaxis]\n",
        "  topic_word_all.append(normalized_array)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QieP5DdU7zY0"
      },
      "source": [
        "# 3. Centralized model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wTQrxfRpRrD1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8120ec5e-5c8e-4eb7-bc39-527a23e10ad5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5000"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ],
      "source": [
        "documents_centr = [*documents_all[0], *documents_all[1], *documents_all[2], *documents_all[3], *documents_all[4]]\n",
        "len(documents_centr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kxW_tMtFRyfX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e03703e-8337-400b-b669-fa9e924f5d83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Settings: \n",
            "               N Components: 10\n",
            "               Topic Prior Mean: 0.0\n",
            "               Topic Prior Variance: 0.9\n",
            "               Model Type: prodLDA\n",
            "               Hidden Sizes: (100, 100)\n",
            "               Activation: softplus\n",
            "               Dropout: 0.2\n",
            "               Learn Priors: True\n",
            "               Learning Rate: 0.002\n",
            "               Momentum: 0.99\n",
            "               Reduce On Plateau: False\n",
            "               Save Dir: None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [1/100]\tSamples: [5000/500000]\tTrain Loss: 2183.912605078125\tTime: 0:00:00.692837\n",
            "Epoch: [2/100]\tSamples: [10000/500000]\tTrain Loss: 2157.90883671875\tTime: 0:00:00.653632\n",
            "Epoch: [3/100]\tSamples: [15000/500000]\tTrain Loss: 2136.135812109375\tTime: 0:00:00.659005\n",
            "Epoch: [4/100]\tSamples: [20000/500000]\tTrain Loss: 2112.539268359375\tTime: 0:00:00.668727\n",
            "Epoch: [5/100]\tSamples: [25000/500000]\tTrain Loss: 2094.055711328125\tTime: 0:00:00.628219\n",
            "Epoch: [6/100]\tSamples: [30000/500000]\tTrain Loss: 2084.238319921875\tTime: 0:00:00.680411\n",
            "Epoch: [7/100]\tSamples: [35000/500000]\tTrain Loss: 2080.6717609375\tTime: 0:00:00.684884\n",
            "Epoch: [8/100]\tSamples: [40000/500000]\tTrain Loss: 2079.126473046875\tTime: 0:00:00.678835\n",
            "Epoch: [9/100]\tSamples: [45000/500000]\tTrain Loss: 2078.6507619140625\tTime: 0:00:00.672240\n",
            "Epoch: [10/100]\tSamples: [50000/500000]\tTrain Loss: 2078.347369140625\tTime: 0:00:00.683641\n",
            "Epoch: [11/100]\tSamples: [55000/500000]\tTrain Loss: 2078.021916015625\tTime: 0:00:00.650029\n",
            "Epoch: [12/100]\tSamples: [60000/500000]\tTrain Loss: 2077.648983984375\tTime: 0:00:00.649279\n",
            "Epoch: [13/100]\tSamples: [65000/500000]\tTrain Loss: 2077.426554296875\tTime: 0:00:00.670494\n",
            "Epoch: [14/100]\tSamples: [70000/500000]\tTrain Loss: 2077.391833203125\tTime: 0:00:00.652143\n",
            "Epoch: [15/100]\tSamples: [75000/500000]\tTrain Loss: 2077.262280078125\tTime: 0:00:00.653634\n",
            "Epoch: [16/100]\tSamples: [80000/500000]\tTrain Loss: 2077.170915625\tTime: 0:00:00.704124\n",
            "Epoch: [17/100]\tSamples: [85000/500000]\tTrain Loss: 2077.101954296875\tTime: 0:00:00.679969\n",
            "Epoch: [18/100]\tSamples: [90000/500000]\tTrain Loss: 2077.08853125\tTime: 0:00:00.644464\n",
            "Epoch: [19/100]\tSamples: [95000/500000]\tTrain Loss: 2077.0354640625\tTime: 0:00:00.692925\n",
            "Epoch: [20/100]\tSamples: [100000/500000]\tTrain Loss: 2076.93782734375\tTime: 0:00:00.667146\n",
            "Epoch: [21/100]\tSamples: [105000/500000]\tTrain Loss: 2076.868502734375\tTime: 0:00:00.664123\n",
            "Epoch: [22/100]\tSamples: [110000/500000]\tTrain Loss: 2076.794203125\tTime: 0:00:00.663385\n",
            "Epoch: [23/100]\tSamples: [115000/500000]\tTrain Loss: 2076.739764453125\tTime: 0:00:00.676608\n",
            "Epoch: [24/100]\tSamples: [120000/500000]\tTrain Loss: 2076.76401953125\tTime: 0:00:00.670716\n",
            "Epoch: [25/100]\tSamples: [125000/500000]\tTrain Loss: 2076.6443640625\tTime: 0:00:00.686355\n",
            "Epoch: [26/100]\tSamples: [130000/500000]\tTrain Loss: 2076.585703125\tTime: 0:00:00.638254\n",
            "Epoch: [27/100]\tSamples: [135000/500000]\tTrain Loss: 2076.5332171875\tTime: 0:00:00.646677\n",
            "Epoch: [28/100]\tSamples: [140000/500000]\tTrain Loss: 2076.469766015625\tTime: 0:00:00.687062\n",
            "Epoch: [29/100]\tSamples: [145000/500000]\tTrain Loss: 2076.440796484375\tTime: 0:00:00.651952\n",
            "Epoch: [30/100]\tSamples: [150000/500000]\tTrain Loss: 2076.4163546875\tTime: 0:00:00.657512\n",
            "Epoch: [31/100]\tSamples: [155000/500000]\tTrain Loss: 2076.361067578125\tTime: 0:00:00.692069\n",
            "Epoch: [32/100]\tSamples: [160000/500000]\tTrain Loss: 2076.3091615234375\tTime: 0:00:00.663217\n",
            "Epoch: [33/100]\tSamples: [165000/500000]\tTrain Loss: 2076.152421875\tTime: 0:00:00.655285\n",
            "Epoch: [34/100]\tSamples: [170000/500000]\tTrain Loss: 2076.152862109375\tTime: 0:00:00.677752\n",
            "Epoch: [35/100]\tSamples: [175000/500000]\tTrain Loss: 2076.1162408203127\tTime: 0:00:00.670313\n",
            "Epoch: [36/100]\tSamples: [180000/500000]\tTrain Loss: 2076.0609171875\tTime: 0:00:00.638611\n",
            "Epoch: [37/100]\tSamples: [185000/500000]\tTrain Loss: 2076.056903125\tTime: 0:00:00.690161\n",
            "Epoch: [38/100]\tSamples: [190000/500000]\tTrain Loss: 2075.96406875\tTime: 0:00:00.658275\n",
            "Epoch: [39/100]\tSamples: [195000/500000]\tTrain Loss: 2075.965684765625\tTime: 0:00:00.655171\n",
            "Epoch: [40/100]\tSamples: [200000/500000]\tTrain Loss: 2075.84189921875\tTime: 0:00:00.744276\n",
            "Epoch: [41/100]\tSamples: [205000/500000]\tTrain Loss: 2075.7965359375\tTime: 0:00:00.695519\n",
            "Epoch: [42/100]\tSamples: [210000/500000]\tTrain Loss: 2075.799239453125\tTime: 0:00:00.669741\n",
            "Epoch: [43/100]\tSamples: [215000/500000]\tTrain Loss: 2075.790308203125\tTime: 0:00:00.701511\n",
            "Epoch: [44/100]\tSamples: [220000/500000]\tTrain Loss: 2075.747780078125\tTime: 0:00:00.703215\n",
            "Epoch: [45/100]\tSamples: [225000/500000]\tTrain Loss: 2075.74049296875\tTime: 0:00:00.690575\n",
            "Epoch: [46/100]\tSamples: [230000/500000]\tTrain Loss: 2075.730287890625\tTime: 0:00:00.697737\n",
            "Epoch: [47/100]\tSamples: [235000/500000]\tTrain Loss: 2075.6738375\tTime: 0:00:00.692443\n",
            "Epoch: [48/100]\tSamples: [240000/500000]\tTrain Loss: 2075.605682421875\tTime: 0:00:00.667049\n",
            "Epoch: [49/100]\tSamples: [245000/500000]\tTrain Loss: 2075.646708203125\tTime: 0:00:00.686699\n",
            "Epoch: [50/100]\tSamples: [250000/500000]\tTrain Loss: 2075.601824609375\tTime: 0:00:00.693462\n",
            "Epoch: [51/100]\tSamples: [255000/500000]\tTrain Loss: 2075.62551328125\tTime: 0:00:00.675079\n",
            "Epoch: [52/100]\tSamples: [260000/500000]\tTrain Loss: 2075.5882904296873\tTime: 0:00:00.690461\n",
            "Epoch: [53/100]\tSamples: [265000/500000]\tTrain Loss: 2075.536075390625\tTime: 0:00:00.700627\n",
            "Epoch: [54/100]\tSamples: [270000/500000]\tTrain Loss: 2075.64604453125\tTime: 0:00:00.710954\n",
            "Epoch: [55/100]\tSamples: [275000/500000]\tTrain Loss: 2075.5404462890624\tTime: 0:00:00.679503\n",
            "Epoch: [56/100]\tSamples: [280000/500000]\tTrain Loss: 2075.47545703125\tTime: 0:00:00.714900\n",
            "Epoch: [57/100]\tSamples: [285000/500000]\tTrain Loss: 2075.547889453125\tTime: 0:00:00.670297\n",
            "Epoch: [58/100]\tSamples: [290000/500000]\tTrain Loss: 2075.541105859375\tTime: 0:00:00.687699\n",
            "Epoch: [59/100]\tSamples: [295000/500000]\tTrain Loss: 2075.516759375\tTime: 0:00:00.700364\n",
            "Epoch: [60/100]\tSamples: [300000/500000]\tTrain Loss: 2075.4773875\tTime: 0:00:00.708931\n",
            "Epoch: [61/100]\tSamples: [305000/500000]\tTrain Loss: 2075.4734765625\tTime: 0:00:00.723027\n",
            "Epoch: [62/100]\tSamples: [310000/500000]\tTrain Loss: 2075.4512609375\tTime: 0:00:00.707084\n",
            "Epoch: [63/100]\tSamples: [315000/500000]\tTrain Loss: 2075.52232734375\tTime: 0:00:00.716953\n",
            "Epoch: [64/100]\tSamples: [320000/500000]\tTrain Loss: 2075.49391640625\tTime: 0:00:00.683727\n",
            "Epoch: [65/100]\tSamples: [325000/500000]\tTrain Loss: 2075.4543375\tTime: 0:00:00.689449\n",
            "Epoch: [66/100]\tSamples: [330000/500000]\tTrain Loss: 2075.36838671875\tTime: 0:00:00.703811\n",
            "Epoch: [67/100]\tSamples: [335000/500000]\tTrain Loss: 2075.42778359375\tTime: 0:00:00.668892\n",
            "Epoch: [68/100]\tSamples: [340000/500000]\tTrain Loss: 2075.4126759765627\tTime: 0:00:00.687631\n",
            "Epoch: [69/100]\tSamples: [345000/500000]\tTrain Loss: 2075.457298828125\tTime: 0:00:00.723369\n",
            "Epoch: [70/100]\tSamples: [350000/500000]\tTrain Loss: 2075.438692578125\tTime: 0:00:00.694602\n",
            "Epoch: [71/100]\tSamples: [355000/500000]\tTrain Loss: 2075.3782265625\tTime: 0:00:00.687960\n",
            "Epoch: [72/100]\tSamples: [360000/500000]\tTrain Loss: 2075.3780609375\tTime: 0:00:00.725406\n",
            "Epoch: [73/100]\tSamples: [365000/500000]\tTrain Loss: 2075.424707421875\tTime: 0:00:00.646854\n",
            "Epoch: [74/100]\tSamples: [370000/500000]\tTrain Loss: 2075.41359375\tTime: 0:00:00.690107\n",
            "Epoch: [75/100]\tSamples: [375000/500000]\tTrain Loss: 2075.374334765625\tTime: 0:00:00.673300\n",
            "Epoch: [76/100]\tSamples: [380000/500000]\tTrain Loss: 2075.3652384765624\tTime: 0:00:00.671301\n",
            "Epoch: [77/100]\tSamples: [385000/500000]\tTrain Loss: 2075.31123984375\tTime: 0:00:00.672194\n",
            "Epoch: [78/100]\tSamples: [390000/500000]\tTrain Loss: 2075.381276171875\tTime: 0:00:00.696820\n",
            "Epoch: [79/100]\tSamples: [395000/500000]\tTrain Loss: 2075.34671328125\tTime: 0:00:00.668404\n",
            "Epoch: [80/100]\tSamples: [400000/500000]\tTrain Loss: 2075.33761328125\tTime: 0:00:00.697000\n",
            "Epoch: [81/100]\tSamples: [405000/500000]\tTrain Loss: 2075.28801171875\tTime: 0:00:00.688169\n",
            "Epoch: [82/100]\tSamples: [410000/500000]\tTrain Loss: 2075.27677109375\tTime: 0:00:00.676649\n",
            "Epoch: [83/100]\tSamples: [415000/500000]\tTrain Loss: 2075.331164453125\tTime: 0:00:00.711700\n",
            "Epoch: [84/100]\tSamples: [420000/500000]\tTrain Loss: 2075.323339453125\tTime: 0:00:00.693166\n",
            "Epoch: [85/100]\tSamples: [425000/500000]\tTrain Loss: 2075.3497203125\tTime: 0:00:00.683037\n",
            "Epoch: [86/100]\tSamples: [430000/500000]\tTrain Loss: 2075.31573203125\tTime: 0:00:00.723386\n",
            "Epoch: [87/100]\tSamples: [435000/500000]\tTrain Loss: 2075.329038671875\tTime: 0:00:00.708161\n",
            "Epoch: [88/100]\tSamples: [440000/500000]\tTrain Loss: 2075.3235751953125\tTime: 0:00:00.705139\n",
            "Epoch: [89/100]\tSamples: [445000/500000]\tTrain Loss: 2075.23700625\tTime: 0:00:00.732515\n",
            "Epoch: [90/100]\tSamples: [450000/500000]\tTrain Loss: 2075.268428515625\tTime: 0:00:00.720765\n",
            "Epoch: [91/100]\tSamples: [455000/500000]\tTrain Loss: 2075.23037109375\tTime: 0:00:00.711879\n",
            "Epoch: [92/100]\tSamples: [460000/500000]\tTrain Loss: 2075.271383203125\tTime: 0:00:00.718662\n",
            "Epoch: [93/100]\tSamples: [465000/500000]\tTrain Loss: 2075.267608203125\tTime: 0:00:00.693080\n",
            "Epoch: [94/100]\tSamples: [470000/500000]\tTrain Loss: 2075.244990234375\tTime: 0:00:00.659127\n",
            "Epoch: [95/100]\tSamples: [475000/500000]\tTrain Loss: 2075.248808203125\tTime: 0:00:00.718906\n",
            "Epoch: [96/100]\tSamples: [480000/500000]\tTrain Loss: 2075.275091796875\tTime: 0:00:00.701026\n",
            "Epoch: [97/100]\tSamples: [485000/500000]\tTrain Loss: 2075.272416015625\tTime: 0:00:00.724025\n",
            "Epoch: [98/100]\tSamples: [490000/500000]\tTrain Loss: 2075.21801484375\tTime: 0:00:00.678690\n",
            "Epoch: [99/100]\tSamples: [495000/500000]\tTrain Loss: 2075.264080859375\tTime: 0:00:00.690685\n",
            "Epoch: [100/100]\tSamples: [500000/500000]\tTrain Loss: 2075.2007814453127\tTime: 0:00:00.687221\n"
          ]
        }
      ],
      "source": [
        "cv = CountVectorizer(input='content', lowercase=True, stop_words='english',\n",
        "                      max_df=0.99, min_df=0.01, binary=False)\n",
        "  \n",
        "docs_centr = [\" \".join(documents_centr[i]) for i in np.arange(len(documents_centr))]\n",
        "\n",
        "train_bow_centr = cv.fit_transform(docs_centr)\n",
        "train_bow_centr = train_bow_centr.toarray()\n",
        "\n",
        "idx2token_centr = cv.get_feature_names()\n",
        "input_size_centr = len(idx2token_centr)\n",
        "\n",
        "id2token_centr = {k: v for k, v in zip(range(0, len(idx2token_centr)), idx2token_centr)}\n",
        "\n",
        "train_data_centr = BOWDataset(train_bow_centr, idx2token_centr)\n",
        "\n",
        "avitm_centr = AVITM(input_size=input_size_centr, n_components=10, model_type='prodLDA',\n",
        "              hidden_sizes=(100, 100), activation='softplus', dropout=0.2,\n",
        "              learn_priors=True, batch_size=64, lr=2e-3, momentum=0.99,\n",
        "              solver='adam', num_epochs=100, reduce_on_plateau=False)\n",
        "\n",
        "avitm_centr.fit(train_data_centr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A3_xM2LUSYAN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "2a388176-38e2-4242-84e2-10a88ad934c2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-9c79a168-373c-48b5-aea4-780d3af598ad\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>wd356</td>\n",
              "      <td>wd298</td>\n",
              "      <td>wd292</td>\n",
              "      <td>wd556</td>\n",
              "      <td>wd411</td>\n",
              "      <td>wd227</td>\n",
              "      <td>wd738</td>\n",
              "      <td>wd471</td>\n",
              "      <td>wd648</td>\n",
              "      <td>wd162</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>wd173</td>\n",
              "      <td>wd82</td>\n",
              "      <td>wd477</td>\n",
              "      <td>wd795</td>\n",
              "      <td>wd99</td>\n",
              "      <td>wd434</td>\n",
              "      <td>wd486</td>\n",
              "      <td>wd399</td>\n",
              "      <td>wd911</td>\n",
              "      <td>wd790</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>wd4</td>\n",
              "      <td>wd589</td>\n",
              "      <td>wd92</td>\n",
              "      <td>wd369</td>\n",
              "      <td>wd685</td>\n",
              "      <td>wd790</td>\n",
              "      <td>wd98</td>\n",
              "      <td>wd909</td>\n",
              "      <td>wd374</td>\n",
              "      <td>wd748</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>wd175</td>\n",
              "      <td>wd355</td>\n",
              "      <td>wd788</td>\n",
              "      <td>wd553</td>\n",
              "      <td>wd571</td>\n",
              "      <td>wd260</td>\n",
              "      <td>wd816</td>\n",
              "      <td>wd728</td>\n",
              "      <td>wd533</td>\n",
              "      <td>wd492</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>wd389</td>\n",
              "      <td>wd772</td>\n",
              "      <td>wd748</td>\n",
              "      <td>wd101</td>\n",
              "      <td>wd553</td>\n",
              "      <td>wd368</td>\n",
              "      <td>wd355</td>\n",
              "      <td>wd445</td>\n",
              "      <td>wd567</td>\n",
              "      <td>wd856</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>wd912</td>\n",
              "      <td>wd21</td>\n",
              "      <td>wd592</td>\n",
              "      <td>wd185</td>\n",
              "      <td>wd221</td>\n",
              "      <td>wd643</td>\n",
              "      <td>wd80</td>\n",
              "      <td>wd599</td>\n",
              "      <td>wd94</td>\n",
              "      <td>wd278</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>wd966</td>\n",
              "      <td>wd952</td>\n",
              "      <td>wd541</td>\n",
              "      <td>wd584</td>\n",
              "      <td>wd81</td>\n",
              "      <td>wd818</td>\n",
              "      <td>wd165</td>\n",
              "      <td>wd773</td>\n",
              "      <td>wd412</td>\n",
              "      <td>wd266</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>wd905</td>\n",
              "      <td>wd119</td>\n",
              "      <td>wd614</td>\n",
              "      <td>wd80</td>\n",
              "      <td>wd319</td>\n",
              "      <td>wd50</td>\n",
              "      <td>wd289</td>\n",
              "      <td>wd718</td>\n",
              "      <td>wd774</td>\n",
              "      <td>wd474</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>wd131</td>\n",
              "      <td>wd601</td>\n",
              "      <td>wd892</td>\n",
              "      <td>wd574</td>\n",
              "      <td>wd448</td>\n",
              "      <td>wd301</td>\n",
              "      <td>wd635</td>\n",
              "      <td>wd752</td>\n",
              "      <td>wd576</td>\n",
              "      <td>wd590</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>wd705</td>\n",
              "      <td>wd806</td>\n",
              "      <td>wd265</td>\n",
              "      <td>wd231</td>\n",
              "      <td>wd752</td>\n",
              "      <td>wd314</td>\n",
              "      <td>wd719</td>\n",
              "      <td>wd894</td>\n",
              "      <td>wd545</td>\n",
              "      <td>wd290</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9c79a168-373c-48b5-aea4-780d3af598ad')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-9c79a168-373c-48b5-aea4-780d3af598ad button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-9c79a168-373c-48b5-aea4-780d3af598ad');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "       0      1      2      3      4      5      6      7      8      9\n",
              "0  wd356  wd298  wd292  wd556  wd411  wd227  wd738  wd471  wd648  wd162\n",
              "1  wd173   wd82  wd477  wd795   wd99  wd434  wd486  wd399  wd911  wd790\n",
              "2    wd4  wd589   wd92  wd369  wd685  wd790   wd98  wd909  wd374  wd748\n",
              "3  wd175  wd355  wd788  wd553  wd571  wd260  wd816  wd728  wd533  wd492\n",
              "4  wd389  wd772  wd748  wd101  wd553  wd368  wd355  wd445  wd567  wd856\n",
              "5  wd912   wd21  wd592  wd185  wd221  wd643   wd80  wd599   wd94  wd278\n",
              "6  wd966  wd952  wd541  wd584   wd81  wd818  wd165  wd773  wd412  wd266\n",
              "7  wd905  wd119  wd614   wd80  wd319   wd50  wd289  wd718  wd774  wd474\n",
              "8  wd131  wd601  wd892  wd574  wd448  wd301  wd635  wd752  wd576  wd590\n",
              "9  wd705  wd806  wd265  wd231  wd752  wd314  wd719  wd894  wd545  wd290"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ],
      "source": [
        "topics_centr = pd.DataFrame(avitm_centr.get_topics(10)).T\n",
        "topics_centr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z1w7cb9r7QAl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da8c2133-485b-43a1-a3c5-1ce90aa095bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Sampling: [5/5]: : 5it [00:01,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5000, 10)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "doc_topic_centr = get_doc_topic_distribution(avitm_centr, train_data_centr, n_samples=5) # get all the topic predictions\n",
        "print(doc_topic_centr.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YHry56Bz7sbV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a035c4ab-77f7-4a65-c5eb-e26e913ac9e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.00102535 0.00095503 0.00101596 ... 0.00098853 0.00100016 0.        ]\n",
            " [0.00100684 0.00094763 0.00101549 ... 0.00101879 0.00101515 0.        ]\n",
            " [0.00097415 0.00103656 0.00098535 ... 0.00103208 0.00099289 0.        ]\n",
            " ...\n",
            " [0.00097276 0.00097275 0.00096688 ... 0.00096017 0.00101089 0.        ]\n",
            " [0.00100298 0.00105892 0.00093564 ... 0.00097189 0.00094689 0.        ]\n",
            " [0.00096845 0.00104093 0.00099957 ... 0.00105812 0.00104947 0.        ]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0000000000000002"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ],
      "source": [
        "w_t_distrib_centr = np.zeros((10,vocab_size), dtype=np.float64) # vocab_size = 10000\n",
        "wd = get_topic_word_distribution(avitm_centr)\n",
        "for i in np.arange(10):\n",
        "  for idx, word in id2token_centr.items():\n",
        "    for j in np.arange(len(all_words)):\n",
        "      if all_words[j] == word:\n",
        "        w_t_distrib_centr[i,j] = wd[i][idx]\n",
        "        break\n",
        "sum_of_rows = w_t_distrib_centr.sum(axis=1)\n",
        "w_t_distrib_centr_norm = w_t_distrib_centr / sum_of_rows[:, np.newaxis]\n",
        "print(w_t_distrib_centr_norm)\n",
        "sum(w_t_distrib_centr_norm[8,:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usiERbR-8Roe"
      },
      "source": [
        "# 4. Get similarity through Frobenius"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ng1ykc1A9wkn"
      },
      "outputs": [],
      "source": [
        "doc_topic_centr_all = []\n",
        "doc_topic_centr_all.append(doc_topic_centr[0:1000,:])\n",
        "doc_topic_centr_all.append(doc_topic_centr[1000:2000,:])\n",
        "doc_topic_centr_all.append(doc_topic_centr[2000:3000,:])\n",
        "doc_topic_centr_all.append(doc_topic_centr[3000:4000,:])\n",
        "doc_topic_centr_all.append(doc_topic_centr[4000:5000,:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-NR5xZgQ8S8W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50a0b59f-cbb4-4ae5-a2b9-ee367beb0ed7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NODE 0\n",
            "GT vs inferred in node: 150.43204439231548\n",
            "GT vs centralized in node 152.12822537569534\n",
            "***************************************************************\n",
            "NODE 1\n",
            "GT vs inferred in node: 153.88007428103734\n",
            "GT vs centralized in node 156.59442718688402\n",
            "***************************************************************\n",
            "NODE 2\n",
            "GT vs inferred in node: 155.77601263229158\n",
            "GT vs centralized in node 157.6421706201403\n",
            "***************************************************************\n",
            "NODE 3\n",
            "GT vs inferred in node: 154.50977278170964\n",
            "GT vs centralized in node 158.81799565968134\n",
            "***************************************************************\n",
            "NODE 4\n",
            "GT vs inferred in node: 152.24692330576306\n",
            "GT vs centralized in node 155.68228087164525\n",
            "***************************************************************\n"
          ]
        }
      ],
      "source": [
        "for node in np.arange(n_nodes):\n",
        "  # Ground truth in node vs inferred in node\n",
        "  doc_topics_avitm_sqrt_node = np.sqrt(doc_topic_all[node])\n",
        "  similarity_avitm_node = doc_topics_avitm_sqrt_node.dot(doc_topics_avitm_sqrt_node.T)\n",
        "\n",
        "  doc_topics_gt_sqrt_node = np.sqrt(doc_topics_all_gt[node])\n",
        "  similarity_gt = doc_topics_gt_sqrt_node.dot(doc_topics_gt_sqrt_node.T)\n",
        "\n",
        "  diff_sims = similarity_avitm_node - similarity_gt\n",
        "  frobenius_diff_sims_node = np.linalg.norm(diff_sims,'fro')\n",
        "\n",
        "  # Ground truth in node vs centralized (for documents of such a node)\n",
        "  doc_topics_avitm_sqrt_centr_node = np.sqrt(doc_topic_centr_all[node])\n",
        "  similarity_avitm_centr = doc_topics_avitm_sqrt_centr_node.dot(doc_topics_avitm_sqrt_centr_node.T)\n",
        "\n",
        "  diff_sims = similarity_avitm_centr - similarity_gt\n",
        "  frobenius_diff_sims_avg = np.linalg.norm(diff_sims,'fro')\n",
        "\n",
        "  print(\"NODE\", str(node))\n",
        "  print(\"GT vs inferred in node:\", frobenius_diff_sims_node)\n",
        "  print(\"GT vs centralized in node\", frobenius_diff_sims_avg)\n",
        "  print(\"***************************************************************\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uQ6YfIbw-72y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1921988c-b450-467a-a451-8494ab318818"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NODE 0\n",
            "Original vs inferred in node sum max row: 9.870300498842633\n",
            "***************************************************************\n",
            "NODE 1\n",
            "Original vs inferred in node sum max row: 9.870742245341376\n",
            "***************************************************************\n",
            "NODE 2\n",
            "Original vs inferred in node sum max row: 9.870425849504631\n",
            "***************************************************************\n",
            "NODE 3\n",
            "Original vs inferred in node sum max row: 9.87049521512419\n",
            "***************************************************************\n",
            "NODE 4\n",
            "Original vs inferred in node sum max row: 9.870070997157123\n",
            "***************************************************************\n",
            "CENTRALIZED\n",
            "Original vs avg of inferred in nodes sum max row 9.870361911979648\n"
          ]
        }
      ],
      "source": [
        "for node in np.arange(n_nodes):\n",
        "  # GT vs inferred in node\n",
        "  topic_words_gt_sqrt = np.sqrt(topic_vectors)\n",
        "  topic_words_avtim_node_sqrt = np.sqrt(topic_word_all[node])\n",
        "  simmat_t_w = topic_words_gt_sqrt.dot(topic_words_avtim_node_sqrt.T)\n",
        "\n",
        "  simmat_t_w_pd = pd.DataFrame(simmat_t_w)\n",
        "  maxValues_rows = simmat_t_w_pd.max(axis = 1)\n",
        "  max_values_rows_sum = maxValues_rows.sum()\n",
        "\n",
        "  print(\"NODE\", str(node))\n",
        "  print(\"Original vs inferred in node sum max row:\", max_values_rows_sum)\n",
        "  print(\"***************************************************************\")\n",
        "\n",
        "# GT vs centralized\n",
        "topic_words_avtim_centr_sqrt = np.sqrt(w_t_distrib_centr_norm)\n",
        "simmat_t_w = topic_words_gt_sqrt.dot(topic_words_avtim_centr_sqrt.T)\n",
        "\n",
        "simmat_t_w_pd = pd.DataFrame(simmat_t_w)\n",
        "maxValues_rows = simmat_t_w_pd.max(axis = 1)\n",
        "max_values_rows_sum_centr = maxValues_rows.sum()\n",
        "\n",
        "print(\"CENTRALIZED\")\n",
        "print(\"Original vs avg of inferred in nodes sum max row\", max_values_rows_sum_centr)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.imshow(simmat_t_w)\n",
        "plt.colorbar()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "f62uaxhLKIOF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "bea4273f-61fe-4a48-fedb-9f59535efaa0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Centralized-ProdLDA-vocab-1000_beta-10_prior-0.9.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}