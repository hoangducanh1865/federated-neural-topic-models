{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sib1HSks6Xqv"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from scipy.special import softmax\n",
        "import multiprocessing as mp\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "from sklearn.preprocessing import normalize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CObHMSd6LLz"
      },
      "source": [
        "# Installing ProdLDA\n",
        "**Restart notbook after the installation!!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-VDAzxJA6FK5",
        "outputId": "b10760a0-376c-4e98-e565-b85c9e7306f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'PyTorchAVITM' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/estebandito22/PyTorchAVITM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6kW5jO66UKj"
      },
      "source": [
        "# 1. Creation of synthetic corpus\n",
        "\n",
        "We consider a scenario with n parties, each of them as an associated corpus.\n",
        "To generate the corpus associated with each of the parties, we consider a common beta distribution (word-topic distribution), but we freeze different topics/ assign different asymmetric Dirichlet priors favoring different topics at the time of generating the document that composes each party's corpus."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twSZ3G0p6d1z"
      },
      "source": [
        "## 1.1. Function for permuting the Dirichlet prior at each node"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tXdkpdrh6Thn"
      },
      "outputs": [],
      "source": [
        "def rotateArray(arr, n, d):\n",
        "    temp = []\n",
        "    i = 0\n",
        "    while (i < d):\n",
        "        temp.append(arr[i])\n",
        "        i = i + 1\n",
        "    i = 0\n",
        "    while (d < n):\n",
        "        arr[i] = arr[d]\n",
        "        i = i + 1\n",
        "        d = d + 1\n",
        "    arr[:] = arr[: i] + temp\n",
        "    return arr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AyFA9eGH6hGH"
      },
      "source": [
        "## 1.2. Topic modeling and node settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5DmfSiuR6iI0",
        "outputId": "40970246-4a8e-4c31-b37b-65c74b01eb87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.1, 0.1, 0.1, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5]\n"
          ]
        }
      ],
      "source": [
        "# Topic modeling settings\n",
        "vocab_size = 1000\n",
        "n_topics = 10\n",
        "beta = 1e-2\n",
        "alpha = 5/n_topics\n",
        "n_docs = 1000\n",
        "nwords = (150, 450) #Min and max lengths of the documents\n",
        "\n",
        "# Nodes settings\n",
        "n_nodes = 5\n",
        "frozen_topics = 3\n",
        "dirichlet_symmetric = False\n",
        "prior = (n_topics)*[0.5]\n",
        "prior[0] = prior[1] = prior[2] = 0.1\n",
        "print(prior)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ylo9Vsu6zpX"
      },
      "source": [
        "## 1.3. Topics generation (common for all nodes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uy3AuOSx6qc1",
        "outputId": "b0d5ad12-d0f7-4f47-cbee-afc05c53a66d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ordered probabilities for the first topic vector:\n",
            "[1.63604483e-001 1.28555985e-001 1.17949550e-001 7.87827092e-002\n",
            " 5.38810525e-002 3.95100794e-002 3.83901077e-002 3.39982857e-002\n",
            " 3.17773224e-002 2.79215339e-002 2.78305313e-002 2.22118361e-002\n",
            " 2.06449417e-002 1.79311645e-002 1.44913899e-002 1.38254114e-002\n",
            " 1.34284536e-002 1.23020411e-002 9.80376782e-003 9.02913802e-003\n",
            " 8.97810797e-003 8.75696560e-003 7.42738077e-003 7.38114243e-003\n",
            " 7.04409636e-003 6.80391179e-003 6.42993429e-003 6.37771610e-003\n",
            " 5.81517050e-003 5.68817297e-003 5.54855181e-003 5.15830669e-003\n",
            " 5.09643038e-003 4.61748539e-003 4.17517747e-003 3.58676750e-003\n",
            " 3.11569414e-003 2.21906691e-003 2.16104175e-003 1.99500825e-003\n",
            " 1.29672456e-003 1.25369920e-003 1.21100210e-003 1.18505223e-003\n",
            " 1.14141320e-003 9.56652415e-004 9.36382934e-004 9.05738003e-004\n",
            " 8.94615487e-004 8.26555995e-004 7.90889044e-004 7.81365522e-004\n",
            " 4.78350877e-004 4.44900540e-004 3.70265593e-004 3.29306631e-004\n",
            " 2.84603168e-004 1.95093053e-004 1.44862564e-004 1.04170384e-004\n",
            " 1.04047892e-004 9.53383994e-005 7.69263049e-005 7.63792912e-005\n",
            " 7.12982900e-005 6.95317535e-005 6.26786177e-005 6.20071495e-005\n",
            " 6.10299274e-005 5.28539481e-005 5.26757193e-005 5.16658473e-005\n",
            " 5.15422890e-005 4.93339548e-005 3.08815324e-005 2.80254597e-005\n",
            " 2.66211504e-005 2.42664719e-005 2.25756694e-005 1.92183754e-005\n",
            " 1.52941035e-005 1.16269770e-005 1.13018113e-005 8.00344616e-006\n",
            " 7.58954586e-006 7.52277487e-006 7.31943335e-006 6.89942356e-006\n",
            " 6.53461756e-006 6.16470110e-006 5.95645335e-006 5.88409752e-006\n",
            " 5.46860035e-006 3.46873622e-006 2.60153283e-006 2.52183600e-006\n",
            " 2.00923456e-006 1.93964413e-006 1.84597447e-006 1.43570330e-006\n",
            " 1.31732770e-006 1.21525533e-006 1.10352606e-006 9.86149528e-007\n",
            " 7.59014001e-007 6.55806505e-007 6.49120094e-007 6.38763999e-007\n",
            " 5.52228054e-007 5.33684425e-007 5.10529359e-007 3.92425851e-007\n",
            " 3.21060893e-007 2.75237619e-007 2.14004002e-007 2.03738956e-007\n",
            " 1.91603859e-007 1.67599085e-007 1.33564293e-007 1.32757577e-007\n",
            " 1.28777839e-007 1.26640886e-007 1.19159420e-007 1.18244607e-007\n",
            " 8.80070960e-008 8.51669787e-008 6.23191339e-008 5.15329659e-008\n",
            " 4.73678923e-008 4.31405739e-008 3.66778285e-008 3.10121941e-008\n",
            " 3.07682890e-008 2.18124558e-008 2.13347570e-008 1.72970473e-008\n",
            " 1.68884576e-008 1.61298117e-008 1.58305134e-008 1.20966995e-008\n",
            " 1.09554481e-008 1.09032141e-008 1.04716259e-008 9.45554765e-009\n",
            " 9.38401413e-009 8.52137312e-009 7.15355781e-009 5.18072323e-009\n",
            " 4.92937427e-009 3.68564375e-009 3.25235512e-009 2.95963965e-009\n",
            " 2.65990480e-009 2.46811765e-009 2.20003530e-009 2.12617967e-009\n",
            " 1.91680591e-009 1.89873812e-009 1.65848616e-009 1.60545541e-009\n",
            " 1.60460139e-009 1.56171241e-009 9.25608171e-010 7.85379421e-010\n",
            " 6.24720327e-010 6.08766593e-010 4.38744390e-010 4.37552230e-010\n",
            " 3.89624306e-010 3.11461075e-010 2.83892071e-010 2.32691621e-010\n",
            " 1.87456573e-010 1.73155773e-010 1.64655442e-010 1.52032117e-010\n",
            " 1.18762424e-010 1.12793382e-010 9.30571502e-011 7.36369082e-011\n",
            " 7.27183290e-011 7.13355052e-011 5.99575616e-011 5.99235057e-011\n",
            " 4.55368795e-011 4.21972595e-011 3.99258938e-011 3.68760208e-011\n",
            " 3.41919201e-011 3.41448924e-011 3.36830308e-011 3.30607311e-011\n",
            " 3.28221282e-011 2.22013716e-011 2.07693901e-011 1.98330654e-011\n",
            " 1.97296570e-011 1.22117339e-011 1.02431737e-011 1.01764198e-011\n",
            " 9.67588936e-012 8.71873346e-012 8.21122243e-012 6.96941477e-012\n",
            " 5.34813425e-012 5.31293308e-012 4.88123858e-012 4.69510161e-012\n",
            " 4.32814594e-012 3.03754321e-012 3.00096053e-012 2.36134102e-012\n",
            " 2.13964269e-012 1.80903797e-012 1.70594268e-012 1.29448322e-012\n",
            " 7.40909867e-013 6.44361616e-013 6.21659810e-013 5.97331281e-013\n",
            " 5.90108669e-013 4.62469334e-013 3.95721967e-013 3.93256952e-013\n",
            " 1.82324176e-013 1.75263214e-013 1.72692859e-013 1.63444608e-013\n",
            " 1.30578435e-013 1.27004124e-013 1.24861740e-013 1.24463824e-013\n",
            " 1.18454577e-013 1.15310396e-013 1.11901170e-013 9.26561981e-014\n",
            " 7.78690511e-014 7.50173578e-014 6.86816774e-014 5.23976941e-014\n",
            " 4.93303117e-014 4.39797739e-014 4.16653249e-014 3.99234292e-014\n",
            " 3.37886961e-014 3.25829930e-014 2.04165298e-014 1.69557419e-014\n",
            " 1.32679178e-014 1.26974133e-014 1.00299282e-014 8.48904031e-015\n",
            " 6.09836856e-015 5.40982365e-015 3.23489407e-015 2.53097236e-015\n",
            " 2.31111860e-015 2.20640653e-015 2.19885446e-015 2.17721460e-015\n",
            " 1.73961353e-015 1.67106040e-015 9.51812723e-016 8.56725955e-016\n",
            " 6.76987489e-016 5.62022610e-016 5.03455173e-016 3.61876149e-016\n",
            " 3.37526105e-016 3.36070488e-016 3.00651803e-016 2.59366107e-016\n",
            " 2.31390780e-016 1.91486125e-016 1.74471130e-016 1.58651668e-016\n",
            " 1.52516186e-016 1.38546010e-016 1.36429979e-016 1.27574745e-016\n",
            " 1.17682093e-016 1.05019233e-016 9.12963921e-017 8.03160453e-017\n",
            " 7.65254019e-017 6.69431947e-017 5.94336934e-017 5.83916257e-017\n",
            " 4.19773214e-017 4.03092749e-017 3.35842857e-017 2.99860970e-017\n",
            " 2.98747000e-017 2.34752103e-017 2.10401054e-017 1.94139973e-017\n",
            " 1.43913978e-017 1.07652940e-017 6.10193866e-018 4.82118073e-018\n",
            " 4.16191273e-018 3.81653918e-018 3.28853004e-018 2.88289949e-018\n",
            " 2.63634023e-018 2.56930721e-018 2.48011882e-018 2.17791343e-018\n",
            " 2.09685678e-018 1.73290244e-018 1.46463700e-018 9.56405709e-019\n",
            " 8.63893970e-019 6.18252391e-019 5.82721774e-019 5.29001363e-019\n",
            " 4.89952493e-019 4.61467158e-019 4.25578100e-019 3.86567093e-019\n",
            " 3.83716842e-019 2.75524138e-019 2.60258149e-019 2.02979592e-019\n",
            " 2.01436636e-019 1.16057393e-019 1.10348477e-019 1.05959284e-019\n",
            " 1.01418241e-019 6.15931405e-020 5.43929587e-020 5.19377420e-020\n",
            " 4.06938580e-020 1.96073098e-020 1.25673483e-020 1.10887375e-020\n",
            " 1.01454992e-020 8.54993560e-021 8.13075758e-021 7.93299236e-021\n",
            " 7.18884037e-021 5.43254986e-021 5.32401025e-021 3.70635812e-021\n",
            " 3.39079336e-021 3.37294627e-021 3.30385490e-021 2.99420600e-021\n",
            " 2.65896440e-021 2.38425986e-021 1.88853598e-021 1.78022865e-021\n",
            " 1.66517458e-021 1.61790876e-021 1.56066386e-021 1.29171733e-021\n",
            " 9.22816206e-022 7.05227410e-022 6.77691548e-022 4.72408442e-022\n",
            " 4.64387576e-022 2.49261744e-022 2.27923462e-022 1.70486209e-022\n",
            " 8.22789711e-023 7.21614949e-023 7.09483566e-023 6.83624404e-023\n",
            " 6.35473291e-023 4.51156347e-023 3.33702559e-023 2.62869367e-023\n",
            " 2.62353632e-023 2.33440482e-023 2.26327410e-023 2.06718827e-023\n",
            " 1.72176212e-023 1.46194651e-023 8.91160559e-024 7.79689737e-024\n",
            " 6.93588135e-024 6.52147514e-024 6.24640573e-024 4.82972965e-024\n",
            " 4.69991691e-024 4.59464645e-024 4.33969507e-024 4.19220404e-024\n",
            " 4.17663442e-024 3.61941847e-024 3.09971704e-024 2.28467230e-024\n",
            " 1.22452568e-024 5.24750660e-025 4.98523793e-025 4.22897263e-025\n",
            " 3.13500771e-025 2.15580769e-025 2.13125693e-025 1.79153547e-025\n",
            " 1.56052906e-025 1.41804266e-025 1.30705455e-025 1.07405349e-025\n",
            " 9.70183941e-026 7.85740992e-026 6.86155680e-026 6.81007147e-026\n",
            " 6.59861914e-026 5.65634439e-026 5.06440052e-026 4.29396463e-026\n",
            " 4.18690203e-026 3.47851226e-026 3.25680105e-026 3.23469581e-026\n",
            " 2.12879394e-026 1.98378872e-026 1.82933773e-026 1.70979223e-026\n",
            " 1.61642170e-026 1.43435234e-026 1.06414541e-026 9.57324370e-027\n",
            " 8.55356208e-027 7.32716009e-027 6.39115484e-027 6.03512277e-027\n",
            " 3.51137584e-027 3.36062028e-027 2.60173103e-027 2.05649545e-027\n",
            " 1.92813209e-027 1.45927223e-027 1.45103684e-027 1.36909278e-027\n",
            " 1.17616524e-027 9.51127611e-028 8.50142212e-028 7.90630846e-028\n",
            " 5.09703374e-028 4.84490972e-028 3.06366601e-028 2.30156012e-028\n",
            " 2.26201187e-028 2.01912210e-028 1.94015000e-028 1.60008408e-028\n",
            " 1.59850144e-028 1.40077081e-028 1.35318938e-028 1.34815247e-028\n",
            " 4.61960062e-029 4.36605284e-029 3.48104053e-029 2.53372620e-029\n",
            " 1.72270045e-029 9.80033966e-030 9.58238075e-030 8.02640883e-030\n",
            " 7.75370389e-030 7.66277869e-030 7.45576926e-030 5.69051177e-030\n",
            " 4.50249897e-030 4.39993467e-030 4.00836875e-030 3.62380087e-030\n",
            " 3.49633904e-030 3.18124574e-030 3.13172890e-030 2.11069566e-030\n",
            " 1.45125024e-030 1.23741282e-030 1.22226152e-030 9.41271499e-031\n",
            " 9.21403653e-031 8.47563258e-031 7.68239402e-031 6.93491891e-031\n",
            " 6.33078071e-031 5.92368350e-031 5.77975877e-031 2.74143344e-031\n",
            " 2.63399103e-031 2.31562769e-031 2.22976344e-031 2.05020528e-031\n",
            " 2.02631681e-031 1.97671363e-031 1.70201073e-031 1.38310133e-031\n",
            " 9.86945761e-032 9.27016321e-032 9.26848674e-032 6.78849642e-032\n",
            " 5.65162859e-032 4.52192991e-032 3.28835642e-032 2.71401388e-032\n",
            " 2.53556067e-032 1.33307635e-032 1.09966255e-032 9.42469745e-033\n",
            " 9.01872901e-033 8.64849597e-033 3.79877080e-033 3.09538767e-033\n",
            " 2.98376975e-033 2.91817653e-033 2.77978737e-033 2.58606544e-033\n",
            " 1.96683325e-033 1.90205764e-033 1.45669964e-033 1.31829383e-033\n",
            " 1.15919327e-033 9.89789773e-034 6.90561390e-034 6.49209096e-034\n",
            " 5.22895681e-034 4.98723587e-034 4.86559143e-034 2.99499042e-034\n",
            " 2.65209011e-034 2.45762647e-034 1.95307319e-034 1.64924728e-034\n",
            " 1.42253229e-034 1.39549824e-034 1.33024674e-034 9.04406305e-035\n",
            " 8.19806455e-035 6.44725926e-035 3.48220146e-035 3.18134177e-035\n",
            " 2.37723672e-035 1.85331342e-035 1.53039559e-035 1.50895673e-035\n",
            " 6.93900575e-036 4.81611053e-036 2.13471447e-036 1.78454267e-036\n",
            " 1.74571410e-036 1.05714578e-036 5.36009920e-037 5.32727011e-037\n",
            " 5.16123998e-037 2.73067374e-037 2.49721972e-037 2.38325111e-037\n",
            " 2.21541381e-037 1.57895037e-037 1.50112736e-037 1.01515087e-037\n",
            " 9.62652283e-038 9.00539992e-038 5.67414406e-038 5.30678165e-038\n",
            " 4.48988461e-038 4.16887632e-038 3.25896217e-038 3.20446837e-038\n",
            " 2.87590678e-038 2.62861017e-038 2.15487452e-038 2.04717424e-038\n",
            " 1.50081686e-038 8.88006039e-039 6.74516772e-039 5.45142593e-039\n",
            " 3.63923589e-039 3.13688025e-039 9.74561537e-040 8.40856906e-040\n",
            " 8.00676382e-040 3.98219702e-040 3.11054853e-040 2.08568054e-040\n",
            " 1.72302645e-040 1.26665458e-040 1.07163283e-040 9.86014116e-041\n",
            " 7.68702785e-041 1.88083674e-041 1.86502293e-041 1.37330272e-041\n",
            " 8.74734170e-042 8.26069212e-042 5.53124603e-042 4.52013457e-042\n",
            " 2.10485399e-042 2.07615297e-042 1.51798112e-042 1.33050678e-042\n",
            " 6.02768954e-043 5.88690046e-043 2.61382600e-043 1.83264848e-043\n",
            " 1.55131572e-043 1.03647067e-043 8.41050462e-044 8.04446707e-044\n",
            " 7.36960937e-044 7.07864814e-044 5.00936058e-044 4.45394226e-044\n",
            " 3.88029019e-044 3.47238644e-044 3.41042511e-044 3.39589753e-044\n",
            " 3.17014019e-044 1.56520634e-044 1.51453788e-044 1.48515084e-044\n",
            " 8.44966490e-045 6.67346812e-045 5.91994878e-045 5.28865955e-045\n",
            " 4.61610071e-045 3.98922713e-045 3.68899394e-045 3.02636862e-045\n",
            " 1.86859242e-045 1.26434518e-045 6.90279125e-046 6.53706492e-046\n",
            " 5.71968108e-046 2.86037672e-046 2.06680140e-046 1.81389529e-046\n",
            " 1.17864227e-046 6.60536233e-047 4.79734303e-047 4.21403696e-047\n",
            " 3.64520082e-047 2.70299619e-047 2.41080218e-047 2.12203433e-047\n",
            " 1.80764990e-047 1.61933996e-047 6.10364727e-048 5.48925008e-048\n",
            " 3.89830874e-048 2.77155322e-048 1.67043663e-048 1.25361512e-048\n",
            " 6.91868887e-049 6.45441803e-049 6.33533892e-049 3.71445093e-049\n",
            " 3.51715746e-049 2.42781686e-049 6.44051631e-050 5.24625637e-050\n",
            " 5.19265087e-050 4.72394013e-050 4.40427515e-050 2.88556225e-050\n",
            " 1.89483434e-050 1.72946933e-050 7.04991949e-051 6.06683980e-051\n",
            " 5.91701728e-051 4.86247832e-051 4.78206082e-051 4.65822049e-051\n",
            " 3.85400876e-051 2.63706551e-051 2.30862952e-051 1.39771292e-051\n",
            " 1.17682296e-051 1.14377948e-051 1.13834475e-051 9.97057024e-052\n",
            " 6.21716876e-052 5.99863446e-052 4.39083245e-052 3.18552985e-052\n",
            " 3.07723854e-052 2.54825702e-052 2.46235435e-052 2.42176994e-052\n",
            " 2.27707251e-052 2.16390741e-052 1.88887588e-052 1.77827126e-052\n",
            " 1.15301011e-052 9.71279996e-053 4.04364202e-053 3.76482430e-053\n",
            " 2.79888344e-053 2.01330233e-053 8.09995569e-054 6.43744336e-054\n",
            " 6.10054293e-054 4.99160133e-054 4.48911694e-054 3.07803072e-054\n",
            " 1.65574671e-054 1.04217670e-054 7.66066423e-055 7.51711941e-055\n",
            " 6.99376903e-055 3.65167272e-055 2.71744154e-055 2.25859792e-055\n",
            " 1.77811817e-055 1.76944246e-055 1.53535365e-055 1.26601393e-055\n",
            " 1.14661666e-055 4.41211831e-056 3.74860046e-056 2.69513343e-056\n",
            " 2.14117031e-056 1.47036766e-056 1.18639881e-056 7.95702112e-057\n",
            " 7.67828271e-057 7.26688264e-057 4.84851696e-057 4.55534266e-057\n",
            " 3.61313970e-057 2.45469374e-057 1.47347575e-057 1.42738130e-057\n",
            " 1.23854497e-057 9.13163710e-058 7.41451780e-058 4.26207010e-058\n",
            " 2.80575698e-058 2.62474241e-058 2.10228992e-058 2.04750900e-058\n",
            " 1.61475019e-058 1.25964125e-058 1.11763985e-058 1.07642826e-058\n",
            " 7.90005452e-059 5.23098444e-059 4.91767592e-059 3.38358900e-059\n",
            " 2.21427786e-059 1.68737823e-059 1.55448650e-059 8.46133127e-060\n",
            " 5.26577532e-060 4.02383381e-060 2.22563517e-060 2.12512925e-060\n",
            " 1.16729658e-060 7.83777642e-061 6.34266256e-061 4.93161763e-061\n",
            " 2.91567071e-061 1.61774737e-061 1.19151278e-062 1.07961022e-062\n",
            " 9.74528023e-063 8.55837127e-063 5.39589513e-063 4.38449185e-063\n",
            " 2.24710479e-063 2.07547275e-063 1.97869156e-063 1.49091510e-063\n",
            " 1.37631740e-063 6.85045995e-064 6.27706453e-064 4.47619449e-064\n",
            " 3.49146436e-064 3.28903007e-064 3.10764761e-064 2.03967526e-064\n",
            " 3.13194373e-065 1.92845220e-065 9.59388034e-066 4.83788722e-066\n",
            " 2.94121485e-066 1.98011927e-066 1.28314097e-066 5.54875501e-067\n",
            " 4.66578332e-067 2.75916989e-067 2.66918840e-067 2.22446729e-067\n",
            " 1.55680177e-067 9.68554900e-068 7.62791902e-068 6.27668773e-068\n",
            " 4.78605594e-068 2.75103634e-068 2.11181646e-068 1.61487763e-068\n",
            " 9.29736059e-069 8.64737087e-069 6.38161574e-069 3.43299385e-069\n",
            " 2.06894774e-069 1.82596186e-069 1.23929367e-069 5.59522834e-070\n",
            " 4.58422430e-070 4.45105891e-070 3.88482030e-070 2.77872851e-070\n",
            " 2.60737991e-070 5.15481084e-071 3.51068236e-071 9.96737280e-072\n",
            " 7.79315589e-072 6.96357954e-072 3.91410368e-072 2.91944074e-072\n",
            " 1.25831539e-072 1.07415327e-072 9.54530918e-073 5.48642044e-073\n",
            " 2.36075560e-073 2.09065620e-073 1.28626654e-073 8.83835435e-074\n",
            " 1.18798154e-074 4.17797234e-075 3.42871454e-075 2.26351520e-076\n",
            " 7.55871180e-077 6.87988065e-077 5.05626938e-077 3.55001268e-077\n",
            " 3.53627823e-077 2.56306564e-077 2.53909827e-077 1.16696138e-077\n",
            " 8.89228327e-078 8.31143691e-078 6.20473989e-078 4.72643333e-078\n",
            " 3.74463261e-078 1.38847085e-078 1.09532938e-078 8.00717255e-079\n",
            " 7.09325766e-079 5.97913933e-079 4.93638046e-079 1.37435313e-079\n",
            " 3.59175952e-080 1.62093410e-080 1.03523018e-080 2.95682336e-081\n",
            " 3.18236514e-082 1.89560858e-082 6.06482877e-083 5.24042621e-083\n",
            " 3.39096327e-083 2.53402935e-083 1.88878258e-083 1.42922420e-083\n",
            " 2.10667864e-084 1.87956045e-084 3.03570556e-085 2.27514523e-085\n",
            " 1.48188902e-085 4.66483470e-086 3.04868998e-086 1.02919244e-086\n",
            " 2.00154505e-087 1.87091770e-087 1.27026080e-087 3.41869949e-088\n",
            " 2.08578228e-088 6.69399287e-089 4.13490216e-089 2.19877206e-089\n",
            " 1.28535248e-089 4.21814230e-090 4.15578970e-090 2.96536525e-090\n",
            " 8.51955207e-091 6.71287975e-091 6.30683888e-091 5.11200570e-091\n",
            " 4.01802739e-091 1.80887818e-091 1.55686628e-091 1.44439632e-091\n",
            " 7.08521505e-092 5.54121592e-092 9.52750713e-093 7.83937448e-093\n",
            " 1.88607506e-093 1.54508507e-093 7.76739087e-094 4.47009741e-094\n",
            " 2.41016389e-094 1.39700768e-094 3.22703597e-095 2.71106401e-095\n",
            " 6.41704350e-096 5.28092413e-096 9.88951640e-097 9.12021118e-097\n",
            " 2.04159255e-097 1.47983036e-097 1.28734314e-098 4.82136509e-099\n",
            " 2.18997802e-099 2.14197614e-099 1.98539681e-099 9.92133026e-100\n",
            " 3.26367453e-100 1.06041553e-100 1.01112729e-100 3.97329064e-101\n",
            " 2.17301700e-101 3.49929915e-102 1.85322504e-102 3.27890495e-103\n",
            " 2.28411573e-104 1.57471699e-104 2.65091882e-105 1.09079329e-105\n",
            " 3.02429917e-106 7.57860036e-108 2.83370375e-108 1.39874085e-108\n",
            " 5.79315667e-110 2.04092691e-110 4.24607419e-112 3.26588313e-112\n",
            " 2.08429738e-113 8.30868937e-114 8.54594697e-115 5.29907430e-115\n",
            " 3.45317585e-116 1.46091520e-116 1.15304178e-117 3.16831098e-120\n",
            " 1.95181420e-121 1.82078083e-121 4.34749515e-123 3.80688171e-123\n",
            " 9.70363679e-125 8.41812968e-125 4.67628083e-125 2.97751419e-126\n",
            " 5.80817460e-129 5.74859192e-130 3.56408851e-131 1.45694274e-133\n",
            " 6.49354478e-134 1.00736256e-134 8.96631941e-135 8.81435993e-135\n",
            " 2.31337371e-138 5.44749939e-141 2.09805692e-143 1.39887662e-143\n",
            " 1.23978344e-144 3.72340648e-145 8.12215885e-149 1.68588891e-149\n",
            " 1.55232580e-150 4.10988281e-151 3.97887636e-151 5.20301946e-152\n",
            " 3.41120031e-154 6.89564054e-155 6.21060363e-155 4.81420228e-155\n",
            " 9.37666438e-157 5.63565387e-157 2.00586422e-157 3.51947110e-159\n",
            " 1.39478333e-162 1.25107508e-162 8.71948788e-164 6.99292663e-167\n",
            " 2.09257748e-169 6.36523526e-170 3.23621211e-172 1.42682434e-172\n",
            " 2.81142561e-173 7.85662814e-177 2.30788309e-182 7.23069637e-183\n",
            " 1.23071129e-185 3.57642960e-189 1.01064232e-189 3.34647917e-191\n",
            " 1.42075493e-202 1.30199970e-204 3.31598085e-205 6.30395571e-217\n",
            " 1.09800531e-217 1.43706621e-243 3.67428684e-285 9.88131292e-324]\n",
            "(10, 1000)\n"
          ]
        }
      ],
      "source": [
        "topic_vectors = np.random.dirichlet(vocab_size*[beta], n_topics)\n",
        "print('Ordered probabilities for the first topic vector:')\n",
        "print(np.sort(topic_vectors[0])[::-1])\n",
        "print(topic_vectors.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BaQEe-yD6vl_"
      },
      "source": [
        "## 1.4. Generation of document topic proportions and documents for each node\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N-BziCW56vFL",
        "outputId": "c35b12bb-abf9-4933-b9f1-f960f00ed35b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.1, 0.1, 0.1]\n",
            "Ordered probabilities for the first document - node 0 :\n",
            "[0.20571229 0.20072752 0.19095116 0.12529139 0.10952615 0.08526831\n",
            " 0.03080007 0.02737487 0.02410951 0.00023873]\n",
            "Documents of node 0 generated.\n",
            "[0.5, 0.5, 0.5, 0.5, 0.1, 0.1, 0.1, 0.5, 0.5, 0.5]\n",
            "Ordered probabilities for the first document - node 1 :\n",
            "[7.18153484e-01 9.65810990e-02 7.90413436e-02 4.60531346e-02\n",
            " 4.57695851e-02 6.96332629e-03 3.48870404e-03 2.74470401e-03\n",
            " 1.18645654e-03 1.81624802e-05]\n",
            "Documents of node 1 generated.\n",
            "[0.5, 0.1, 0.1, 0.1, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5]\n",
            "Ordered probabilities for the first document - node 2 :\n",
            "[6.64129215e-01 1.72829412e-01 1.00839746e-01 2.97307524e-02\n",
            " 1.84086601e-02 7.45472192e-03 4.61461953e-03 1.74319590e-03\n",
            " 2.32621062e-04 1.70561653e-05]\n",
            "Documents of node 2 generated.\n",
            "[0.1, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.1, 0.1]\n",
            "Ordered probabilities for the first document - node 3 :\n",
            "[0.35040885 0.14670737 0.13604994 0.10887501 0.08640093 0.07581999\n",
            " 0.0640265  0.02934055 0.0020062  0.00036466]\n",
            "Documents of node 3 generated.\n",
            "[0.5, 0.5, 0.5, 0.5, 0.5, 0.1, 0.1, 0.1, 0.5, 0.5]\n",
            "Ordered probabilities for the first document - node 4 :\n",
            "[5.15932289e-01 1.99596022e-01 1.44442477e-01 1.17790028e-01\n",
            " 9.79001161e-03 8.23125113e-03 3.08919208e-03 1.04155465e-03\n",
            " 8.71722981e-05 2.94576291e-09]\n",
            "Documents of node 4 generated.\n"
          ]
        }
      ],
      "source": [
        "doc_topics_all_gt = []\n",
        "documents_all = []\n",
        "z_all = []\n",
        "for i in np.arange(n_nodes):\n",
        "  # Step 2 - generation of document topic proportions for each node\n",
        "  if dirichlet_symmetric:\n",
        "    doc_topics = np.random.dirichlet((n_topics)*[alpha], n_docs)\n",
        "  else:\n",
        "    doc_topics = np.random.dirichlet(prior, n_docs)\n",
        "    prior = rotateArray(prior, len(prior), 3)\n",
        "    print(prior)\n",
        "  print('Ordered probabilities for the first document - node', str(i), ':')\n",
        "  print(np.sort(doc_topics[0])[::-1])\n",
        "  doc_topics_all_gt.append(doc_topics)\n",
        "  # Step 3 - Document generation\n",
        "  documents = [] # Document words\n",
        "  z = [] # Assignments\n",
        "  for docid in np.arange(n_docs):\n",
        "      doc_len = np.random.randint(low=nwords[0], high=nwords[1])\n",
        "      this_doc_words = []\n",
        "      this_doc_assigns = []\n",
        "      for wd_idx in np.arange(doc_len):\n",
        "          tpc = np.nonzero(np.random.multinomial(1, doc_topics[docid]))[0][0]\n",
        "          this_doc_assigns.append(tpc)\n",
        "          word = np.nonzero(np.random.multinomial(1, topic_vectors[tpc]))[0][0]\n",
        "          this_doc_words.append('wd'+str(word))\n",
        "      z.append(this_doc_assigns)\n",
        "      documents.append(this_doc_words)\n",
        "  print(\"Documents of node\", str(i), \"generated.\")\n",
        "  documents_all.append(documents)\n",
        "  z_all.append(z)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HuJUlOIJ69iw"
      },
      "source": [
        "# 2. Preprocessing, generation of training dataset and training of a ProdLDA model at each node"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1XwiP814FZ5U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab52d2b9-5710-4c13-8761-8f2a7b4539a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "call_signature.png  LICENSE.md  \u001b[0m\u001b[01;34mpytorchavitm\u001b[0m/  README.md  train_abs.py\n",
            "\u001b[01;34mdata\u001b[0m/               \u001b[01;34moutputs\u001b[0m/    \u001b[01;34mPyTorchAVITM\u001b[0m/  setup.py   train.py\n"
          ]
        }
      ],
      "source": [
        "ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "14tnh0ndFpb0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a03978f-b3be-4a35-aebf-15db44657842"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/PyTorchAVITM/pytorchavitm/datasets\n"
          ]
        }
      ],
      "source": [
        "cd /content/PyTorchAVITM/pytorchavitm/datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Is7SF6iQcqA"
      },
      "outputs": [],
      "source": [
        "from bow import BOWDataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k4okSYycQaOK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce47eee3-b12a-4f2a-a8de-99c6bbd75691"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/PyTorchAVITM\n"
          ]
        }
      ],
      "source": [
        "cd /content/PyTorchAVITM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JrcVglDoQhU0"
      },
      "outputs": [],
      "source": [
        "from pytorchavitm import AVITM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wchhyQ5bDIhK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ba75c54-83c0-4aaf-ba54-9338d970c08f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Settings: \n",
            "               N Components: 10\n",
            "               Topic Prior Mean: 0.0\n",
            "               Topic Prior Variance: 0.9\n",
            "               Model Type: prodLDA\n",
            "               Hidden Sizes: (100, 100)\n",
            "               Activation: softplus\n",
            "               Dropout: 0.2\n",
            "               Learn Priors: True\n",
            "               Learning Rate: 0.002\n",
            "               Momentum: 0.99\n",
            "               Reduce On Plateau: False\n",
            "               Save Dir: None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [1/100]\tSamples: [1000/100000]\tTrain Loss: 1880.1211953125\tTime: 0:00:00.235274\n",
            "Epoch: [2/100]\tSamples: [2000/100000]\tTrain Loss: 1831.1458515625\tTime: 0:00:00.226442\n",
            "Epoch: [3/100]\tSamples: [3000/100000]\tTrain Loss: 1799.8812265625\tTime: 0:00:00.212776\n",
            "Epoch: [4/100]\tSamples: [4000/100000]\tTrain Loss: 1778.8224453125\tTime: 0:00:00.212842\n",
            "Epoch: [5/100]\tSamples: [5000/100000]\tTrain Loss: 1760.7985\tTime: 0:00:00.219815\n",
            "Epoch: [6/100]\tSamples: [6000/100000]\tTrain Loss: 1758.2469609375\tTime: 0:00:00.198123\n",
            "Epoch: [7/100]\tSamples: [7000/100000]\tTrain Loss: 1744.1291953125\tTime: 0:00:00.220910\n",
            "Epoch: [8/100]\tSamples: [8000/100000]\tTrain Loss: 1738.540453125\tTime: 0:00:00.221149\n",
            "Epoch: [9/100]\tSamples: [9000/100000]\tTrain Loss: 1733.8543984375\tTime: 0:00:00.202841\n",
            "Epoch: [10/100]\tSamples: [10000/100000]\tTrain Loss: 1728.93165625\tTime: 0:00:00.204876\n",
            "Epoch: [11/100]\tSamples: [11000/100000]\tTrain Loss: 1723.24856640625\tTime: 0:00:00.200083\n",
            "Epoch: [12/100]\tSamples: [12000/100000]\tTrain Loss: 1718.3026875\tTime: 0:00:00.218604\n",
            "Epoch: [13/100]\tSamples: [13000/100000]\tTrain Loss: 1713.26590625\tTime: 0:00:00.213134\n",
            "Epoch: [14/100]\tSamples: [14000/100000]\tTrain Loss: 1713.5578828125\tTime: 0:00:00.206940\n",
            "Epoch: [15/100]\tSamples: [15000/100000]\tTrain Loss: 1709.8586484375\tTime: 0:00:00.209582\n",
            "Epoch: [16/100]\tSamples: [16000/100000]\tTrain Loss: 1706.1217890625\tTime: 0:00:00.258202\n",
            "Epoch: [17/100]\tSamples: [17000/100000]\tTrain Loss: 1704.6296484375\tTime: 0:00:00.196569\n",
            "Epoch: [18/100]\tSamples: [18000/100000]\tTrain Loss: 1698.8428203125\tTime: 0:00:00.209306\n",
            "Epoch: [19/100]\tSamples: [19000/100000]\tTrain Loss: 1699.126921875\tTime: 0:00:00.195463\n",
            "Epoch: [20/100]\tSamples: [20000/100000]\tTrain Loss: 1700.7343984375\tTime: 0:00:00.198687\n",
            "Epoch: [21/100]\tSamples: [21000/100000]\tTrain Loss: 1698.070921875\tTime: 0:00:00.204161\n",
            "Epoch: [22/100]\tSamples: [22000/100000]\tTrain Loss: 1691.9969609375\tTime: 0:00:00.255018\n",
            "Epoch: [23/100]\tSamples: [23000/100000]\tTrain Loss: 1687.63142578125\tTime: 0:00:00.305781\n",
            "Epoch: [24/100]\tSamples: [24000/100000]\tTrain Loss: 1687.2205859375\tTime: 0:00:00.212903\n",
            "Epoch: [25/100]\tSamples: [25000/100000]\tTrain Loss: 1679.019984375\tTime: 0:00:00.215271\n",
            "Epoch: [26/100]\tSamples: [26000/100000]\tTrain Loss: 1681.20165625\tTime: 0:00:00.213676\n",
            "Epoch: [27/100]\tSamples: [27000/100000]\tTrain Loss: 1682.955625\tTime: 0:00:00.209994\n",
            "Epoch: [28/100]\tSamples: [28000/100000]\tTrain Loss: 1679.7037109375\tTime: 0:00:00.207055\n",
            "Epoch: [29/100]\tSamples: [29000/100000]\tTrain Loss: 1680.662140625\tTime: 0:00:00.216448\n",
            "Epoch: [30/100]\tSamples: [30000/100000]\tTrain Loss: 1677.0697109375\tTime: 0:00:00.198393\n",
            "Epoch: [31/100]\tSamples: [31000/100000]\tTrain Loss: 1676.9778359375\tTime: 0:00:00.210098\n",
            "Epoch: [32/100]\tSamples: [32000/100000]\tTrain Loss: 1676.4757890625\tTime: 0:00:00.215154\n",
            "Epoch: [33/100]\tSamples: [33000/100000]\tTrain Loss: 1676.27246875\tTime: 0:00:00.201157\n",
            "Epoch: [34/100]\tSamples: [34000/100000]\tTrain Loss: 1673.7048046875\tTime: 0:00:00.203442\n",
            "Epoch: [35/100]\tSamples: [35000/100000]\tTrain Loss: 1676.17853125\tTime: 0:00:00.210843\n",
            "Epoch: [36/100]\tSamples: [36000/100000]\tTrain Loss: 1668.9511015625\tTime: 0:00:00.197482\n",
            "Epoch: [37/100]\tSamples: [37000/100000]\tTrain Loss: 1673.060578125\tTime: 0:00:00.220846\n",
            "Epoch: [38/100]\tSamples: [38000/100000]\tTrain Loss: 1676.5975234375\tTime: 0:00:00.210873\n",
            "Epoch: [39/100]\tSamples: [39000/100000]\tTrain Loss: 1669.83013671875\tTime: 0:00:00.198545\n",
            "Epoch: [40/100]\tSamples: [40000/100000]\tTrain Loss: 1673.475140625\tTime: 0:00:00.208441\n",
            "Epoch: [41/100]\tSamples: [41000/100000]\tTrain Loss: 1674.1821015625\tTime: 0:00:00.205093\n",
            "Epoch: [42/100]\tSamples: [42000/100000]\tTrain Loss: 1671.25958984375\tTime: 0:00:00.208320\n",
            "Epoch: [43/100]\tSamples: [43000/100000]\tTrain Loss: 1665.948984375\tTime: 0:00:00.212723\n",
            "Epoch: [44/100]\tSamples: [44000/100000]\tTrain Loss: 1668.6079140625\tTime: 0:00:00.204556\n",
            "Epoch: [45/100]\tSamples: [45000/100000]\tTrain Loss: 1668.42190625\tTime: 0:00:00.198882\n",
            "Epoch: [46/100]\tSamples: [46000/100000]\tTrain Loss: 1669.61759765625\tTime: 0:00:00.202181\n",
            "Epoch: [47/100]\tSamples: [47000/100000]\tTrain Loss: 1672.409734375\tTime: 0:00:00.221380\n",
            "Epoch: [48/100]\tSamples: [48000/100000]\tTrain Loss: 1668.106515625\tTime: 0:00:00.755866\n",
            "Epoch: [49/100]\tSamples: [49000/100000]\tTrain Loss: 1670.41037890625\tTime: 0:00:00.211851\n",
            "Epoch: [50/100]\tSamples: [50000/100000]\tTrain Loss: 1668.62590234375\tTime: 0:00:00.201049\n",
            "Epoch: [51/100]\tSamples: [51000/100000]\tTrain Loss: 1669.814984375\tTime: 0:00:00.202133\n",
            "Epoch: [52/100]\tSamples: [52000/100000]\tTrain Loss: 1666.4333828125\tTime: 0:00:00.193541\n",
            "Epoch: [53/100]\tSamples: [53000/100000]\tTrain Loss: 1667.5718203125\tTime: 0:00:00.196437\n",
            "Epoch: [54/100]\tSamples: [54000/100000]\tTrain Loss: 1664.521046875\tTime: 0:00:00.206640\n",
            "Epoch: [55/100]\tSamples: [55000/100000]\tTrain Loss: 1669.694828125\tTime: 0:00:00.199261\n",
            "Epoch: [56/100]\tSamples: [56000/100000]\tTrain Loss: 1666.6098125\tTime: 0:00:00.199782\n",
            "Epoch: [57/100]\tSamples: [57000/100000]\tTrain Loss: 1666.662109375\tTime: 0:00:00.206036\n",
            "Epoch: [58/100]\tSamples: [58000/100000]\tTrain Loss: 1666.44756640625\tTime: 0:00:00.213404\n",
            "Epoch: [59/100]\tSamples: [59000/100000]\tTrain Loss: 1666.847453125\tTime: 0:00:00.221567\n",
            "Epoch: [60/100]\tSamples: [60000/100000]\tTrain Loss: 1664.7901328125\tTime: 0:00:00.197035\n",
            "Epoch: [61/100]\tSamples: [61000/100000]\tTrain Loss: 1664.7930078125\tTime: 0:00:00.212596\n",
            "Epoch: [62/100]\tSamples: [62000/100000]\tTrain Loss: 1661.0743984375\tTime: 0:00:00.207016\n",
            "Epoch: [63/100]\tSamples: [63000/100000]\tTrain Loss: 1664.231921875\tTime: 0:00:00.198341\n",
            "Epoch: [64/100]\tSamples: [64000/100000]\tTrain Loss: 1660.829125\tTime: 0:00:00.219714\n",
            "Epoch: [65/100]\tSamples: [65000/100000]\tTrain Loss: 1667.710484375\tTime: 0:00:00.202322\n",
            "Epoch: [66/100]\tSamples: [66000/100000]\tTrain Loss: 1661.7299765625\tTime: 0:00:00.215278\n",
            "Epoch: [67/100]\tSamples: [67000/100000]\tTrain Loss: 1667.8278125\tTime: 0:00:00.207896\n",
            "Epoch: [68/100]\tSamples: [68000/100000]\tTrain Loss: 1662.30480078125\tTime: 0:00:00.205533\n",
            "Epoch: [69/100]\tSamples: [69000/100000]\tTrain Loss: 1667.4252734375\tTime: 0:00:00.231474\n",
            "Epoch: [70/100]\tSamples: [70000/100000]\tTrain Loss: 1663.2351484375\tTime: 0:00:00.205019\n",
            "Epoch: [71/100]\tSamples: [71000/100000]\tTrain Loss: 1663.590109375\tTime: 0:00:00.199407\n",
            "Epoch: [72/100]\tSamples: [72000/100000]\tTrain Loss: 1662.6711796875\tTime: 0:00:00.201446\n",
            "Epoch: [73/100]\tSamples: [73000/100000]\tTrain Loss: 1662.8644609375\tTime: 0:00:00.204498\n",
            "Epoch: [74/100]\tSamples: [74000/100000]\tTrain Loss: 1661.57496875\tTime: 0:00:00.215495\n",
            "Epoch: [75/100]\tSamples: [75000/100000]\tTrain Loss: 1667.688078125\tTime: 0:00:00.198089\n",
            "Epoch: [76/100]\tSamples: [76000/100000]\tTrain Loss: 1662.1183984375\tTime: 0:00:00.202370\n",
            "Epoch: [77/100]\tSamples: [77000/100000]\tTrain Loss: 1661.2889296875\tTime: 0:00:00.205887\n",
            "Epoch: [78/100]\tSamples: [78000/100000]\tTrain Loss: 1667.4515703125\tTime: 0:00:00.212668\n",
            "Epoch: [79/100]\tSamples: [79000/100000]\tTrain Loss: 1664.0154609375\tTime: 0:00:00.208644\n",
            "Epoch: [80/100]\tSamples: [80000/100000]\tTrain Loss: 1661.3078046875\tTime: 0:00:00.206666\n",
            "Epoch: [81/100]\tSamples: [81000/100000]\tTrain Loss: 1666.15165625\tTime: 0:00:00.197753\n",
            "Epoch: [82/100]\tSamples: [82000/100000]\tTrain Loss: 1664.9750234375\tTime: 0:00:00.195994\n",
            "Epoch: [83/100]\tSamples: [83000/100000]\tTrain Loss: 1663.31934765625\tTime: 0:00:00.202838\n",
            "Epoch: [84/100]\tSamples: [84000/100000]\tTrain Loss: 1663.9300625\tTime: 0:00:00.216490\n",
            "Epoch: [85/100]\tSamples: [85000/100000]\tTrain Loss: 1660.84673828125\tTime: 0:00:00.200817\n",
            "Epoch: [86/100]\tSamples: [86000/100000]\tTrain Loss: 1662.537609375\tTime: 0:00:00.206730\n",
            "Epoch: [87/100]\tSamples: [87000/100000]\tTrain Loss: 1661.0341953125\tTime: 0:00:00.209334\n",
            "Epoch: [88/100]\tSamples: [88000/100000]\tTrain Loss: 1660.996625\tTime: 0:00:00.203449\n",
            "Epoch: [89/100]\tSamples: [89000/100000]\tTrain Loss: 1658.91687109375\tTime: 0:00:00.221415\n",
            "Epoch: [90/100]\tSamples: [90000/100000]\tTrain Loss: 1665.65596875\tTime: 0:00:00.200350\n",
            "Epoch: [91/100]\tSamples: [91000/100000]\tTrain Loss: 1656.9565234375\tTime: 0:00:00.200092\n",
            "Epoch: [92/100]\tSamples: [92000/100000]\tTrain Loss: 1658.3794765625\tTime: 0:00:00.210441\n",
            "Epoch: [93/100]\tSamples: [93000/100000]\tTrain Loss: 1663.733859375\tTime: 0:00:00.208122\n",
            "Epoch: [94/100]\tSamples: [94000/100000]\tTrain Loss: 1662.8006328125\tTime: 0:00:00.211923\n",
            "Epoch: [95/100]\tSamples: [95000/100000]\tTrain Loss: 1663.82005078125\tTime: 0:00:00.205465\n",
            "Epoch: [96/100]\tSamples: [96000/100000]\tTrain Loss: 1661.49293359375\tTime: 0:00:00.208763\n",
            "Epoch: [97/100]\tSamples: [97000/100000]\tTrain Loss: 1660.3453671875\tTime: 0:00:00.199410\n",
            "Epoch: [98/100]\tSamples: [98000/100000]\tTrain Loss: 1659.4219765625\tTime: 0:00:00.201935\n",
            "Epoch: [99/100]\tSamples: [99000/100000]\tTrain Loss: 1658.976515625\tTime: 0:00:00.222685\n",
            "Epoch: [100/100]\tSamples: [100000/100000]\tTrain Loss: 1659.975453125\tTime: 0:00:00.193338\n",
            "Settings: \n",
            "               N Components: 10\n",
            "               Topic Prior Mean: 0.0\n",
            "               Topic Prior Variance: 0.9\n",
            "               Model Type: prodLDA\n",
            "               Hidden Sizes: (100, 100)\n",
            "               Activation: softplus\n",
            "               Dropout: 0.2\n",
            "               Learn Priors: True\n",
            "               Learning Rate: 0.002\n",
            "               Momentum: 0.99\n",
            "               Reduce On Plateau: False\n",
            "               Save Dir: None\n",
            "Epoch: [1/100]\tSamples: [1000/100000]\tTrain Loss: 1921.9245234375\tTime: 0:00:00.207401\n",
            "Epoch: [2/100]\tSamples: [2000/100000]\tTrain Loss: 1871.26625\tTime: 0:00:00.206668\n",
            "Epoch: [3/100]\tSamples: [3000/100000]\tTrain Loss: 1841.9990078125\tTime: 0:00:00.209122\n",
            "Epoch: [4/100]\tSamples: [4000/100000]\tTrain Loss: 1814.401171875\tTime: 0:00:00.204786\n",
            "Epoch: [5/100]\tSamples: [5000/100000]\tTrain Loss: 1795.5972890625\tTime: 0:00:00.210383\n",
            "Epoch: [6/100]\tSamples: [6000/100000]\tTrain Loss: 1789.142453125\tTime: 0:00:00.203570\n",
            "Epoch: [7/100]\tSamples: [7000/100000]\tTrain Loss: 1777.48825\tTime: 0:00:00.203342\n",
            "Epoch: [8/100]\tSamples: [8000/100000]\tTrain Loss: 1772.1463359375\tTime: 0:00:00.217020\n",
            "Epoch: [9/100]\tSamples: [9000/100000]\tTrain Loss: 1763.536765625\tTime: 0:00:00.202626\n",
            "Epoch: [10/100]\tSamples: [10000/100000]\tTrain Loss: 1760.1014453125\tTime: 0:00:00.192531\n",
            "Epoch: [11/100]\tSamples: [11000/100000]\tTrain Loss: 1753.0979921875\tTime: 0:00:00.220157\n",
            "Epoch: [12/100]\tSamples: [12000/100000]\tTrain Loss: 1750.0825078125\tTime: 0:00:00.208711\n",
            "Epoch: [13/100]\tSamples: [13000/100000]\tTrain Loss: 1746.1251484375\tTime: 0:00:00.225036\n",
            "Epoch: [14/100]\tSamples: [14000/100000]\tTrain Loss: 1743.7895625\tTime: 0:00:00.222391\n",
            "Epoch: [15/100]\tSamples: [15000/100000]\tTrain Loss: 1735.8355703125\tTime: 0:00:00.205670\n",
            "Epoch: [16/100]\tSamples: [16000/100000]\tTrain Loss: 1735.0878203125\tTime: 0:00:00.217954\n",
            "Epoch: [17/100]\tSamples: [17000/100000]\tTrain Loss: 1728.6847265625\tTime: 0:00:00.215611\n",
            "Epoch: [18/100]\tSamples: [18000/100000]\tTrain Loss: 1729.4107890625\tTime: 0:00:00.192622\n",
            "Epoch: [19/100]\tSamples: [19000/100000]\tTrain Loss: 1731.94615625\tTime: 0:00:00.211093\n",
            "Epoch: [20/100]\tSamples: [20000/100000]\tTrain Loss: 1722.41246875\tTime: 0:00:00.206135\n",
            "Epoch: [21/100]\tSamples: [21000/100000]\tTrain Loss: 1720.1415390625\tTime: 0:00:00.197774\n",
            "Epoch: [22/100]\tSamples: [22000/100000]\tTrain Loss: 1724.18555859375\tTime: 0:00:00.219994\n",
            "Epoch: [23/100]\tSamples: [23000/100000]\tTrain Loss: 1715.8375703125\tTime: 0:00:00.206325\n",
            "Epoch: [24/100]\tSamples: [24000/100000]\tTrain Loss: 1713.7990390625\tTime: 0:00:00.212816\n",
            "Epoch: [25/100]\tSamples: [25000/100000]\tTrain Loss: 1708.4082890625\tTime: 0:00:00.198273\n",
            "Epoch: [26/100]\tSamples: [26000/100000]\tTrain Loss: 1711.76428125\tTime: 0:00:00.208988\n",
            "Epoch: [27/100]\tSamples: [27000/100000]\tTrain Loss: 1712.0333046875\tTime: 0:00:00.218085\n",
            "Epoch: [28/100]\tSamples: [28000/100000]\tTrain Loss: 1707.534296875\tTime: 0:00:00.212704\n",
            "Epoch: [29/100]\tSamples: [29000/100000]\tTrain Loss: 1707.5386171875\tTime: 0:00:00.207755\n",
            "Epoch: [30/100]\tSamples: [30000/100000]\tTrain Loss: 1703.984796875\tTime: 0:00:00.228711\n",
            "Epoch: [31/100]\tSamples: [31000/100000]\tTrain Loss: 1700.4983203125\tTime: 0:00:00.196430\n",
            "Epoch: [32/100]\tSamples: [32000/100000]\tTrain Loss: 1701.744546875\tTime: 0:00:00.207298\n",
            "Epoch: [33/100]\tSamples: [33000/100000]\tTrain Loss: 1701.892390625\tTime: 0:00:00.211820\n",
            "Epoch: [34/100]\tSamples: [34000/100000]\tTrain Loss: 1706.26573828125\tTime: 0:00:00.211027\n",
            "Epoch: [35/100]\tSamples: [35000/100000]\tTrain Loss: 1701.6635078125\tTime: 0:00:00.212393\n",
            "Epoch: [36/100]\tSamples: [36000/100000]\tTrain Loss: 1703.556171875\tTime: 0:00:00.207157\n",
            "Epoch: [37/100]\tSamples: [37000/100000]\tTrain Loss: 1701.2250625\tTime: 0:00:00.231114\n",
            "Epoch: [38/100]\tSamples: [38000/100000]\tTrain Loss: 1703.3988359375\tTime: 0:00:00.196793\n",
            "Epoch: [39/100]\tSamples: [39000/100000]\tTrain Loss: 1699.0370859375\tTime: 0:00:00.201295\n",
            "Epoch: [40/100]\tSamples: [40000/100000]\tTrain Loss: 1694.597109375\tTime: 0:00:00.205563\n",
            "Epoch: [41/100]\tSamples: [41000/100000]\tTrain Loss: 1697.359796875\tTime: 0:00:00.207882\n",
            "Epoch: [42/100]\tSamples: [42000/100000]\tTrain Loss: 1694.93146875\tTime: 0:00:00.212928\n",
            "Epoch: [43/100]\tSamples: [43000/100000]\tTrain Loss: 1698.5372890625\tTime: 0:00:00.202674\n",
            "Epoch: [44/100]\tSamples: [44000/100000]\tTrain Loss: 1692.7719140625\tTime: 0:00:00.188689\n",
            "Epoch: [45/100]\tSamples: [45000/100000]\tTrain Loss: 1690.5863984375\tTime: 0:00:00.205280\n",
            "Epoch: [46/100]\tSamples: [46000/100000]\tTrain Loss: 1694.2031328125\tTime: 0:00:00.212938\n",
            "Epoch: [47/100]\tSamples: [47000/100000]\tTrain Loss: 1693.8961953125\tTime: 0:00:00.206416\n",
            "Epoch: [48/100]\tSamples: [48000/100000]\tTrain Loss: 1697.2375859375\tTime: 0:00:00.191910\n",
            "Epoch: [49/100]\tSamples: [49000/100000]\tTrain Loss: 1693.375109375\tTime: 0:00:00.205653\n",
            "Epoch: [50/100]\tSamples: [50000/100000]\tTrain Loss: 1696.4205703125\tTime: 0:00:00.197816\n",
            "Epoch: [51/100]\tSamples: [51000/100000]\tTrain Loss: 1692.2381328125\tTime: 0:00:00.201110\n",
            "Epoch: [52/100]\tSamples: [52000/100000]\tTrain Loss: 1684.656015625\tTime: 0:00:00.211436\n",
            "Epoch: [53/100]\tSamples: [53000/100000]\tTrain Loss: 1692.92825\tTime: 0:00:00.219306\n",
            "Epoch: [54/100]\tSamples: [54000/100000]\tTrain Loss: 1691.693890625\tTime: 0:00:00.215429\n",
            "Epoch: [55/100]\tSamples: [55000/100000]\tTrain Loss: 1692.7157109375\tTime: 0:00:00.203098\n",
            "Epoch: [56/100]\tSamples: [56000/100000]\tTrain Loss: 1692.0456640625\tTime: 0:00:00.212673\n",
            "Epoch: [57/100]\tSamples: [57000/100000]\tTrain Loss: 1694.18696875\tTime: 0:00:00.215748\n",
            "Epoch: [58/100]\tSamples: [58000/100000]\tTrain Loss: 1684.454484375\tTime: 0:00:00.205549\n",
            "Epoch: [59/100]\tSamples: [59000/100000]\tTrain Loss: 1689.7539296875\tTime: 0:00:00.195639\n",
            "Epoch: [60/100]\tSamples: [60000/100000]\tTrain Loss: 1691.0153515625\tTime: 0:00:00.212454\n",
            "Epoch: [61/100]\tSamples: [61000/100000]\tTrain Loss: 1693.933046875\tTime: 0:00:00.203152\n",
            "Epoch: [62/100]\tSamples: [62000/100000]\tTrain Loss: 1688.3035390625\tTime: 0:00:00.217748\n",
            "Epoch: [63/100]\tSamples: [63000/100000]\tTrain Loss: 1691.0552734375\tTime: 0:00:00.201935\n",
            "Epoch: [64/100]\tSamples: [64000/100000]\tTrain Loss: 1689.032078125\tTime: 0:00:00.213401\n",
            "Epoch: [65/100]\tSamples: [65000/100000]\tTrain Loss: 1683.8313828125\tTime: 0:00:00.197560\n",
            "Epoch: [66/100]\tSamples: [66000/100000]\tTrain Loss: 1691.3846875\tTime: 0:00:00.206204\n",
            "Epoch: [67/100]\tSamples: [67000/100000]\tTrain Loss: 1685.274234375\tTime: 0:00:00.215180\n",
            "Epoch: [68/100]\tSamples: [68000/100000]\tTrain Loss: 1687.44840625\tTime: 0:00:00.188616\n",
            "Epoch: [69/100]\tSamples: [69000/100000]\tTrain Loss: 1688.42\tTime: 0:00:00.206482\n",
            "Epoch: [70/100]\tSamples: [70000/100000]\tTrain Loss: 1686.481546875\tTime: 0:00:00.196950\n",
            "Epoch: [71/100]\tSamples: [71000/100000]\tTrain Loss: 1688.4673125\tTime: 0:00:00.193610\n",
            "Epoch: [72/100]\tSamples: [72000/100000]\tTrain Loss: 1686.692296875\tTime: 0:00:00.210311\n",
            "Epoch: [73/100]\tSamples: [73000/100000]\tTrain Loss: 1687.3532421875\tTime: 0:00:00.217276\n",
            "Epoch: [74/100]\tSamples: [74000/100000]\tTrain Loss: 1684.81846875\tTime: 0:00:00.205933\n",
            "Epoch: [75/100]\tSamples: [75000/100000]\tTrain Loss: 1687.0504296875\tTime: 0:00:00.210023\n",
            "Epoch: [76/100]\tSamples: [76000/100000]\tTrain Loss: 1688.9298046875\tTime: 0:00:00.203615\n",
            "Epoch: [77/100]\tSamples: [77000/100000]\tTrain Loss: 1687.801625\tTime: 0:00:00.213544\n",
            "Epoch: [78/100]\tSamples: [78000/100000]\tTrain Loss: 1693.0301328125\tTime: 0:00:00.198299\n",
            "Epoch: [79/100]\tSamples: [79000/100000]\tTrain Loss: 1687.893140625\tTime: 0:00:00.212485\n",
            "Epoch: [80/100]\tSamples: [80000/100000]\tTrain Loss: 1682.13493359375\tTime: 0:00:00.203425\n",
            "Epoch: [81/100]\tSamples: [81000/100000]\tTrain Loss: 1684.1765078125\tTime: 0:00:00.193137\n",
            "Epoch: [82/100]\tSamples: [82000/100000]\tTrain Loss: 1680.65509375\tTime: 0:00:00.215378\n",
            "Epoch: [83/100]\tSamples: [83000/100000]\tTrain Loss: 1687.2985703125\tTime: 0:00:00.209760\n",
            "Epoch: [84/100]\tSamples: [84000/100000]\tTrain Loss: 1683.547375\tTime: 0:00:00.205481\n",
            "Epoch: [85/100]\tSamples: [85000/100000]\tTrain Loss: 1680.85078515625\tTime: 0:00:00.211146\n",
            "Epoch: [86/100]\tSamples: [86000/100000]\tTrain Loss: 1684.538359375\tTime: 0:00:00.200213\n",
            "Epoch: [87/100]\tSamples: [87000/100000]\tTrain Loss: 1686.1925625\tTime: 0:00:00.214745\n",
            "Epoch: [88/100]\tSamples: [88000/100000]\tTrain Loss: 1685.1928984375\tTime: 0:00:00.218370\n",
            "Epoch: [89/100]\tSamples: [89000/100000]\tTrain Loss: 1685.737140625\tTime: 0:00:00.207901\n",
            "Epoch: [90/100]\tSamples: [90000/100000]\tTrain Loss: 1687.4182109375\tTime: 0:00:00.209215\n",
            "Epoch: [91/100]\tSamples: [91000/100000]\tTrain Loss: 1683.981875\tTime: 0:00:00.200480\n",
            "Epoch: [92/100]\tSamples: [92000/100000]\tTrain Loss: 1690.714640625\tTime: 0:00:00.237674\n",
            "Epoch: [93/100]\tSamples: [93000/100000]\tTrain Loss: 1680.48676953125\tTime: 0:00:00.221848\n",
            "Epoch: [94/100]\tSamples: [94000/100000]\tTrain Loss: 1679.40321875\tTime: 0:00:00.202871\n",
            "Epoch: [95/100]\tSamples: [95000/100000]\tTrain Loss: 1681.84096875\tTime: 0:00:00.206158\n",
            "Epoch: [96/100]\tSamples: [96000/100000]\tTrain Loss: 1685.8730234375\tTime: 0:00:00.206060\n",
            "Epoch: [97/100]\tSamples: [97000/100000]\tTrain Loss: 1686.4110859375\tTime: 0:00:00.206839\n",
            "Epoch: [98/100]\tSamples: [98000/100000]\tTrain Loss: 1685.1548125\tTime: 0:00:00.213445\n",
            "Epoch: [99/100]\tSamples: [99000/100000]\tTrain Loss: 1684.126796875\tTime: 0:00:00.204741\n",
            "Epoch: [100/100]\tSamples: [100000/100000]\tTrain Loss: 1683.674421875\tTime: 0:00:00.201936\n",
            "Settings: \n",
            "               N Components: 10\n",
            "               Topic Prior Mean: 0.0\n",
            "               Topic Prior Variance: 0.9\n",
            "               Model Type: prodLDA\n",
            "               Hidden Sizes: (100, 100)\n",
            "               Activation: softplus\n",
            "               Dropout: 0.2\n",
            "               Learn Priors: True\n",
            "               Learning Rate: 0.002\n",
            "               Momentum: 0.99\n",
            "               Reduce On Plateau: False\n",
            "               Save Dir: None\n",
            "Epoch: [1/100]\tSamples: [1000/100000]\tTrain Loss: 1915.471171875\tTime: 0:00:00.206243\n",
            "Epoch: [2/100]\tSamples: [2000/100000]\tTrain Loss: 1864.7370234375\tTime: 0:00:00.201098\n",
            "Epoch: [3/100]\tSamples: [3000/100000]\tTrain Loss: 1832.2709296875\tTime: 0:00:00.211746\n",
            "Epoch: [4/100]\tSamples: [4000/100000]\tTrain Loss: 1811.46215625\tTime: 0:00:00.199682\n",
            "Epoch: [5/100]\tSamples: [5000/100000]\tTrain Loss: 1795.930734375\tTime: 0:00:00.213654\n",
            "Epoch: [6/100]\tSamples: [6000/100000]\tTrain Loss: 1783.9903515625\tTime: 0:00:00.201440\n",
            "Epoch: [7/100]\tSamples: [7000/100000]\tTrain Loss: 1770.218328125\tTime: 0:00:00.212498\n",
            "Epoch: [8/100]\tSamples: [8000/100000]\tTrain Loss: 1769.5967109375\tTime: 0:00:00.204567\n",
            "Epoch: [9/100]\tSamples: [9000/100000]\tTrain Loss: 1757.9243046875\tTime: 0:00:00.205500\n",
            "Epoch: [10/100]\tSamples: [10000/100000]\tTrain Loss: 1758.3598203125\tTime: 0:00:00.221006\n",
            "Epoch: [11/100]\tSamples: [11000/100000]\tTrain Loss: 1750.3500859375\tTime: 0:00:00.217643\n",
            "Epoch: [12/100]\tSamples: [12000/100000]\tTrain Loss: 1749.4811796875\tTime: 0:00:00.203168\n",
            "Epoch: [13/100]\tSamples: [13000/100000]\tTrain Loss: 1747.9377890625\tTime: 0:00:00.209489\n",
            "Epoch: [14/100]\tSamples: [14000/100000]\tTrain Loss: 1748.05490625\tTime: 0:00:00.206852\n",
            "Epoch: [15/100]\tSamples: [15000/100000]\tTrain Loss: 1744.877203125\tTime: 0:00:00.215731\n",
            "Epoch: [16/100]\tSamples: [16000/100000]\tTrain Loss: 1743.7817734375\tTime: 0:00:00.212609\n",
            "Epoch: [17/100]\tSamples: [17000/100000]\tTrain Loss: 1735.748765625\tTime: 0:00:00.211633\n",
            "Epoch: [18/100]\tSamples: [18000/100000]\tTrain Loss: 1738.0643984375\tTime: 0:00:00.207710\n",
            "Epoch: [19/100]\tSamples: [19000/100000]\tTrain Loss: 1740.50803125\tTime: 0:00:00.215130\n",
            "Epoch: [20/100]\tSamples: [20000/100000]\tTrain Loss: 1733.85283203125\tTime: 0:00:00.232765\n",
            "Epoch: [21/100]\tSamples: [21000/100000]\tTrain Loss: 1733.406234375\tTime: 0:00:00.207899\n",
            "Epoch: [22/100]\tSamples: [22000/100000]\tTrain Loss: 1730.3463203125\tTime: 0:00:00.208455\n",
            "Epoch: [23/100]\tSamples: [23000/100000]\tTrain Loss: 1729.8786640625\tTime: 0:00:00.205376\n",
            "Epoch: [24/100]\tSamples: [24000/100000]\tTrain Loss: 1725.3356953125\tTime: 0:00:00.215285\n",
            "Epoch: [25/100]\tSamples: [25000/100000]\tTrain Loss: 1724.554296875\tTime: 0:00:00.205886\n",
            "Epoch: [26/100]\tSamples: [26000/100000]\tTrain Loss: 1720.06637109375\tTime: 0:00:00.198496\n",
            "Epoch: [27/100]\tSamples: [27000/100000]\tTrain Loss: 1716.6665\tTime: 0:00:00.200989\n",
            "Epoch: [28/100]\tSamples: [28000/100000]\tTrain Loss: 1713.9221015625\tTime: 0:00:00.215633\n",
            "Epoch: [29/100]\tSamples: [29000/100000]\tTrain Loss: 1711.39015625\tTime: 0:00:00.218656\n",
            "Epoch: [30/100]\tSamples: [30000/100000]\tTrain Loss: 1705.5367890625\tTime: 0:00:00.230944\n",
            "Epoch: [31/100]\tSamples: [31000/100000]\tTrain Loss: 1705.8555625\tTime: 0:00:00.205659\n",
            "Epoch: [32/100]\tSamples: [32000/100000]\tTrain Loss: 1706.9920625\tTime: 0:00:00.195334\n",
            "Epoch: [33/100]\tSamples: [33000/100000]\tTrain Loss: 1703.1335625\tTime: 0:00:00.205668\n",
            "Epoch: [34/100]\tSamples: [34000/100000]\tTrain Loss: 1703.4634296875\tTime: 0:00:00.201502\n",
            "Epoch: [35/100]\tSamples: [35000/100000]\tTrain Loss: 1702.4321796875\tTime: 0:00:00.221688\n",
            "Epoch: [36/100]\tSamples: [36000/100000]\tTrain Loss: 1696.543375\tTime: 0:00:00.211387\n",
            "Epoch: [37/100]\tSamples: [37000/100000]\tTrain Loss: 1698.6244375\tTime: 0:00:00.193486\n",
            "Epoch: [38/100]\tSamples: [38000/100000]\tTrain Loss: 1702.066984375\tTime: 0:00:00.218352\n",
            "Epoch: [39/100]\tSamples: [39000/100000]\tTrain Loss: 1692.88059375\tTime: 0:00:00.217537\n",
            "Epoch: [40/100]\tSamples: [40000/100000]\tTrain Loss: 1693.6949296875\tTime: 0:00:00.206639\n",
            "Epoch: [41/100]\tSamples: [41000/100000]\tTrain Loss: 1693.8703515625\tTime: 0:00:00.207414\n",
            "Epoch: [42/100]\tSamples: [42000/100000]\tTrain Loss: 1695.9134921875\tTime: 0:00:00.201332\n",
            "Epoch: [43/100]\tSamples: [43000/100000]\tTrain Loss: 1694.548890625\tTime: 0:00:00.206523\n",
            "Epoch: [44/100]\tSamples: [44000/100000]\tTrain Loss: 1689.9086171875\tTime: 0:00:00.214640\n",
            "Epoch: [45/100]\tSamples: [45000/100000]\tTrain Loss: 1695.409890625\tTime: 0:00:00.203064\n",
            "Epoch: [46/100]\tSamples: [46000/100000]\tTrain Loss: 1692.1462734375\tTime: 0:00:00.200002\n",
            "Epoch: [47/100]\tSamples: [47000/100000]\tTrain Loss: 1688.43521875\tTime: 0:00:00.199206\n",
            "Epoch: [48/100]\tSamples: [48000/100000]\tTrain Loss: 1690.894609375\tTime: 0:00:00.210349\n",
            "Epoch: [49/100]\tSamples: [49000/100000]\tTrain Loss: 1691.91375\tTime: 0:00:00.220386\n",
            "Epoch: [50/100]\tSamples: [50000/100000]\tTrain Loss: 1690.0413359375\tTime: 0:00:00.193226\n",
            "Epoch: [51/100]\tSamples: [51000/100000]\tTrain Loss: 1693.02209375\tTime: 0:00:00.217244\n",
            "Epoch: [52/100]\tSamples: [52000/100000]\tTrain Loss: 1691.9484296875\tTime: 0:00:00.204530\n",
            "Epoch: [53/100]\tSamples: [53000/100000]\tTrain Loss: 1689.8634375\tTime: 0:00:00.196820\n",
            "Epoch: [54/100]\tSamples: [54000/100000]\tTrain Loss: 1685.7757265625\tTime: 0:00:00.221167\n",
            "Epoch: [55/100]\tSamples: [55000/100000]\tTrain Loss: 1694.4581953125\tTime: 0:00:00.210350\n",
            "Epoch: [56/100]\tSamples: [56000/100000]\tTrain Loss: 1691.69128125\tTime: 0:00:00.202049\n",
            "Epoch: [57/100]\tSamples: [57000/100000]\tTrain Loss: 1688.20519921875\tTime: 0:00:00.199758\n",
            "Epoch: [58/100]\tSamples: [58000/100000]\tTrain Loss: 1689.4736875\tTime: 0:00:00.219897\n",
            "Epoch: [59/100]\tSamples: [59000/100000]\tTrain Loss: 1690.7735390625\tTime: 0:00:00.218586\n",
            "Epoch: [60/100]\tSamples: [60000/100000]\tTrain Loss: 1686.03025\tTime: 0:00:00.205331\n",
            "Epoch: [61/100]\tSamples: [61000/100000]\tTrain Loss: 1677.748359375\tTime: 0:00:00.199583\n",
            "Epoch: [62/100]\tSamples: [62000/100000]\tTrain Loss: 1682.981046875\tTime: 0:00:00.197228\n",
            "Epoch: [63/100]\tSamples: [63000/100000]\tTrain Loss: 1687.08609375\tTime: 0:00:00.207068\n",
            "Epoch: [64/100]\tSamples: [64000/100000]\tTrain Loss: 1685.288796875\tTime: 0:00:00.223469\n",
            "Epoch: [65/100]\tSamples: [65000/100000]\tTrain Loss: 1684.18844140625\tTime: 0:00:00.218154\n",
            "Epoch: [66/100]\tSamples: [66000/100000]\tTrain Loss: 1689.3817578125\tTime: 0:00:00.197991\n",
            "Epoch: [67/100]\tSamples: [67000/100000]\tTrain Loss: 1685.99715625\tTime: 0:00:00.208203\n",
            "Epoch: [68/100]\tSamples: [68000/100000]\tTrain Loss: 1688.1400703125\tTime: 0:00:00.222445\n",
            "Epoch: [69/100]\tSamples: [69000/100000]\tTrain Loss: 1688.339265625\tTime: 0:00:00.215225\n",
            "Epoch: [70/100]\tSamples: [70000/100000]\tTrain Loss: 1686.6222265625\tTime: 0:00:00.212434\n",
            "Epoch: [71/100]\tSamples: [71000/100000]\tTrain Loss: 1685.4797890625\tTime: 0:00:00.205374\n",
            "Epoch: [72/100]\tSamples: [72000/100000]\tTrain Loss: 1687.1505\tTime: 0:00:00.196926\n",
            "Epoch: [73/100]\tSamples: [73000/100000]\tTrain Loss: 1682.9636171875\tTime: 0:00:00.206482\n",
            "Epoch: [74/100]\tSamples: [74000/100000]\tTrain Loss: 1683.737390625\tTime: 0:00:00.214889\n",
            "Epoch: [75/100]\tSamples: [75000/100000]\tTrain Loss: 1688.0061796875\tTime: 0:00:00.197019\n",
            "Epoch: [76/100]\tSamples: [76000/100000]\tTrain Loss: 1684.81890625\tTime: 0:00:00.200729\n",
            "Epoch: [77/100]\tSamples: [77000/100000]\tTrain Loss: 1688.790828125\tTime: 0:00:00.215812\n",
            "Epoch: [78/100]\tSamples: [78000/100000]\tTrain Loss: 1682.6000625\tTime: 0:00:00.217996\n",
            "Epoch: [79/100]\tSamples: [79000/100000]\tTrain Loss: 1683.0728515625\tTime: 0:00:00.221948\n",
            "Epoch: [80/100]\tSamples: [80000/100000]\tTrain Loss: 1684.520171875\tTime: 0:00:00.211951\n",
            "Epoch: [81/100]\tSamples: [81000/100000]\tTrain Loss: 1682.30566015625\tTime: 0:00:00.202598\n",
            "Epoch: [82/100]\tSamples: [82000/100000]\tTrain Loss: 1683.6234296875\tTime: 0:00:00.194388\n",
            "Epoch: [83/100]\tSamples: [83000/100000]\tTrain Loss: 1684.8526328125\tTime: 0:00:00.213643\n",
            "Epoch: [84/100]\tSamples: [84000/100000]\tTrain Loss: 1679.641625\tTime: 0:00:00.213194\n",
            "Epoch: [85/100]\tSamples: [85000/100000]\tTrain Loss: 1680.7505078125\tTime: 0:00:00.210064\n",
            "Epoch: [86/100]\tSamples: [86000/100000]\tTrain Loss: 1679.7593125\tTime: 0:00:00.212046\n",
            "Epoch: [87/100]\tSamples: [87000/100000]\tTrain Loss: 1684.1445390625\tTime: 0:00:00.209764\n",
            "Epoch: [88/100]\tSamples: [88000/100000]\tTrain Loss: 1680.66990625\tTime: 0:00:00.202641\n",
            "Epoch: [89/100]\tSamples: [89000/100000]\tTrain Loss: 1687.30709375\tTime: 0:00:00.216130\n",
            "Epoch: [90/100]\tSamples: [90000/100000]\tTrain Loss: 1680.4208359375\tTime: 0:00:00.208177\n",
            "Epoch: [91/100]\tSamples: [91000/100000]\tTrain Loss: 1685.41590625\tTime: 0:00:00.202760\n",
            "Epoch: [92/100]\tSamples: [92000/100000]\tTrain Loss: 1678.214\tTime: 0:00:00.216160\n",
            "Epoch: [93/100]\tSamples: [93000/100000]\tTrain Loss: 1682.6695234375\tTime: 0:00:00.218214\n",
            "Epoch: [94/100]\tSamples: [94000/100000]\tTrain Loss: 1685.464453125\tTime: 0:00:00.217345\n",
            "Epoch: [95/100]\tSamples: [95000/100000]\tTrain Loss: 1680.8312734375\tTime: 0:00:00.206400\n",
            "Epoch: [96/100]\tSamples: [96000/100000]\tTrain Loss: 1678.65359375\tTime: 0:00:00.216220\n",
            "Epoch: [97/100]\tSamples: [97000/100000]\tTrain Loss: 1681.9791953125\tTime: 0:00:00.214274\n",
            "Epoch: [98/100]\tSamples: [98000/100000]\tTrain Loss: 1681.4075078125\tTime: 0:00:00.217265\n",
            "Epoch: [99/100]\tSamples: [99000/100000]\tTrain Loss: 1680.5308125\tTime: 0:00:00.197057\n",
            "Epoch: [100/100]\tSamples: [100000/100000]\tTrain Loss: 1681.10648828125\tTime: 0:00:00.201040\n",
            "Settings: \n",
            "               N Components: 10\n",
            "               Topic Prior Mean: 0.0\n",
            "               Topic Prior Variance: 0.9\n",
            "               Model Type: prodLDA\n",
            "               Hidden Sizes: (100, 100)\n",
            "               Activation: softplus\n",
            "               Dropout: 0.2\n",
            "               Learn Priors: True\n",
            "               Learning Rate: 0.002\n",
            "               Momentum: 0.99\n",
            "               Reduce On Plateau: False\n",
            "               Save Dir: None\n",
            "Epoch: [1/100]\tSamples: [1000/100000]\tTrain Loss: 1926.024390625\tTime: 0:00:00.220484\n",
            "Epoch: [2/100]\tSamples: [2000/100000]\tTrain Loss: 1875.708671875\tTime: 0:00:00.218523\n",
            "Epoch: [3/100]\tSamples: [3000/100000]\tTrain Loss: 1844.2640859375\tTime: 0:00:00.202992\n",
            "Epoch: [4/100]\tSamples: [4000/100000]\tTrain Loss: 1825.6920625\tTime: 0:00:00.210301\n",
            "Epoch: [5/100]\tSamples: [5000/100000]\tTrain Loss: 1804.8821484375\tTime: 0:00:00.219778\n",
            "Epoch: [6/100]\tSamples: [6000/100000]\tTrain Loss: 1796.5351640625\tTime: 0:00:00.210569\n",
            "Epoch: [7/100]\tSamples: [7000/100000]\tTrain Loss: 1786.18953125\tTime: 0:00:00.217006\n",
            "Epoch: [8/100]\tSamples: [8000/100000]\tTrain Loss: 1779.055203125\tTime: 0:00:00.197559\n",
            "Epoch: [9/100]\tSamples: [9000/100000]\tTrain Loss: 1769.887515625\tTime: 0:00:00.209157\n",
            "Epoch: [10/100]\tSamples: [10000/100000]\tTrain Loss: 1769.0978515625\tTime: 0:00:00.190539\n",
            "Epoch: [11/100]\tSamples: [11000/100000]\tTrain Loss: 1762.859484375\tTime: 0:00:00.197298\n",
            "Epoch: [12/100]\tSamples: [12000/100000]\tTrain Loss: 1762.6200859375\tTime: 0:00:00.219607\n",
            "Epoch: [13/100]\tSamples: [13000/100000]\tTrain Loss: 1762.4773125\tTime: 0:00:00.202787\n",
            "Epoch: [14/100]\tSamples: [14000/100000]\tTrain Loss: 1757.86515625\tTime: 0:00:00.207677\n",
            "Epoch: [15/100]\tSamples: [15000/100000]\tTrain Loss: 1754.1347578125\tTime: 0:00:00.213938\n",
            "Epoch: [16/100]\tSamples: [16000/100000]\tTrain Loss: 1750.5261796875\tTime: 0:00:00.210775\n",
            "Epoch: [17/100]\tSamples: [17000/100000]\tTrain Loss: 1742.9410078125\tTime: 0:00:00.218522\n",
            "Epoch: [18/100]\tSamples: [18000/100000]\tTrain Loss: 1741.505703125\tTime: 0:00:00.198865\n",
            "Epoch: [19/100]\tSamples: [19000/100000]\tTrain Loss: 1737.8119609375\tTime: 0:00:00.203084\n",
            "Epoch: [20/100]\tSamples: [20000/100000]\tTrain Loss: 1734.9928359375\tTime: 0:00:00.210087\n",
            "Epoch: [21/100]\tSamples: [21000/100000]\tTrain Loss: 1732.51421875\tTime: 0:00:00.222252\n",
            "Epoch: [22/100]\tSamples: [22000/100000]\tTrain Loss: 1730.365359375\tTime: 0:00:00.210353\n",
            "Epoch: [23/100]\tSamples: [23000/100000]\tTrain Loss: 1734.2717421875\tTime: 0:00:00.210333\n",
            "Epoch: [24/100]\tSamples: [24000/100000]\tTrain Loss: 1726.89628125\tTime: 0:00:00.211822\n",
            "Epoch: [25/100]\tSamples: [25000/100000]\tTrain Loss: 1726.8298671875\tTime: 0:00:00.210545\n",
            "Epoch: [26/100]\tSamples: [26000/100000]\tTrain Loss: 1718.2360625\tTime: 0:00:00.213478\n",
            "Epoch: [27/100]\tSamples: [27000/100000]\tTrain Loss: 1722.2600234375\tTime: 0:00:00.217706\n",
            "Epoch: [28/100]\tSamples: [28000/100000]\tTrain Loss: 1718.0233125\tTime: 0:00:00.214775\n",
            "Epoch: [29/100]\tSamples: [29000/100000]\tTrain Loss: 1720.8147109375\tTime: 0:00:00.211315\n",
            "Epoch: [30/100]\tSamples: [30000/100000]\tTrain Loss: 1717.8465234375\tTime: 0:00:00.205318\n",
            "Epoch: [31/100]\tSamples: [31000/100000]\tTrain Loss: 1720.9724296875\tTime: 0:00:00.202343\n",
            "Epoch: [32/100]\tSamples: [32000/100000]\tTrain Loss: 1716.42534375\tTime: 0:00:00.212814\n",
            "Epoch: [33/100]\tSamples: [33000/100000]\tTrain Loss: 1717.604578125\tTime: 0:00:00.205913\n",
            "Epoch: [34/100]\tSamples: [34000/100000]\tTrain Loss: 1717.20403125\tTime: 0:00:00.210957\n",
            "Epoch: [35/100]\tSamples: [35000/100000]\tTrain Loss: 1718.21090234375\tTime: 0:00:00.208433\n",
            "Epoch: [36/100]\tSamples: [36000/100000]\tTrain Loss: 1717.05309375\tTime: 0:00:00.202729\n",
            "Epoch: [37/100]\tSamples: [37000/100000]\tTrain Loss: 1713.3362421875\tTime: 0:00:00.209884\n",
            "Epoch: [38/100]\tSamples: [38000/100000]\tTrain Loss: 1715.159796875\tTime: 0:00:00.205544\n",
            "Epoch: [39/100]\tSamples: [39000/100000]\tTrain Loss: 1709.9709609375\tTime: 0:00:00.209664\n",
            "Epoch: [40/100]\tSamples: [40000/100000]\tTrain Loss: 1711.2022265625\tTime: 0:00:00.219864\n",
            "Epoch: [41/100]\tSamples: [41000/100000]\tTrain Loss: 1706.77003125\tTime: 0:00:00.204872\n",
            "Epoch: [42/100]\tSamples: [42000/100000]\tTrain Loss: 1712.4882265625\tTime: 0:00:00.220341\n",
            "Epoch: [43/100]\tSamples: [43000/100000]\tTrain Loss: 1707.2817109375\tTime: 0:00:00.211795\n",
            "Epoch: [44/100]\tSamples: [44000/100000]\tTrain Loss: 1705.1169765625\tTime: 0:00:00.223401\n",
            "Epoch: [45/100]\tSamples: [45000/100000]\tTrain Loss: 1705.567125\tTime: 0:00:00.208248\n",
            "Epoch: [46/100]\tSamples: [46000/100000]\tTrain Loss: 1708.166796875\tTime: 0:00:00.224478\n",
            "Epoch: [47/100]\tSamples: [47000/100000]\tTrain Loss: 1710.34717578125\tTime: 0:00:00.224818\n",
            "Epoch: [48/100]\tSamples: [48000/100000]\tTrain Loss: 1711.4112734375\tTime: 0:00:00.208680\n",
            "Epoch: [49/100]\tSamples: [49000/100000]\tTrain Loss: 1705.3063203125\tTime: 0:00:00.203466\n",
            "Epoch: [50/100]\tSamples: [50000/100000]\tTrain Loss: 1705.316296875\tTime: 0:00:00.208581\n",
            "Epoch: [51/100]\tSamples: [51000/100000]\tTrain Loss: 1708.68242578125\tTime: 0:00:00.221914\n",
            "Epoch: [52/100]\tSamples: [52000/100000]\tTrain Loss: 1706.8350625\tTime: 0:00:00.224435\n",
            "Epoch: [53/100]\tSamples: [53000/100000]\tTrain Loss: 1705.29393359375\tTime: 0:00:00.226962\n",
            "Epoch: [54/100]\tSamples: [54000/100000]\tTrain Loss: 1702.9848828125\tTime: 0:00:00.207866\n",
            "Epoch: [55/100]\tSamples: [55000/100000]\tTrain Loss: 1705.71094921875\tTime: 0:00:00.202611\n",
            "Epoch: [56/100]\tSamples: [56000/100000]\tTrain Loss: 1701.5038359375\tTime: 0:00:00.223994\n",
            "Epoch: [57/100]\tSamples: [57000/100000]\tTrain Loss: 1700.74533984375\tTime: 0:00:00.220970\n",
            "Epoch: [58/100]\tSamples: [58000/100000]\tTrain Loss: 1704.438625\tTime: 0:00:00.209369\n",
            "Epoch: [59/100]\tSamples: [59000/100000]\tTrain Loss: 1706.984859375\tTime: 0:00:00.213863\n",
            "Epoch: [60/100]\tSamples: [60000/100000]\tTrain Loss: 1706.8805703125\tTime: 0:00:00.207153\n",
            "Epoch: [61/100]\tSamples: [61000/100000]\tTrain Loss: 1703.5598828125\tTime: 0:00:00.233037\n",
            "Epoch: [62/100]\tSamples: [62000/100000]\tTrain Loss: 1703.568203125\tTime: 0:00:00.210221\n",
            "Epoch: [63/100]\tSamples: [63000/100000]\tTrain Loss: 1703.6893125\tTime: 0:00:00.208948\n",
            "Epoch: [64/100]\tSamples: [64000/100000]\tTrain Loss: 1703.3252421875\tTime: 0:00:00.206377\n",
            "Epoch: [65/100]\tSamples: [65000/100000]\tTrain Loss: 1704.77203125\tTime: 0:00:00.207642\n",
            "Epoch: [66/100]\tSamples: [66000/100000]\tTrain Loss: 1705.103859375\tTime: 0:00:00.222382\n",
            "Epoch: [67/100]\tSamples: [67000/100000]\tTrain Loss: 1702.7504453125\tTime: 0:00:00.205804\n",
            "Epoch: [68/100]\tSamples: [68000/100000]\tTrain Loss: 1700.934203125\tTime: 0:00:00.197797\n",
            "Epoch: [69/100]\tSamples: [69000/100000]\tTrain Loss: 1703.0289296875\tTime: 0:00:00.205531\n",
            "Epoch: [70/100]\tSamples: [70000/100000]\tTrain Loss: 1701.145765625\tTime: 0:00:00.216998\n",
            "Epoch: [71/100]\tSamples: [71000/100000]\tTrain Loss: 1698.7513515625\tTime: 0:00:00.227267\n",
            "Epoch: [72/100]\tSamples: [72000/100000]\tTrain Loss: 1697.9003359375\tTime: 0:00:00.217173\n",
            "Epoch: [73/100]\tSamples: [73000/100000]\tTrain Loss: 1697.54015625\tTime: 0:00:00.207950\n",
            "Epoch: [74/100]\tSamples: [74000/100000]\tTrain Loss: 1697.0215703125\tTime: 0:00:00.206364\n",
            "Epoch: [75/100]\tSamples: [75000/100000]\tTrain Loss: 1698.5498203125\tTime: 0:00:00.206310\n",
            "Epoch: [76/100]\tSamples: [76000/100000]\tTrain Loss: 1700.6093203125\tTime: 0:00:00.213764\n",
            "Epoch: [77/100]\tSamples: [77000/100000]\tTrain Loss: 1702.12397265625\tTime: 0:00:00.205700\n",
            "Epoch: [78/100]\tSamples: [78000/100000]\tTrain Loss: 1698.20621875\tTime: 0:00:00.191573\n",
            "Epoch: [79/100]\tSamples: [79000/100000]\tTrain Loss: 1699.955828125\tTime: 0:00:00.206644\n",
            "Epoch: [80/100]\tSamples: [80000/100000]\tTrain Loss: 1701.518765625\tTime: 0:00:00.236954\n",
            "Epoch: [81/100]\tSamples: [81000/100000]\tTrain Loss: 1700.2994453125\tTime: 0:00:00.203396\n",
            "Epoch: [82/100]\tSamples: [82000/100000]\tTrain Loss: 1699.9392890625\tTime: 0:00:00.214343\n",
            "Epoch: [83/100]\tSamples: [83000/100000]\tTrain Loss: 1700.74659375\tTime: 0:00:00.206783\n",
            "Epoch: [84/100]\tSamples: [84000/100000]\tTrain Loss: 1702.121375\tTime: 0:00:00.203899\n",
            "Epoch: [85/100]\tSamples: [85000/100000]\tTrain Loss: 1699.2862265625\tTime: 0:00:00.224207\n",
            "Epoch: [86/100]\tSamples: [86000/100000]\tTrain Loss: 1696.8455234375\tTime: 0:00:00.215990\n",
            "Epoch: [87/100]\tSamples: [87000/100000]\tTrain Loss: 1699.2159375\tTime: 0:00:00.214276\n",
            "Epoch: [88/100]\tSamples: [88000/100000]\tTrain Loss: 1698.416984375\tTime: 0:00:00.220298\n",
            "Epoch: [89/100]\tSamples: [89000/100000]\tTrain Loss: 1698.745390625\tTime: 0:00:00.210244\n",
            "Epoch: [90/100]\tSamples: [90000/100000]\tTrain Loss: 1693.424734375\tTime: 0:00:00.236834\n",
            "Epoch: [91/100]\tSamples: [91000/100000]\tTrain Loss: 1698.8051015625\tTime: 0:00:00.200333\n",
            "Epoch: [92/100]\tSamples: [92000/100000]\tTrain Loss: 1697.4388828125\tTime: 0:00:00.206864\n",
            "Epoch: [93/100]\tSamples: [93000/100000]\tTrain Loss: 1700.88875\tTime: 0:00:00.204233\n",
            "Epoch: [94/100]\tSamples: [94000/100000]\tTrain Loss: 1698.42275\tTime: 0:00:00.205611\n",
            "Epoch: [95/100]\tSamples: [95000/100000]\tTrain Loss: 1697.574015625\tTime: 0:00:00.212290\n",
            "Epoch: [96/100]\tSamples: [96000/100000]\tTrain Loss: 1700.03853125\tTime: 0:00:00.216419\n",
            "Epoch: [97/100]\tSamples: [97000/100000]\tTrain Loss: 1691.159984375\tTime: 0:00:00.197626\n",
            "Epoch: [98/100]\tSamples: [98000/100000]\tTrain Loss: 1692.6119375\tTime: 0:00:00.200580\n",
            "Epoch: [99/100]\tSamples: [99000/100000]\tTrain Loss: 1694.1203671875\tTime: 0:00:00.222380\n",
            "Epoch: [100/100]\tSamples: [100000/100000]\tTrain Loss: 1696.35184375\tTime: 0:00:00.228993\n",
            "Settings: \n",
            "               N Components: 10\n",
            "               Topic Prior Mean: 0.0\n",
            "               Topic Prior Variance: 0.9\n",
            "               Model Type: prodLDA\n",
            "               Hidden Sizes: (100, 100)\n",
            "               Activation: softplus\n",
            "               Dropout: 0.2\n",
            "               Learn Priors: True\n",
            "               Learning Rate: 0.002\n",
            "               Momentum: 0.99\n",
            "               Reduce On Plateau: False\n",
            "               Save Dir: None\n",
            "Epoch: [1/100]\tSamples: [1000/100000]\tTrain Loss: 1904.181875\tTime: 0:00:00.203760\n",
            "Epoch: [2/100]\tSamples: [2000/100000]\tTrain Loss: 1856.2917109375\tTime: 0:00:00.204462\n",
            "Epoch: [3/100]\tSamples: [3000/100000]\tTrain Loss: 1826.440515625\tTime: 0:00:00.194760\n",
            "Epoch: [4/100]\tSamples: [4000/100000]\tTrain Loss: 1800.79825\tTime: 0:00:00.223433\n",
            "Epoch: [5/100]\tSamples: [5000/100000]\tTrain Loss: 1787.246015625\tTime: 0:00:00.216387\n",
            "Epoch: [6/100]\tSamples: [6000/100000]\tTrain Loss: 1770.529359375\tTime: 0:00:00.210601\n",
            "Epoch: [7/100]\tSamples: [7000/100000]\tTrain Loss: 1765.0987421875\tTime: 0:00:00.212812\n",
            "Epoch: [8/100]\tSamples: [8000/100000]\tTrain Loss: 1754.5667109375\tTime: 0:00:00.235647\n",
            "Epoch: [9/100]\tSamples: [9000/100000]\tTrain Loss: 1749.50571875\tTime: 0:00:00.208999\n",
            "Epoch: [10/100]\tSamples: [10000/100000]\tTrain Loss: 1743.4977421875\tTime: 0:00:00.207047\n",
            "Epoch: [11/100]\tSamples: [11000/100000]\tTrain Loss: 1736.359203125\tTime: 0:00:00.197440\n",
            "Epoch: [12/100]\tSamples: [12000/100000]\tTrain Loss: 1733.5822890625\tTime: 0:00:00.208843\n",
            "Epoch: [13/100]\tSamples: [13000/100000]\tTrain Loss: 1729.1016796875\tTime: 0:00:00.211315\n",
            "Epoch: [14/100]\tSamples: [14000/100000]\tTrain Loss: 1724.59575\tTime: 0:00:00.208215\n",
            "Epoch: [15/100]\tSamples: [15000/100000]\tTrain Loss: 1723.1336171875\tTime: 0:00:00.218626\n",
            "Epoch: [16/100]\tSamples: [16000/100000]\tTrain Loss: 1717.1115625\tTime: 0:00:00.197049\n",
            "Epoch: [17/100]\tSamples: [17000/100000]\tTrain Loss: 1714.6096171875\tTime: 0:00:00.225161\n",
            "Epoch: [18/100]\tSamples: [18000/100000]\tTrain Loss: 1713.349640625\tTime: 0:00:00.215575\n",
            "Epoch: [19/100]\tSamples: [19000/100000]\tTrain Loss: 1718.167921875\tTime: 0:00:00.208143\n",
            "Epoch: [20/100]\tSamples: [20000/100000]\tTrain Loss: 1713.68821875\tTime: 0:00:00.222511\n",
            "Epoch: [21/100]\tSamples: [21000/100000]\tTrain Loss: 1706.1110390625\tTime: 0:00:00.212190\n",
            "Epoch: [22/100]\tSamples: [22000/100000]\tTrain Loss: 1707.7721171875\tTime: 0:00:00.221369\n",
            "Epoch: [23/100]\tSamples: [23000/100000]\tTrain Loss: 1701.8371640625\tTime: 0:00:00.225877\n",
            "Epoch: [24/100]\tSamples: [24000/100000]\tTrain Loss: 1703.7143359375\tTime: 0:00:00.205280\n",
            "Epoch: [25/100]\tSamples: [25000/100000]\tTrain Loss: 1697.5328125\tTime: 0:00:00.208053\n",
            "Epoch: [26/100]\tSamples: [26000/100000]\tTrain Loss: 1700.35240625\tTime: 0:00:00.211802\n",
            "Epoch: [27/100]\tSamples: [27000/100000]\tTrain Loss: 1695.1155625\tTime: 0:00:00.217109\n",
            "Epoch: [28/100]\tSamples: [28000/100000]\tTrain Loss: 1692.4811171875\tTime: 0:00:00.231112\n",
            "Epoch: [29/100]\tSamples: [29000/100000]\tTrain Loss: 1691.62255078125\tTime: 0:00:00.209226\n",
            "Epoch: [30/100]\tSamples: [30000/100000]\tTrain Loss: 1694.37209375\tTime: 0:00:00.209069\n",
            "Epoch: [31/100]\tSamples: [31000/100000]\tTrain Loss: 1691.4546640625\tTime: 0:00:00.208652\n",
            "Epoch: [32/100]\tSamples: [32000/100000]\tTrain Loss: 1693.16139453125\tTime: 0:00:00.212670\n",
            "Epoch: [33/100]\tSamples: [33000/100000]\tTrain Loss: 1687.7628125\tTime: 0:00:00.204759\n",
            "Epoch: [34/100]\tSamples: [34000/100000]\tTrain Loss: 1686.687875\tTime: 0:00:00.208924\n",
            "Epoch: [35/100]\tSamples: [35000/100000]\tTrain Loss: 1684.1556328125\tTime: 0:00:00.220781\n",
            "Epoch: [36/100]\tSamples: [36000/100000]\tTrain Loss: 1682.1024453125\tTime: 0:00:00.219335\n",
            "Epoch: [37/100]\tSamples: [37000/100000]\tTrain Loss: 1681.69175390625\tTime: 0:00:00.218403\n",
            "Epoch: [38/100]\tSamples: [38000/100000]\tTrain Loss: 1686.090640625\tTime: 0:00:00.205686\n",
            "Epoch: [39/100]\tSamples: [39000/100000]\tTrain Loss: 1686.125171875\tTime: 0:00:00.203168\n",
            "Epoch: [40/100]\tSamples: [40000/100000]\tTrain Loss: 1682.0267265625\tTime: 0:00:00.218156\n",
            "Epoch: [41/100]\tSamples: [41000/100000]\tTrain Loss: 1683.1220625\tTime: 0:00:00.211232\n",
            "Epoch: [42/100]\tSamples: [42000/100000]\tTrain Loss: 1682.43859375\tTime: 0:00:00.222811\n",
            "Epoch: [43/100]\tSamples: [43000/100000]\tTrain Loss: 1681.6610859375\tTime: 0:00:00.198858\n",
            "Epoch: [44/100]\tSamples: [44000/100000]\tTrain Loss: 1674.5177421875\tTime: 0:00:00.213814\n",
            "Epoch: [45/100]\tSamples: [45000/100000]\tTrain Loss: 1679.3425546875\tTime: 0:00:00.232726\n",
            "Epoch: [46/100]\tSamples: [46000/100000]\tTrain Loss: 1681.5824453125\tTime: 0:00:00.207823\n",
            "Epoch: [47/100]\tSamples: [47000/100000]\tTrain Loss: 1681.0574296875\tTime: 0:00:00.224092\n",
            "Epoch: [48/100]\tSamples: [48000/100000]\tTrain Loss: 1678.2631953125\tTime: 0:00:00.213287\n",
            "Epoch: [49/100]\tSamples: [49000/100000]\tTrain Loss: 1675.7155234375\tTime: 0:00:00.208844\n",
            "Epoch: [50/100]\tSamples: [50000/100000]\tTrain Loss: 1683.1813046875\tTime: 0:00:00.205788\n",
            "Epoch: [51/100]\tSamples: [51000/100000]\tTrain Loss: 1684.344234375\tTime: 0:00:00.204885\n",
            "Epoch: [52/100]\tSamples: [52000/100000]\tTrain Loss: 1680.8698359375\tTime: 0:00:00.216188\n",
            "Epoch: [53/100]\tSamples: [53000/100000]\tTrain Loss: 1679.02434375\tTime: 0:00:00.209306\n",
            "Epoch: [54/100]\tSamples: [54000/100000]\tTrain Loss: 1680.4603359375\tTime: 0:00:00.219368\n",
            "Epoch: [55/100]\tSamples: [55000/100000]\tTrain Loss: 1675.76886328125\tTime: 0:00:00.220409\n",
            "Epoch: [56/100]\tSamples: [56000/100000]\tTrain Loss: 1679.642546875\tTime: 0:00:00.214900\n",
            "Epoch: [57/100]\tSamples: [57000/100000]\tTrain Loss: 1677.28982421875\tTime: 0:00:00.217182\n",
            "Epoch: [58/100]\tSamples: [58000/100000]\tTrain Loss: 1676.6462265625\tTime: 0:00:00.216254\n",
            "Epoch: [59/100]\tSamples: [59000/100000]\tTrain Loss: 1679.2808515625\tTime: 0:00:00.223243\n",
            "Epoch: [60/100]\tSamples: [60000/100000]\tTrain Loss: 1676.77133984375\tTime: 0:00:00.202384\n",
            "Epoch: [61/100]\tSamples: [61000/100000]\tTrain Loss: 1677.069625\tTime: 0:00:00.222539\n",
            "Epoch: [62/100]\tSamples: [62000/100000]\tTrain Loss: 1677.5814375\tTime: 0:00:00.211768\n",
            "Epoch: [63/100]\tSamples: [63000/100000]\tTrain Loss: 1678.3904453125\tTime: 0:00:00.205018\n",
            "Epoch: [64/100]\tSamples: [64000/100000]\tTrain Loss: 1681.2185234375\tTime: 0:00:00.224414\n",
            "Epoch: [65/100]\tSamples: [65000/100000]\tTrain Loss: 1676.7621328125\tTime: 0:00:00.231477\n",
            "Epoch: [66/100]\tSamples: [66000/100000]\tTrain Loss: 1672.70846875\tTime: 0:00:00.226274\n",
            "Epoch: [67/100]\tSamples: [67000/100000]\tTrain Loss: 1673.2134609375\tTime: 0:00:00.216312\n",
            "Epoch: [68/100]\tSamples: [68000/100000]\tTrain Loss: 1678.1780078125\tTime: 0:00:00.229750\n",
            "Epoch: [69/100]\tSamples: [69000/100000]\tTrain Loss: 1671.5308203125\tTime: 0:00:00.224342\n",
            "Epoch: [70/100]\tSamples: [70000/100000]\tTrain Loss: 1676.5471328125\tTime: 0:00:00.212381\n",
            "Epoch: [71/100]\tSamples: [71000/100000]\tTrain Loss: 1676.15008203125\tTime: 0:00:00.221249\n",
            "Epoch: [72/100]\tSamples: [72000/100000]\tTrain Loss: 1676.8698671875\tTime: 0:00:00.209348\n",
            "Epoch: [73/100]\tSamples: [73000/100000]\tTrain Loss: 1679.4197734375\tTime: 0:00:00.230028\n",
            "Epoch: [74/100]\tSamples: [74000/100000]\tTrain Loss: 1676.38153125\tTime: 0:00:00.221738\n",
            "Epoch: [75/100]\tSamples: [75000/100000]\tTrain Loss: 1673.256109375\tTime: 0:00:00.233358\n",
            "Epoch: [76/100]\tSamples: [76000/100000]\tTrain Loss: 1675.8524140625\tTime: 0:00:00.209096\n",
            "Epoch: [77/100]\tSamples: [77000/100000]\tTrain Loss: 1675.452453125\tTime: 0:00:00.223011\n",
            "Epoch: [78/100]\tSamples: [78000/100000]\tTrain Loss: 1675.0344140625\tTime: 0:00:00.209667\n",
            "Epoch: [79/100]\tSamples: [79000/100000]\tTrain Loss: 1674.33328125\tTime: 0:00:00.211948\n",
            "Epoch: [80/100]\tSamples: [80000/100000]\tTrain Loss: 1672.36697265625\tTime: 0:00:00.216963\n",
            "Epoch: [81/100]\tSamples: [81000/100000]\tTrain Loss: 1671.685234375\tTime: 0:00:00.213008\n",
            "Epoch: [82/100]\tSamples: [82000/100000]\tTrain Loss: 1671.65979296875\tTime: 0:00:00.228351\n",
            "Epoch: [83/100]\tSamples: [83000/100000]\tTrain Loss: 1671.0448828125\tTime: 0:00:00.212811\n",
            "Epoch: [84/100]\tSamples: [84000/100000]\tTrain Loss: 1676.3298203125\tTime: 0:00:00.212356\n",
            "Epoch: [85/100]\tSamples: [85000/100000]\tTrain Loss: 1669.95375\tTime: 0:00:00.213392\n",
            "Epoch: [86/100]\tSamples: [86000/100000]\tTrain Loss: 1675.381109375\tTime: 0:00:00.205282\n",
            "Epoch: [87/100]\tSamples: [87000/100000]\tTrain Loss: 1675.8228359375\tTime: 0:00:00.215591\n",
            "Epoch: [88/100]\tSamples: [88000/100000]\tTrain Loss: 1670.69305859375\tTime: 0:00:00.214220\n",
            "Epoch: [89/100]\tSamples: [89000/100000]\tTrain Loss: 1675.9406171875\tTime: 0:00:00.207622\n",
            "Epoch: [90/100]\tSamples: [90000/100000]\tTrain Loss: 1668.51634375\tTime: 0:00:00.232387\n",
            "Epoch: [91/100]\tSamples: [91000/100000]\tTrain Loss: 1673.74362890625\tTime: 0:00:00.222485\n",
            "Epoch: [92/100]\tSamples: [92000/100000]\tTrain Loss: 1677.3237734375\tTime: 0:00:00.201711\n",
            "Epoch: [93/100]\tSamples: [93000/100000]\tTrain Loss: 1670.3741171875\tTime: 0:00:00.212444\n",
            "Epoch: [94/100]\tSamples: [94000/100000]\tTrain Loss: 1676.7968203125\tTime: 0:00:00.220545\n",
            "Epoch: [95/100]\tSamples: [95000/100000]\tTrain Loss: 1675.184609375\tTime: 0:00:00.216052\n",
            "Epoch: [96/100]\tSamples: [96000/100000]\tTrain Loss: 1674.1899453125\tTime: 0:00:00.215191\n",
            "Epoch: [97/100]\tSamples: [97000/100000]\tTrain Loss: 1669.5600546875\tTime: 0:00:00.206940\n",
            "Epoch: [98/100]\tSamples: [98000/100000]\tTrain Loss: 1674.26553125\tTime: 0:00:00.219061\n",
            "Epoch: [99/100]\tSamples: [99000/100000]\tTrain Loss: 1674.5862109375\tTime: 0:00:00.213397\n",
            "Epoch: [100/100]\tSamples: [100000/100000]\tTrain Loss: 1673.5351796875\tTime: 0:00:00.216774\n"
          ]
        }
      ],
      "source": [
        "train_datasets = []\n",
        "avitms = []\n",
        "id2tokens = []\n",
        "for corpus_node in documents_all:\n",
        "  cv = CountVectorizer(input='content', lowercase=True, stop_words='english',\n",
        "                      max_df=0.99, min_df=0.01, binary=False)\n",
        "  \n",
        "  docs = [\" \".join(corpus_node[i]) for i in np.arange(len(corpus_node))]\n",
        "\n",
        "  train_bow = cv.fit_transform(docs)\n",
        "  train_bow = train_bow.toarray()\n",
        "\n",
        "  idx2token = cv.get_feature_names()\n",
        "  input_size = len(idx2token)\n",
        "\n",
        "  id2token = {k: v for k, v in zip(range(0, len(idx2token)), idx2token)}\n",
        "  id2tokens.append(id2token)\n",
        "\n",
        "  train_data = BOWDataset(train_bow, idx2token)\n",
        "\n",
        "  avitm = AVITM(input_size=input_size, n_components=10, model_type='prodLDA',\n",
        "                hidden_sizes=(100, 100), activation='softplus', dropout=0.2,\n",
        "                learn_priors=True, batch_size=64, lr=2e-3, momentum=0.99,\n",
        "                solver='adam', num_epochs=100, reduce_on_plateau=False)\n",
        "\n",
        "  avitm.fit(train_data)\n",
        "  avitms.append(avitm)\n",
        "\n",
        "  train_datasets.append(train_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CD0GfefPvJD"
      },
      "source": [
        "# 2.1 Topics at each node"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-X9g0dNGk0bC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "8c2c6857-6d6f-426e-b974-b2d0ad469ce4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-4c1a4830-bd41-473a-b805-2df1ef64468a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>wd695</td>\n",
              "      <td>wd367</td>\n",
              "      <td>wd899</td>\n",
              "      <td>wd134</td>\n",
              "      <td>wd203</td>\n",
              "      <td>wd743</td>\n",
              "      <td>wd808</td>\n",
              "      <td>wd299</td>\n",
              "      <td>wd713</td>\n",
              "      <td>wd585</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>wd927</td>\n",
              "      <td>wd223</td>\n",
              "      <td>wd268</td>\n",
              "      <td>wd583</td>\n",
              "      <td>wd297</td>\n",
              "      <td>wd875</td>\n",
              "      <td>wd916</td>\n",
              "      <td>wd618</td>\n",
              "      <td>wd64</td>\n",
              "      <td>wd811</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>wd468</td>\n",
              "      <td>wd246</td>\n",
              "      <td>wd31</td>\n",
              "      <td>wd434</td>\n",
              "      <td>wd466</td>\n",
              "      <td>wd687</td>\n",
              "      <td>wd321</td>\n",
              "      <td>wd332</td>\n",
              "      <td>wd494</td>\n",
              "      <td>wd591</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>wd192</td>\n",
              "      <td>wd9</td>\n",
              "      <td>wd868</td>\n",
              "      <td>wd505</td>\n",
              "      <td>wd750</td>\n",
              "      <td>wd932</td>\n",
              "      <td>wd178</td>\n",
              "      <td>wd261</td>\n",
              "      <td>wd869</td>\n",
              "      <td>wd442</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>wd686</td>\n",
              "      <td>wd954</td>\n",
              "      <td>wd361</td>\n",
              "      <td>wd197</td>\n",
              "      <td>wd477</td>\n",
              "      <td>wd363</td>\n",
              "      <td>wd598</td>\n",
              "      <td>wd781</td>\n",
              "      <td>wd632</td>\n",
              "      <td>wd961</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>wd695</td>\n",
              "      <td>wd899</td>\n",
              "      <td>wd367</td>\n",
              "      <td>wd134</td>\n",
              "      <td>wd808</td>\n",
              "      <td>wd203</td>\n",
              "      <td>wd743</td>\n",
              "      <td>wd299</td>\n",
              "      <td>wd675</td>\n",
              "      <td>wd713</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>wd686</td>\n",
              "      <td>wd954</td>\n",
              "      <td>wd361</td>\n",
              "      <td>wd197</td>\n",
              "      <td>wd363</td>\n",
              "      <td>wd781</td>\n",
              "      <td>wd678</td>\n",
              "      <td>wd358</td>\n",
              "      <td>wd598</td>\n",
              "      <td>wd219</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>wd936</td>\n",
              "      <td>wd543</td>\n",
              "      <td>wd174</td>\n",
              "      <td>wd673</td>\n",
              "      <td>wd877</td>\n",
              "      <td>wd779</td>\n",
              "      <td>wd464</td>\n",
              "      <td>wd554</td>\n",
              "      <td>wd133</td>\n",
              "      <td>wd216</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>wd323</td>\n",
              "      <td>wd313</td>\n",
              "      <td>wd346</td>\n",
              "      <td>wd385</td>\n",
              "      <td>wd183</td>\n",
              "      <td>wd600</td>\n",
              "      <td>wd355</td>\n",
              "      <td>wd44</td>\n",
              "      <td>wd465</td>\n",
              "      <td>wd193</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>wd540</td>\n",
              "      <td>wd692</td>\n",
              "      <td>wd250</td>\n",
              "      <td>wd121</td>\n",
              "      <td>wd327</td>\n",
              "      <td>wd50</td>\n",
              "      <td>wd711</td>\n",
              "      <td>wd286</td>\n",
              "      <td>wd949</td>\n",
              "      <td>wd560</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4c1a4830-bd41-473a-b805-2df1ef64468a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-4c1a4830-bd41-473a-b805-2df1ef64468a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-4c1a4830-bd41-473a-b805-2df1ef64468a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "       0      1      2      3      4      5      6      7      8      9\n",
              "0  wd695  wd367  wd899  wd134  wd203  wd743  wd808  wd299  wd713  wd585\n",
              "1  wd927  wd223  wd268  wd583  wd297  wd875  wd916  wd618   wd64  wd811\n",
              "2  wd468  wd246   wd31  wd434  wd466  wd687  wd321  wd332  wd494  wd591\n",
              "3  wd192    wd9  wd868  wd505  wd750  wd932  wd178  wd261  wd869  wd442\n",
              "4  wd686  wd954  wd361  wd197  wd477  wd363  wd598  wd781  wd632  wd961\n",
              "5  wd695  wd899  wd367  wd134  wd808  wd203  wd743  wd299  wd675  wd713\n",
              "6  wd686  wd954  wd361  wd197  wd363  wd781  wd678  wd358  wd598  wd219\n",
              "7  wd936  wd543  wd174  wd673  wd877  wd779  wd464  wd554  wd133  wd216\n",
              "8  wd323  wd313  wd346  wd385  wd183  wd600  wd355   wd44  wd465  wd193\n",
              "9  wd540  wd692  wd250  wd121  wd327   wd50  wd711  wd286  wd949  wd560"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ],
      "source": [
        "topics_all = []\n",
        "for node in np.arange(n_nodes):\n",
        "  topics = pd.DataFrame(avitm.get_topics(10)).T\n",
        "  topics_all.append(topics)\n",
        "topics_all[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4kezlbmRaRV"
      },
      "source": [
        "## 2.2 Document-topic distributions at each node"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "orpK--hqxQkG"
      },
      "outputs": [],
      "source": [
        "def get_doc_topic_distribution(avitm, dataset, n_samples=20):\n",
        "    avitm.model.eval()\n",
        "\n",
        "    loader = DataLoader(\n",
        "            avitm.train_data, batch_size=avitm.batch_size, shuffle=True,\n",
        "            num_workers=mp.cpu_count())\n",
        "\n",
        "    pbar = tqdm(n_samples, position=0, leave=True)\n",
        "\n",
        "    final_thetas = []\n",
        "    for sample_index in range(n_samples):\n",
        "        with torch.no_grad():\n",
        "            collect_theta = []\n",
        "\n",
        "            for batch_samples in loader:\n",
        "                X = batch_samples['X']\n",
        "\n",
        "                if avitm.USE_CUDA:\n",
        "                  X = X.cuda()\n",
        "\n",
        "                # forward pass\n",
        "                avitm.model.zero_grad()\n",
        "                \n",
        "                with torch.no_grad():\n",
        "                  posterior_mu, posterior_log_sigma = avitm.model.inf_net(X)\n",
        "\n",
        "                  # Generate samples from theta\n",
        "                  theta = F.softmax(\n",
        "                          avitm.model.reparameterize(posterior_mu, posterior_log_sigma), dim=1)\n",
        "                  theta = avitm.model.drop_theta(theta)\n",
        "\n",
        "                collect_theta.extend(theta.cpu().numpy().tolist())\n",
        "\n",
        "            pbar.update(1)\n",
        "            pbar.set_description(\"Sampling: [{}/{}]\".format(sample_index + 1, n_samples))\n",
        "\n",
        "            final_thetas.append(np.array(collect_theta))\n",
        "    pbar.close()\n",
        "    return np.sum(final_thetas, axis=0) / n_samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PXTjce2LlVRG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56f69ebe-bb35-47d4-e1ea-dca109fa9098"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Sampling: [5/5]: : 5it [00:00,  5.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document-topic distribution node 0 \n",
            "(1000, 10)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Sampling: [5/5]: : 5it [00:00,  5.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document-topic distribution node 1 \n",
            "(1000, 10)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Sampling: [5/5]: : 5it [00:00,  5.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document-topic distribution node 2 \n",
            "(1000, 10)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Sampling: [5/5]: : 5it [00:00,  5.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document-topic distribution node 3 \n",
            "(1000, 10)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Sampling: [5/5]: : 5it [00:00,  5.67it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document-topic distribution node 4 \n",
            "(1000, 10)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "doc_topic_all = []\n",
        "for node in np.arange(n_nodes):\n",
        "  doc_topic = get_doc_topic_distribution(avitms[node], train_datasets[node], n_samples=5) # get all the topic predictions\n",
        "  print(\"Document-topic distribution node\", str(node), \"\")\n",
        "  doc_topic_all.append(doc_topic)\n",
        "  print(np.array(doc_topics).shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aEE_kyedRgYv"
      },
      "source": [
        "## 2.3 Word-topic distributions attained at each node"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "douV1llyTM4b"
      },
      "outputs": [],
      "source": [
        "def get_topic_word_distribution(avtim_model):\n",
        "  topic_word_matrix = avtim_model.model.beta.cpu().detach().numpy()\n",
        "  return softmax(topic_word_matrix, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BdM2jxRJUvk3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45af85eb-1b75-4039-e691-faa3b154863d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['wd1', 'wd2', 'wd3', 'wd4', 'wd5', 'wd6', 'wd7', 'wd8', 'wd9', 'wd10', 'wd11', 'wd12', 'wd13', 'wd14', 'wd15', 'wd16', 'wd17', 'wd18', 'wd19', 'wd20', 'wd21', 'wd22', 'wd23', 'wd24', 'wd25', 'wd26', 'wd27', 'wd28', 'wd29', 'wd30', 'wd31', 'wd32', 'wd33', 'wd34', 'wd35', 'wd36', 'wd37', 'wd38', 'wd39', 'wd40', 'wd41', 'wd42', 'wd43', 'wd44', 'wd45', 'wd46', 'wd47', 'wd48', 'wd49', 'wd50', 'wd51', 'wd52', 'wd53', 'wd54', 'wd55', 'wd56', 'wd57', 'wd58', 'wd59', 'wd60', 'wd61', 'wd62', 'wd63', 'wd64', 'wd65', 'wd66', 'wd67', 'wd68', 'wd69', 'wd70', 'wd71', 'wd72', 'wd73', 'wd74', 'wd75', 'wd76', 'wd77', 'wd78', 'wd79', 'wd80', 'wd81', 'wd82', 'wd83', 'wd84', 'wd85', 'wd86', 'wd87', 'wd88', 'wd89', 'wd90', 'wd91', 'wd92', 'wd93', 'wd94', 'wd95', 'wd96', 'wd97', 'wd98', 'wd99', 'wd100', 'wd101', 'wd102', 'wd103', 'wd104', 'wd105', 'wd106', 'wd107', 'wd108', 'wd109', 'wd110', 'wd111', 'wd112', 'wd113', 'wd114', 'wd115', 'wd116', 'wd117', 'wd118', 'wd119', 'wd120', 'wd121', 'wd122', 'wd123', 'wd124', 'wd125', 'wd126', 'wd127', 'wd128', 'wd129', 'wd130', 'wd131', 'wd132', 'wd133', 'wd134', 'wd135', 'wd136', 'wd137', 'wd138', 'wd139', 'wd140', 'wd141', 'wd142', 'wd143', 'wd144', 'wd145', 'wd146', 'wd147', 'wd148', 'wd149', 'wd150', 'wd151', 'wd152', 'wd153', 'wd154', 'wd155', 'wd156', 'wd157', 'wd158', 'wd159', 'wd160', 'wd161', 'wd162', 'wd163', 'wd164', 'wd165', 'wd166', 'wd167', 'wd168', 'wd169', 'wd170', 'wd171', 'wd172', 'wd173', 'wd174', 'wd175', 'wd176', 'wd177', 'wd178', 'wd179', 'wd180', 'wd181', 'wd182', 'wd183', 'wd184', 'wd185', 'wd186', 'wd187', 'wd188', 'wd189', 'wd190', 'wd191', 'wd192', 'wd193', 'wd194', 'wd195', 'wd196', 'wd197', 'wd198', 'wd199', 'wd200', 'wd201', 'wd202', 'wd203', 'wd204', 'wd205', 'wd206', 'wd207', 'wd208', 'wd209', 'wd210', 'wd211', 'wd212', 'wd213', 'wd214', 'wd215', 'wd216', 'wd217', 'wd218', 'wd219', 'wd220', 'wd221', 'wd222', 'wd223', 'wd224', 'wd225', 'wd226', 'wd227', 'wd228', 'wd229', 'wd230', 'wd231', 'wd232', 'wd233', 'wd234', 'wd235', 'wd236', 'wd237', 'wd238', 'wd239', 'wd240', 'wd241', 'wd242', 'wd243', 'wd244', 'wd245', 'wd246', 'wd247', 'wd248', 'wd249', 'wd250', 'wd251', 'wd252', 'wd253', 'wd254', 'wd255', 'wd256', 'wd257', 'wd258', 'wd259', 'wd260', 'wd261', 'wd262', 'wd263', 'wd264', 'wd265', 'wd266', 'wd267', 'wd268', 'wd269', 'wd270', 'wd271', 'wd272', 'wd273', 'wd274', 'wd275', 'wd276', 'wd277', 'wd278', 'wd279', 'wd280', 'wd281', 'wd282', 'wd283', 'wd284', 'wd285', 'wd286', 'wd287', 'wd288', 'wd289', 'wd290', 'wd291', 'wd292', 'wd293', 'wd294', 'wd295', 'wd296', 'wd297', 'wd298', 'wd299', 'wd300', 'wd301', 'wd302', 'wd303', 'wd304', 'wd305', 'wd306', 'wd307', 'wd308', 'wd309', 'wd310', 'wd311', 'wd312', 'wd313', 'wd314', 'wd315', 'wd316', 'wd317', 'wd318', 'wd319', 'wd320', 'wd321', 'wd322', 'wd323', 'wd324', 'wd325', 'wd326', 'wd327', 'wd328', 'wd329', 'wd330', 'wd331', 'wd332', 'wd333', 'wd334', 'wd335', 'wd336', 'wd337', 'wd338', 'wd339', 'wd340', 'wd341', 'wd342', 'wd343', 'wd344', 'wd345', 'wd346', 'wd347', 'wd348', 'wd349', 'wd350', 'wd351', 'wd352', 'wd353', 'wd354', 'wd355', 'wd356', 'wd357', 'wd358', 'wd359', 'wd360', 'wd361', 'wd362', 'wd363', 'wd364', 'wd365', 'wd366', 'wd367', 'wd368', 'wd369', 'wd370', 'wd371', 'wd372', 'wd373', 'wd374', 'wd375', 'wd376', 'wd377', 'wd378', 'wd379', 'wd380', 'wd381', 'wd382', 'wd383', 'wd384', 'wd385', 'wd386', 'wd387', 'wd388', 'wd389', 'wd390', 'wd391', 'wd392', 'wd393', 'wd394', 'wd395', 'wd396', 'wd397', 'wd398', 'wd399', 'wd400', 'wd401', 'wd402', 'wd403', 'wd404', 'wd405', 'wd406', 'wd407', 'wd408', 'wd409', 'wd410', 'wd411', 'wd412', 'wd413', 'wd414', 'wd415', 'wd416', 'wd417', 'wd418', 'wd419', 'wd420', 'wd421', 'wd422', 'wd423', 'wd424', 'wd425', 'wd426', 'wd427', 'wd428', 'wd429', 'wd430', 'wd431', 'wd432', 'wd433', 'wd434', 'wd435', 'wd436', 'wd437', 'wd438', 'wd439', 'wd440', 'wd441', 'wd442', 'wd443', 'wd444', 'wd445', 'wd446', 'wd447', 'wd448', 'wd449', 'wd450', 'wd451', 'wd452', 'wd453', 'wd454', 'wd455', 'wd456', 'wd457', 'wd458', 'wd459', 'wd460', 'wd461', 'wd462', 'wd463', 'wd464', 'wd465', 'wd466', 'wd467', 'wd468', 'wd469', 'wd470', 'wd471', 'wd472', 'wd473', 'wd474', 'wd475', 'wd476', 'wd477', 'wd478', 'wd479', 'wd480', 'wd481', 'wd482', 'wd483', 'wd484', 'wd485', 'wd486', 'wd487', 'wd488', 'wd489', 'wd490', 'wd491', 'wd492', 'wd493', 'wd494', 'wd495', 'wd496', 'wd497', 'wd498', 'wd499', 'wd500', 'wd501', 'wd502', 'wd503', 'wd504', 'wd505', 'wd506', 'wd507', 'wd508', 'wd509', 'wd510', 'wd511', 'wd512', 'wd513', 'wd514', 'wd515', 'wd516', 'wd517', 'wd518', 'wd519', 'wd520', 'wd521', 'wd522', 'wd523', 'wd524', 'wd525', 'wd526', 'wd527', 'wd528', 'wd529', 'wd530', 'wd531', 'wd532', 'wd533', 'wd534', 'wd535', 'wd536', 'wd537', 'wd538', 'wd539', 'wd540', 'wd541', 'wd542', 'wd543', 'wd544', 'wd545', 'wd546', 'wd547', 'wd548', 'wd549', 'wd550', 'wd551', 'wd552', 'wd553', 'wd554', 'wd555', 'wd556', 'wd557', 'wd558', 'wd559', 'wd560', 'wd561', 'wd562', 'wd563', 'wd564', 'wd565', 'wd566', 'wd567', 'wd568', 'wd569', 'wd570', 'wd571', 'wd572', 'wd573', 'wd574', 'wd575', 'wd576', 'wd577', 'wd578', 'wd579', 'wd580', 'wd581', 'wd582', 'wd583', 'wd584', 'wd585', 'wd586', 'wd587', 'wd588', 'wd589', 'wd590', 'wd591', 'wd592', 'wd593', 'wd594', 'wd595', 'wd596', 'wd597', 'wd598', 'wd599', 'wd600', 'wd601', 'wd602', 'wd603', 'wd604', 'wd605', 'wd606', 'wd607', 'wd608', 'wd609', 'wd610', 'wd611', 'wd612', 'wd613', 'wd614', 'wd615', 'wd616', 'wd617', 'wd618', 'wd619', 'wd620', 'wd621', 'wd622', 'wd623', 'wd624', 'wd625', 'wd626', 'wd627', 'wd628', 'wd629', 'wd630', 'wd631', 'wd632', 'wd633', 'wd634', 'wd635', 'wd636', 'wd637', 'wd638', 'wd639', 'wd640', 'wd641', 'wd642', 'wd643', 'wd644', 'wd645', 'wd646', 'wd647', 'wd648', 'wd649', 'wd650', 'wd651', 'wd652', 'wd653', 'wd654', 'wd655', 'wd656', 'wd657', 'wd658', 'wd659', 'wd660', 'wd661', 'wd662', 'wd663', 'wd664', 'wd665', 'wd666', 'wd667', 'wd668', 'wd669', 'wd670', 'wd671', 'wd672', 'wd673', 'wd674', 'wd675', 'wd676', 'wd677', 'wd678', 'wd679', 'wd680', 'wd681', 'wd682', 'wd683', 'wd684', 'wd685', 'wd686', 'wd687', 'wd688', 'wd689', 'wd690', 'wd691', 'wd692', 'wd693', 'wd694', 'wd695', 'wd696', 'wd697', 'wd698', 'wd699', 'wd700', 'wd701', 'wd702', 'wd703', 'wd704', 'wd705', 'wd706', 'wd707', 'wd708', 'wd709', 'wd710', 'wd711', 'wd712', 'wd713', 'wd714', 'wd715', 'wd716', 'wd717', 'wd718', 'wd719', 'wd720', 'wd721', 'wd722', 'wd723', 'wd724', 'wd725', 'wd726', 'wd727', 'wd728', 'wd729', 'wd730', 'wd731', 'wd732', 'wd733', 'wd734', 'wd735', 'wd736', 'wd737', 'wd738', 'wd739', 'wd740', 'wd741', 'wd742', 'wd743', 'wd744', 'wd745', 'wd746', 'wd747', 'wd748', 'wd749', 'wd750', 'wd751', 'wd752', 'wd753', 'wd754', 'wd755', 'wd756', 'wd757', 'wd758', 'wd759', 'wd760', 'wd761', 'wd762', 'wd763', 'wd764', 'wd765', 'wd766', 'wd767', 'wd768', 'wd769', 'wd770', 'wd771', 'wd772', 'wd773', 'wd774', 'wd775', 'wd776', 'wd777', 'wd778', 'wd779', 'wd780', 'wd781', 'wd782', 'wd783', 'wd784', 'wd785', 'wd786', 'wd787', 'wd788', 'wd789', 'wd790', 'wd791', 'wd792', 'wd793', 'wd794', 'wd795', 'wd796', 'wd797', 'wd798', 'wd799', 'wd800', 'wd801', 'wd802', 'wd803', 'wd804', 'wd805', 'wd806', 'wd807', 'wd808', 'wd809', 'wd810', 'wd811', 'wd812', 'wd813', 'wd814', 'wd815', 'wd816', 'wd817', 'wd818', 'wd819', 'wd820', 'wd821', 'wd822', 'wd823', 'wd824', 'wd825', 'wd826', 'wd827', 'wd828', 'wd829', 'wd830', 'wd831', 'wd832', 'wd833', 'wd834', 'wd835', 'wd836', 'wd837', 'wd838', 'wd839', 'wd840', 'wd841', 'wd842', 'wd843', 'wd844', 'wd845', 'wd846', 'wd847', 'wd848', 'wd849', 'wd850', 'wd851', 'wd852', 'wd853', 'wd854', 'wd855', 'wd856', 'wd857', 'wd858', 'wd859', 'wd860', 'wd861', 'wd862', 'wd863', 'wd864', 'wd865', 'wd866', 'wd867', 'wd868', 'wd869', 'wd870', 'wd871', 'wd872', 'wd873', 'wd874', 'wd875', 'wd876', 'wd877', 'wd878', 'wd879', 'wd880', 'wd881', 'wd882', 'wd883', 'wd884', 'wd885', 'wd886', 'wd887', 'wd888', 'wd889', 'wd890', 'wd891', 'wd892', 'wd893', 'wd894', 'wd895', 'wd896', 'wd897', 'wd898', 'wd899', 'wd900', 'wd901', 'wd902', 'wd903', 'wd904', 'wd905', 'wd906', 'wd907', 'wd908', 'wd909', 'wd910', 'wd911', 'wd912', 'wd913', 'wd914', 'wd915', 'wd916', 'wd917', 'wd918', 'wd919', 'wd920', 'wd921', 'wd922', 'wd923', 'wd924', 'wd925', 'wd926', 'wd927', 'wd928', 'wd929', 'wd930', 'wd931', 'wd932', 'wd933', 'wd934', 'wd935', 'wd936', 'wd937', 'wd938', 'wd939', 'wd940', 'wd941', 'wd942', 'wd943', 'wd944', 'wd945', 'wd946', 'wd947', 'wd948', 'wd949', 'wd950', 'wd951', 'wd952', 'wd953', 'wd954', 'wd955', 'wd956', 'wd957', 'wd958', 'wd959', 'wd960', 'wd961', 'wd962', 'wd963', 'wd964', 'wd965', 'wd966', 'wd967', 'wd968', 'wd969', 'wd970', 'wd971', 'wd972', 'wd973', 'wd974', 'wd975', 'wd976', 'wd977', 'wd978', 'wd979', 'wd980', 'wd981', 'wd982', 'wd983', 'wd984', 'wd985', 'wd986', 'wd987', 'wd988', 'wd989', 'wd990', 'wd991', 'wd992', 'wd993', 'wd994', 'wd995', 'wd996', 'wd997', 'wd998', 'wd999', 'wd1000']\n"
          ]
        }
      ],
      "source": [
        "all_words = []\n",
        "for word in np.arange(vocab_size+1):\n",
        "  if word > 0:\n",
        "    all_words.append('wd'+str(word))\n",
        "print(all_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MJfQe7PdUmkC"
      },
      "outputs": [],
      "source": [
        "topic_word_all = []\n",
        "for node in np.arange(n_nodes):\n",
        "  w_t_distrib = np.zeros((10,vocab_size), dtype=np.float64) \n",
        "  wd = get_topic_word_distribution(avitms[node])\n",
        "  for i in np.arange(10):\n",
        "    for idx, word in id2tokens[node].items():\n",
        "      for j in np.arange(len(all_words)):\n",
        "        if all_words[j] == word:\n",
        "          w_t_distrib[i,j] = wd[i][idx]\n",
        "          break\n",
        "  sum_of_rows = w_t_distrib.sum(axis=1)\n",
        "  normalized_array = w_t_distrib / sum_of_rows[:, np.newaxis]\n",
        "  topic_word_all.append(normalized_array)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QieP5DdU7zY0"
      },
      "source": [
        "# 3. Centralized model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wTQrxfRpRrD1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a2b5782-bfd0-41ce-b727-4471f798457a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5000"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ],
      "source": [
        "documents_centr = [*documents_all[0], *documents_all[1], *documents_all[2], *documents_all[3], *documents_all[4]]\n",
        "len(documents_centr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kxW_tMtFRyfX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f224cd45-e491-4978-98d6-4ee4f0a664cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Settings: \n",
            "               N Components: 10\n",
            "               Topic Prior Mean: 0.0\n",
            "               Topic Prior Variance: 0.9\n",
            "               Model Type: prodLDA\n",
            "               Hidden Sizes: (100, 100)\n",
            "               Activation: softplus\n",
            "               Dropout: 0.2\n",
            "               Learn Priors: True\n",
            "               Learning Rate: 0.002\n",
            "               Momentum: 0.99\n",
            "               Reduce On Plateau: False\n",
            "               Save Dir: None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [1/100]\tSamples: [5000/500000]\tTrain Loss: 1845.97925390625\tTime: 0:00:00.592948\n",
            "Epoch: [2/100]\tSamples: [10000/500000]\tTrain Loss: 1770.353270703125\tTime: 0:00:00.566519\n",
            "Epoch: [3/100]\tSamples: [15000/500000]\tTrain Loss: 1747.76899140625\tTime: 0:00:00.593393\n",
            "Epoch: [4/100]\tSamples: [20000/500000]\tTrain Loss: 1733.783206640625\tTime: 0:00:00.551633\n",
            "Epoch: [5/100]\tSamples: [25000/500000]\tTrain Loss: 1725.75482890625\tTime: 0:00:00.539926\n",
            "Epoch: [6/100]\tSamples: [30000/500000]\tTrain Loss: 1718.508185546875\tTime: 0:00:00.555262\n",
            "Epoch: [7/100]\tSamples: [35000/500000]\tTrain Loss: 1710.2378388671875\tTime: 0:00:00.574936\n",
            "Epoch: [8/100]\tSamples: [40000/500000]\tTrain Loss: 1704.9462158203125\tTime: 0:00:00.539135\n",
            "Epoch: [9/100]\tSamples: [45000/500000]\tTrain Loss: 1695.9770697265626\tTime: 0:00:00.554984\n",
            "Epoch: [10/100]\tSamples: [50000/500000]\tTrain Loss: 1696.4189505859374\tTime: 0:00:00.599552\n",
            "Epoch: [11/100]\tSamples: [55000/500000]\tTrain Loss: 1694.6548703125\tTime: 0:00:00.558937\n",
            "Epoch: [12/100]\tSamples: [60000/500000]\tTrain Loss: 1690.9347056640624\tTime: 0:00:00.576211\n",
            "Epoch: [13/100]\tSamples: [65000/500000]\tTrain Loss: 1691.44458671875\tTime: 0:00:00.555155\n",
            "Epoch: [14/100]\tSamples: [70000/500000]\tTrain Loss: 1688.474051171875\tTime: 0:00:00.572641\n",
            "Epoch: [15/100]\tSamples: [75000/500000]\tTrain Loss: 1688.980963671875\tTime: 0:00:00.566965\n",
            "Epoch: [16/100]\tSamples: [80000/500000]\tTrain Loss: 1685.35317890625\tTime: 0:00:00.547086\n",
            "Epoch: [17/100]\tSamples: [85000/500000]\tTrain Loss: 1687.632676171875\tTime: 0:00:00.588719\n",
            "Epoch: [18/100]\tSamples: [90000/500000]\tTrain Loss: 1683.4978201171875\tTime: 0:00:00.543625\n",
            "Epoch: [19/100]\tSamples: [95000/500000]\tTrain Loss: 1685.6885029296875\tTime: 0:00:00.556521\n",
            "Epoch: [20/100]\tSamples: [100000/500000]\tTrain Loss: 1682.59333671875\tTime: 0:00:00.537396\n",
            "Epoch: [21/100]\tSamples: [105000/500000]\tTrain Loss: 1683.01011171875\tTime: 0:00:00.559327\n",
            "Epoch: [22/100]\tSamples: [110000/500000]\tTrain Loss: 1683.477953125\tTime: 0:00:00.550882\n",
            "Epoch: [23/100]\tSamples: [115000/500000]\tTrain Loss: 1681.7339669921876\tTime: 0:00:00.571244\n",
            "Epoch: [24/100]\tSamples: [120000/500000]\tTrain Loss: 1681.9979400390625\tTime: 0:00:00.603187\n",
            "Epoch: [25/100]\tSamples: [125000/500000]\tTrain Loss: 1681.4855072265625\tTime: 0:00:00.565470\n",
            "Epoch: [26/100]\tSamples: [130000/500000]\tTrain Loss: 1680.5582828125\tTime: 0:00:00.568714\n",
            "Epoch: [27/100]\tSamples: [135000/500000]\tTrain Loss: 1680.511450390625\tTime: 0:00:00.546653\n",
            "Epoch: [28/100]\tSamples: [140000/500000]\tTrain Loss: 1679.9125498046874\tTime: 0:00:00.594076\n",
            "Epoch: [29/100]\tSamples: [145000/500000]\tTrain Loss: 1679.452801171875\tTime: 0:00:00.550344\n",
            "Epoch: [30/100]\tSamples: [150000/500000]\tTrain Loss: 1679.179866796875\tTime: 0:00:00.552613\n",
            "Epoch: [31/100]\tSamples: [155000/500000]\tTrain Loss: 1676.6806833984374\tTime: 0:00:00.565146\n",
            "Epoch: [32/100]\tSamples: [160000/500000]\tTrain Loss: 1679.54996796875\tTime: 0:00:00.580904\n",
            "Epoch: [33/100]\tSamples: [165000/500000]\tTrain Loss: 1679.697533203125\tTime: 0:00:00.584285\n",
            "Epoch: [34/100]\tSamples: [170000/500000]\tTrain Loss: 1677.9036828125\tTime: 0:00:00.575919\n",
            "Epoch: [35/100]\tSamples: [175000/500000]\tTrain Loss: 1677.1831126953125\tTime: 0:00:00.600376\n",
            "Epoch: [36/100]\tSamples: [180000/500000]\tTrain Loss: 1676.7767357421876\tTime: 0:00:00.554789\n",
            "Epoch: [37/100]\tSamples: [185000/500000]\tTrain Loss: 1678.4638599609375\tTime: 0:00:00.565503\n",
            "Epoch: [38/100]\tSamples: [190000/500000]\tTrain Loss: 1676.2903546875\tTime: 0:00:00.559071\n",
            "Epoch: [39/100]\tSamples: [195000/500000]\tTrain Loss: 1677.04336328125\tTime: 0:00:00.559103\n",
            "Epoch: [40/100]\tSamples: [200000/500000]\tTrain Loss: 1676.3522818359374\tTime: 0:00:00.539786\n",
            "Epoch: [41/100]\tSamples: [205000/500000]\tTrain Loss: 1679.02845703125\tTime: 0:00:00.546844\n",
            "Epoch: [42/100]\tSamples: [210000/500000]\tTrain Loss: 1675.345462109375\tTime: 0:00:00.565204\n",
            "Epoch: [43/100]\tSamples: [215000/500000]\tTrain Loss: 1678.967051171875\tTime: 0:00:00.569598\n",
            "Epoch: [44/100]\tSamples: [220000/500000]\tTrain Loss: 1676.2829625\tTime: 0:00:00.574766\n",
            "Epoch: [45/100]\tSamples: [225000/500000]\tTrain Loss: 1677.40776171875\tTime: 0:00:00.558887\n",
            "Epoch: [46/100]\tSamples: [230000/500000]\tTrain Loss: 1676.9722673828126\tTime: 0:00:00.591535\n",
            "Epoch: [47/100]\tSamples: [235000/500000]\tTrain Loss: 1675.2609865234374\tTime: 0:00:00.578117\n",
            "Epoch: [48/100]\tSamples: [240000/500000]\tTrain Loss: 1676.89501640625\tTime: 0:00:00.568549\n",
            "Epoch: [49/100]\tSamples: [245000/500000]\tTrain Loss: 1675.4090203125\tTime: 0:00:00.593444\n",
            "Epoch: [50/100]\tSamples: [250000/500000]\tTrain Loss: 1677.9439259765625\tTime: 0:00:00.567351\n",
            "Epoch: [51/100]\tSamples: [255000/500000]\tTrain Loss: 1675.56070625\tTime: 0:00:00.578833\n",
            "Epoch: [52/100]\tSamples: [260000/500000]\tTrain Loss: 1675.9817162109375\tTime: 0:00:00.615068\n",
            "Epoch: [53/100]\tSamples: [265000/500000]\tTrain Loss: 1678.01441640625\tTime: 0:00:00.567910\n",
            "Epoch: [54/100]\tSamples: [270000/500000]\tTrain Loss: 1677.8905703125\tTime: 0:00:00.551918\n",
            "Epoch: [55/100]\tSamples: [275000/500000]\tTrain Loss: 1678.1335447265626\tTime: 0:00:00.563561\n",
            "Epoch: [56/100]\tSamples: [280000/500000]\tTrain Loss: 1674.9197658203125\tTime: 0:00:00.584075\n",
            "Epoch: [57/100]\tSamples: [285000/500000]\tTrain Loss: 1676.6442546875\tTime: 0:00:00.591421\n",
            "Epoch: [58/100]\tSamples: [290000/500000]\tTrain Loss: 1673.73154296875\tTime: 0:00:00.558649\n",
            "Epoch: [59/100]\tSamples: [295000/500000]\tTrain Loss: 1676.21776484375\tTime: 0:00:00.588927\n",
            "Epoch: [60/100]\tSamples: [300000/500000]\tTrain Loss: 1676.301710546875\tTime: 0:00:00.600572\n",
            "Epoch: [61/100]\tSamples: [305000/500000]\tTrain Loss: 1675.7424041015624\tTime: 0:00:00.562897\n",
            "Epoch: [62/100]\tSamples: [310000/500000]\tTrain Loss: 1674.764102734375\tTime: 0:00:00.572317\n",
            "Epoch: [63/100]\tSamples: [315000/500000]\tTrain Loss: 1674.76819921875\tTime: 0:00:00.571233\n",
            "Epoch: [64/100]\tSamples: [320000/500000]\tTrain Loss: 1674.937812890625\tTime: 0:00:00.580682\n",
            "Epoch: [65/100]\tSamples: [325000/500000]\tTrain Loss: 1674.39499453125\tTime: 0:00:00.547750\n",
            "Epoch: [66/100]\tSamples: [330000/500000]\tTrain Loss: 1674.4785009765626\tTime: 0:00:00.589932\n",
            "Epoch: [67/100]\tSamples: [335000/500000]\tTrain Loss: 1675.2331533203126\tTime: 0:00:00.564978\n",
            "Epoch: [68/100]\tSamples: [340000/500000]\tTrain Loss: 1675.2963890625\tTime: 0:00:00.575888\n",
            "Epoch: [69/100]\tSamples: [345000/500000]\tTrain Loss: 1674.2616974609375\tTime: 0:00:00.605581\n",
            "Epoch: [70/100]\tSamples: [350000/500000]\tTrain Loss: 1674.45901328125\tTime: 0:00:00.569941\n",
            "Epoch: [71/100]\tSamples: [355000/500000]\tTrain Loss: 1675.1048802734374\tTime: 0:00:00.577408\n",
            "Epoch: [72/100]\tSamples: [360000/500000]\tTrain Loss: 1672.944903515625\tTime: 0:00:00.562910\n",
            "Epoch: [73/100]\tSamples: [365000/500000]\tTrain Loss: 1674.0789390625\tTime: 0:00:00.592239\n",
            "Epoch: [74/100]\tSamples: [370000/500000]\tTrain Loss: 1671.923771484375\tTime: 0:00:00.581153\n",
            "Epoch: [75/100]\tSamples: [375000/500000]\tTrain Loss: 1674.02079921875\tTime: 0:00:00.579542\n",
            "Epoch: [76/100]\tSamples: [380000/500000]\tTrain Loss: 1673.931476171875\tTime: 0:00:00.585754\n",
            "Epoch: [77/100]\tSamples: [385000/500000]\tTrain Loss: 1674.5359359375\tTime: 0:00:00.592453\n",
            "Epoch: [78/100]\tSamples: [390000/500000]\tTrain Loss: 1674.715018359375\tTime: 0:00:00.617970\n",
            "Epoch: [79/100]\tSamples: [395000/500000]\tTrain Loss: 1676.4594576171876\tTime: 0:00:00.565838\n",
            "Epoch: [80/100]\tSamples: [400000/500000]\tTrain Loss: 1674.3275515625\tTime: 0:00:00.611462\n",
            "Epoch: [81/100]\tSamples: [405000/500000]\tTrain Loss: 1674.029350390625\tTime: 0:00:00.587056\n",
            "Epoch: [82/100]\tSamples: [410000/500000]\tTrain Loss: 1675.2098759765624\tTime: 0:00:00.588338\n",
            "Epoch: [83/100]\tSamples: [415000/500000]\tTrain Loss: 1673.3116982421875\tTime: 0:00:00.609147\n",
            "Epoch: [84/100]\tSamples: [420000/500000]\tTrain Loss: 1672.5206978515625\tTime: 0:00:00.587242\n",
            "Epoch: [85/100]\tSamples: [425000/500000]\tTrain Loss: 1674.134351171875\tTime: 0:00:00.598726\n",
            "Epoch: [86/100]\tSamples: [430000/500000]\tTrain Loss: 1673.1940986328125\tTime: 0:00:00.578557\n",
            "Epoch: [87/100]\tSamples: [435000/500000]\tTrain Loss: 1672.41179140625\tTime: 0:00:00.609716\n",
            "Epoch: [88/100]\tSamples: [440000/500000]\tTrain Loss: 1671.24420859375\tTime: 0:00:00.581351\n",
            "Epoch: [89/100]\tSamples: [445000/500000]\tTrain Loss: 1673.4583734375\tTime: 0:00:00.590489\n",
            "Epoch: [90/100]\tSamples: [450000/500000]\tTrain Loss: 1674.219976953125\tTime: 0:00:00.608795\n",
            "Epoch: [91/100]\tSamples: [455000/500000]\tTrain Loss: 1671.720876171875\tTime: 0:00:00.578288\n",
            "Epoch: [92/100]\tSamples: [460000/500000]\tTrain Loss: 1675.101621484375\tTime: 0:00:00.573613\n",
            "Epoch: [93/100]\tSamples: [465000/500000]\tTrain Loss: 1672.7901330078125\tTime: 0:00:00.587989\n",
            "Epoch: [94/100]\tSamples: [470000/500000]\tTrain Loss: 1671.69605078125\tTime: 0:00:00.617487\n",
            "Epoch: [95/100]\tSamples: [475000/500000]\tTrain Loss: 1673.0781087890625\tTime: 0:00:00.575539\n",
            "Epoch: [96/100]\tSamples: [480000/500000]\tTrain Loss: 1673.906642578125\tTime: 0:00:00.593765\n",
            "Epoch: [97/100]\tSamples: [485000/500000]\tTrain Loss: 1675.6434861328125\tTime: 0:00:00.594337\n",
            "Epoch: [98/100]\tSamples: [490000/500000]\tTrain Loss: 1674.22572421875\tTime: 0:00:00.587982\n",
            "Epoch: [99/100]\tSamples: [495000/500000]\tTrain Loss: 1671.252978125\tTime: 0:00:00.579469\n",
            "Epoch: [100/100]\tSamples: [500000/500000]\tTrain Loss: 1673.464337109375\tTime: 0:00:00.598349\n"
          ]
        }
      ],
      "source": [
        "cv = CountVectorizer(input='content', lowercase=True, stop_words='english',\n",
        "                      max_df=0.99, min_df=0.01, binary=False)\n",
        "  \n",
        "docs_centr = [\" \".join(documents_centr[i]) for i in np.arange(len(documents_centr))]\n",
        "\n",
        "train_bow_centr = cv.fit_transform(docs_centr)\n",
        "train_bow_centr = train_bow_centr.toarray()\n",
        "\n",
        "idx2token_centr = cv.get_feature_names()\n",
        "input_size_centr = len(idx2token_centr)\n",
        "\n",
        "id2token_centr = {k: v for k, v in zip(range(0, len(idx2token_centr)), idx2token_centr)}\n",
        "\n",
        "train_data_centr = BOWDataset(train_bow_centr, idx2token_centr)\n",
        "\n",
        "avitm_centr = AVITM(input_size=input_size_centr, n_components=10, model_type='prodLDA',\n",
        "              hidden_sizes=(100, 100), activation='softplus', dropout=0.2,\n",
        "              learn_priors=True, batch_size=64, lr=2e-3, momentum=0.99,\n",
        "              solver='adam', num_epochs=100, reduce_on_plateau=False)\n",
        "\n",
        "avitm_centr.fit(train_data_centr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A3_xM2LUSYAN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "7b51d347-d2fd-40c5-a2de-699c49e49a39"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-3f4b2b53-a049-46cd-b2b3-33d2be68da28\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>wd299</td>\n",
              "      <td>wd695</td>\n",
              "      <td>wd899</td>\n",
              "      <td>wd367</td>\n",
              "      <td>wd203</td>\n",
              "      <td>wd848</td>\n",
              "      <td>wd743</td>\n",
              "      <td>wd365</td>\n",
              "      <td>wd808</td>\n",
              "      <td>wd57</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>wd192</td>\n",
              "      <td>wd9</td>\n",
              "      <td>wd932</td>\n",
              "      <td>wd178</td>\n",
              "      <td>wd505</td>\n",
              "      <td>wd295</td>\n",
              "      <td>wd102</td>\n",
              "      <td>wd407</td>\n",
              "      <td>wd868</td>\n",
              "      <td>wd750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>wd151</td>\n",
              "      <td>wd45</td>\n",
              "      <td>wd742</td>\n",
              "      <td>wd755</td>\n",
              "      <td>wd503</td>\n",
              "      <td>wd510</td>\n",
              "      <td>wd569</td>\n",
              "      <td>wd946</td>\n",
              "      <td>wd102</td>\n",
              "      <td>wd506</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>wd949</td>\n",
              "      <td>wd235</td>\n",
              "      <td>wd540</td>\n",
              "      <td>wd692</td>\n",
              "      <td>wd286</td>\n",
              "      <td>wd121</td>\n",
              "      <td>wd250</td>\n",
              "      <td>wd711</td>\n",
              "      <td>wd914</td>\n",
              "      <td>wd520</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>wd936</td>\n",
              "      <td>wd543</td>\n",
              "      <td>wd673</td>\n",
              "      <td>wd779</td>\n",
              "      <td>wd174</td>\n",
              "      <td>wd877</td>\n",
              "      <td>wd554</td>\n",
              "      <td>wd464</td>\n",
              "      <td>wd414</td>\n",
              "      <td>wd840</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>wd183</td>\n",
              "      <td>wd346</td>\n",
              "      <td>wd313</td>\n",
              "      <td>wd600</td>\n",
              "      <td>wd385</td>\n",
              "      <td>wd44</td>\n",
              "      <td>wd193</td>\n",
              "      <td>wd140</td>\n",
              "      <td>wd703</td>\n",
              "      <td>wd960</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>wd75</td>\n",
              "      <td>wd235</td>\n",
              "      <td>wd931</td>\n",
              "      <td>wd957</td>\n",
              "      <td>wd228</td>\n",
              "      <td>wd698</td>\n",
              "      <td>wd844</td>\n",
              "      <td>wd651</td>\n",
              "      <td>wd74</td>\n",
              "      <td>wd477</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>wd59</td>\n",
              "      <td>wd583</td>\n",
              "      <td>wd223</td>\n",
              "      <td>wd927</td>\n",
              "      <td>wd64</td>\n",
              "      <td>wd916</td>\n",
              "      <td>wd811</td>\n",
              "      <td>wd297</td>\n",
              "      <td>wd618</td>\n",
              "      <td>wd840</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>wd687</td>\n",
              "      <td>wd466</td>\n",
              "      <td>wd988</td>\n",
              "      <td>wd31</td>\n",
              "      <td>wd277</td>\n",
              "      <td>wd734</td>\n",
              "      <td>wd468</td>\n",
              "      <td>wd494</td>\n",
              "      <td>wd72</td>\n",
              "      <td>wd434</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>wd563</td>\n",
              "      <td>wd595</td>\n",
              "      <td>wd886</td>\n",
              "      <td>wd196</td>\n",
              "      <td>wd428</td>\n",
              "      <td>wd747</td>\n",
              "      <td>wd273</td>\n",
              "      <td>wd857</td>\n",
              "      <td>wd97</td>\n",
              "      <td>wd298</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3f4b2b53-a049-46cd-b2b3-33d2be68da28')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3f4b2b53-a049-46cd-b2b3-33d2be68da28 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3f4b2b53-a049-46cd-b2b3-33d2be68da28');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "       0      1      2      3      4      5      6      7      8      9\n",
              "0  wd299  wd695  wd899  wd367  wd203  wd848  wd743  wd365  wd808   wd57\n",
              "1  wd192    wd9  wd932  wd178  wd505  wd295  wd102  wd407  wd868  wd750\n",
              "2  wd151   wd45  wd742  wd755  wd503  wd510  wd569  wd946  wd102  wd506\n",
              "3  wd949  wd235  wd540  wd692  wd286  wd121  wd250  wd711  wd914  wd520\n",
              "4  wd936  wd543  wd673  wd779  wd174  wd877  wd554  wd464  wd414  wd840\n",
              "5  wd183  wd346  wd313  wd600  wd385   wd44  wd193  wd140  wd703  wd960\n",
              "6   wd75  wd235  wd931  wd957  wd228  wd698  wd844  wd651   wd74  wd477\n",
              "7   wd59  wd583  wd223  wd927   wd64  wd916  wd811  wd297  wd618  wd840\n",
              "8  wd687  wd466  wd988   wd31  wd277  wd734  wd468  wd494   wd72  wd434\n",
              "9  wd563  wd595  wd886  wd196  wd428  wd747  wd273  wd857   wd97  wd298"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ],
      "source": [
        "topics_centr = pd.DataFrame(avitm_centr.get_topics(10)).T\n",
        "topics_centr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z1w7cb9r7QAl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f31722e5-1422-4ef7-8339-4ca72fe72ea0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Sampling: [5/5]: : 5it [00:01,  2.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5000, 10)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "doc_topic_centr = get_doc_topic_distribution(avitm_centr, train_data_centr, n_samples=5) # get all the topic predictions\n",
        "print(doc_topic_centr.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YHry56Bz7sbV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99641b4f-c7b0-45bb-e888-9a6e851fa4af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.         0.         0.         ... 0.00231558 0.         0.        ]\n",
            " [0.         0.         0.         ... 0.00226759 0.         0.        ]\n",
            " [0.         0.         0.         ... 0.00228929 0.         0.        ]\n",
            " ...\n",
            " [0.         0.         0.         ... 0.00232625 0.         0.        ]\n",
            " [0.         0.         0.         ... 0.00230814 0.         0.        ]\n",
            " [0.         0.         0.         ... 0.00229191 0.         0.        ]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9999999999999993"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ],
      "source": [
        "w_t_distrib_centr = np.zeros((10,vocab_size), dtype=np.float64) # vocab_size = 10000\n",
        "wd = get_topic_word_distribution(avitm_centr)\n",
        "for i in np.arange(10):\n",
        "  for idx, word in id2token_centr.items():\n",
        "    for j in np.arange(len(all_words)):\n",
        "      if all_words[j] == word:\n",
        "        w_t_distrib_centr[i,j] = wd[i][idx]\n",
        "        break\n",
        "sum_of_rows = w_t_distrib_centr.sum(axis=1)\n",
        "w_t_distrib_centr_norm = w_t_distrib_centr / sum_of_rows[:, np.newaxis]\n",
        "print(w_t_distrib_centr_norm)\n",
        "sum(w_t_distrib_centr_norm[8,:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usiERbR-8Roe"
      },
      "source": [
        "# 4. Get similarity through Frobenius"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ng1ykc1A9wkn"
      },
      "outputs": [],
      "source": [
        "doc_topic_centr_all = []\n",
        "doc_topic_centr_all.append(doc_topic_centr[0:1000,:])\n",
        "doc_topic_centr_all.append(doc_topic_centr[1000:2000,:])\n",
        "doc_topic_centr_all.append(doc_topic_centr[2000:3000,:])\n",
        "doc_topic_centr_all.append(doc_topic_centr[3000:4000,:])\n",
        "doc_topic_centr_all.append(doc_topic_centr[4000:5000,:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-NR5xZgQ8S8W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb02005d-5054-4130-eeea-14902f3e9d19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NODE 0\n",
            "GT vs inferred in node: 287.22899328830357\n",
            "GT vs centralized in node 282.6274262262078\n",
            "***************************************************************\n",
            "NODE 1\n",
            "GT vs inferred in node: 286.912069724552\n",
            "GT vs centralized in node 285.37066121923334\n",
            "***************************************************************\n",
            "NODE 2\n",
            "GT vs inferred in node: 284.24708007151776\n",
            "GT vs centralized in node 284.44226885643246\n",
            "***************************************************************\n",
            "NODE 3\n",
            "GT vs inferred in node: 287.79330734013394\n",
            "GT vs centralized in node 284.4844914002617\n",
            "***************************************************************\n",
            "NODE 4\n",
            "GT vs inferred in node: 291.5531050392916\n",
            "GT vs centralized in node 291.4189817720329\n",
            "***************************************************************\n"
          ]
        }
      ],
      "source": [
        "for node in np.arange(n_nodes):\n",
        "  # Ground truth in node vs inferred in node\n",
        "  doc_topics_avitm_sqrt_node = np.sqrt(doc_topic_all[node])\n",
        "  similarity_avitm_node = doc_topics_avitm_sqrt_node.dot(doc_topics_avitm_sqrt_node.T)\n",
        "\n",
        "  doc_topics_gt_sqrt_node = np.sqrt(doc_topics_all_gt[node])\n",
        "  similarity_gt = doc_topics_gt_sqrt_node.dot(doc_topics_gt_sqrt_node.T)\n",
        "\n",
        "  diff_sims = similarity_avitm_node - similarity_gt\n",
        "  frobenius_diff_sims_node = np.linalg.norm(diff_sims,'fro')\n",
        "\n",
        "  # Ground truth in node vs centralized (for documents of such a node)\n",
        "  doc_topics_avitm_sqrt_centr_node = np.sqrt(doc_topic_centr_all[node])\n",
        "  similarity_avitm_centr = doc_topics_avitm_sqrt_centr_node.dot(doc_topics_avitm_sqrt_centr_node.T)\n",
        "\n",
        "  diff_sims = similarity_avitm_centr - similarity_gt\n",
        "  frobenius_diff_sims_avg = np.linalg.norm(diff_sims,'fro')\n",
        "\n",
        "  print(\"NODE\", str(node))\n",
        "  print(\"GT vs inferred in node:\", frobenius_diff_sims_node)\n",
        "  print(\"GT vs centralized in node\", frobenius_diff_sims_avg)\n",
        "  print(\"***************************************************************\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uQ6YfIbw-72y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d83faa94-479f-4fe0-ab0a-896bb94ceadd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NODE 0\n",
            "Original vs inferred in node sum max row: 1.0923508780294666\n",
            "***************************************************************\n",
            "NODE 1\n",
            "Original vs inferred in node sum max row: 1.1560196602730293\n",
            "***************************************************************\n",
            "NODE 2\n",
            "Original vs inferred in node sum max row: 1.1830830689462692\n",
            "***************************************************************\n",
            "NODE 3\n",
            "Original vs inferred in node sum max row: 1.0744695974009768\n",
            "***************************************************************\n",
            "NODE 4\n",
            "Original vs inferred in node sum max row: 1.1517160148190082\n",
            "***************************************************************\n",
            "CENTRALIZED\n",
            "Original vs avg of inferred in nodes sum max row 1.1576834722534899\n"
          ]
        }
      ],
      "source": [
        "for node in np.arange(n_nodes):\n",
        "  # GT vs inferred in node\n",
        "  topic_words_gt_sqrt = np.sqrt(topic_vectors)\n",
        "  topic_words_avtim_node_sqrt = np.sqrt(topic_word_all[node])\n",
        "  simmat_t_w = topic_words_gt_sqrt.dot(topic_words_avtim_node_sqrt.T)\n",
        "\n",
        "  simmat_t_w_pd = pd.DataFrame(simmat_t_w)\n",
        "  maxValues_rows = simmat_t_w_pd.max(axis = 1)\n",
        "  max_values_rows_sum = maxValues_rows.sum()\n",
        "\n",
        "  print(\"NODE\", str(node))\n",
        "  print(\"Original vs inferred in node sum max row:\", max_values_rows_sum)\n",
        "  print(\"***************************************************************\")\n",
        "\n",
        "# GT vs centralized\n",
        "topic_words_avtim_centr_sqrt = np.sqrt(w_t_distrib_centr_norm)\n",
        "simmat_t_w = topic_words_gt_sqrt.dot(topic_words_avtim_centr_sqrt.T)\n",
        "\n",
        "simmat_t_w_pd = pd.DataFrame(simmat_t_w)\n",
        "maxValues_rows = simmat_t_w_pd.max(axis = 1)\n",
        "max_values_rows_sum_centr = maxValues_rows.sum()\n",
        "\n",
        "print(\"CENTRALIZED\")\n",
        "print(\"Original vs avg of inferred in nodes sum max row\", max_values_rows_sum_centr)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.imshow(simmat_t_w)\n",
        "plt.colorbar()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "f62uaxhLKIOF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "a2733409-d0d7-4747-a8ab-666a30049e51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Centralized-ProdLDA-vocab-1000_beta-1e-2_prior-0.5.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}