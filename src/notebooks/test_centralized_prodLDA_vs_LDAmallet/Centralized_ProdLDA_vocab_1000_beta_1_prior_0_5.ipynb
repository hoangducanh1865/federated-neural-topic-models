{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sib1HSks6Xqv"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from scipy.special import softmax\n",
        "import multiprocessing as mp\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "from sklearn.preprocessing import normalize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CObHMSd6LLz"
      },
      "source": [
        "# Installing ProdLDA\n",
        "**Restart notbook after the installation!!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-VDAzxJA6FK5",
        "outputId": "42073240-5b77-43c3-ce14-65371661900c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'PyTorchAVITM' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/estebandito22/PyTorchAVITM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6kW5jO66UKj"
      },
      "source": [
        "# 1. Creation of synthetic corpus\n",
        "\n",
        "We consider a scenario with n parties, each of them as an associated corpus.\n",
        "To generate the corpus associated with each of the parties, we consider a common beta distribution (word-topic distribution), but we freeze different topics/ assign different asymmetric Dirichlet priors favoring different topics at the time of generating the document that composes each party's corpus."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twSZ3G0p6d1z"
      },
      "source": [
        "## 1.1. Function for permuting the Dirichlet prior at each node"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tXdkpdrh6Thn"
      },
      "outputs": [],
      "source": [
        "def rotateArray(arr, n, d):\n",
        "    temp = []\n",
        "    i = 0\n",
        "    while (i < d):\n",
        "        temp.append(arr[i])\n",
        "        i = i + 1\n",
        "    i = 0\n",
        "    while (d < n):\n",
        "        arr[i] = arr[d]\n",
        "        i = i + 1\n",
        "        d = d + 1\n",
        "    arr[:] = arr[: i] + temp\n",
        "    return arr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AyFA9eGH6hGH"
      },
      "source": [
        "## 1.2. Topic modeling and node settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5DmfSiuR6iI0",
        "outputId": "e92531f5-4d9f-478c-f664-323232d989f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.1, 0.1, 0.1, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5]\n"
          ]
        }
      ],
      "source": [
        "# Topic modeling settings\n",
        "vocab_size = 1000\n",
        "n_topics = 10\n",
        "beta = 1\n",
        "alpha = 5/n_topics\n",
        "n_docs = 1000\n",
        "nwords = (150, 450) #Min and max lengths of the documents\n",
        "\n",
        "# Nodes settings\n",
        "n_nodes = 5\n",
        "frozen_topics = 3\n",
        "dirichlet_symmetric = False\n",
        "prior = (n_topics)*[0.5]\n",
        "prior[0] = prior[1] = prior[2] = 0.1\n",
        "print(prior)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ylo9Vsu6zpX"
      },
      "source": [
        "## 1.3. Topics generation (common for all nodes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uy3AuOSx6qc1",
        "outputId": "5d779296-810c-422b-9f3a-99e5f150fda4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ordered probabilities for the first topic vector:\n",
            "[8.07766146e-03 5.90590006e-03 5.03706131e-03 4.77474708e-03\n",
            " 4.71579092e-03 4.63659749e-03 4.48036648e-03 4.17679839e-03\n",
            " 4.14668626e-03 4.12846695e-03 4.09037318e-03 4.07047039e-03\n",
            " 4.06895228e-03 4.05587156e-03 4.01421607e-03 3.88812003e-03\n",
            " 3.76428794e-03 3.73171344e-03 3.70796800e-03 3.68973680e-03\n",
            " 3.67250998e-03 3.66480226e-03 3.58738841e-03 3.55786975e-03\n",
            " 3.51525670e-03 3.51021667e-03 3.49744246e-03 3.48378456e-03\n",
            " 3.46692597e-03 3.44714707e-03 3.36346715e-03 3.33104507e-03\n",
            " 3.28025098e-03 3.27820366e-03 3.26609394e-03 3.23595961e-03\n",
            " 3.22964144e-03 3.16067722e-03 3.14533650e-03 3.13585512e-03\n",
            " 3.11260136e-03 3.05817373e-03 3.03869747e-03 3.02797234e-03\n",
            " 2.96710504e-03 2.96204238e-03 2.96112737e-03 2.94330453e-03\n",
            " 2.90185082e-03 2.87304483e-03 2.86957986e-03 2.85407630e-03\n",
            " 2.83168041e-03 2.82443396e-03 2.82359645e-03 2.81372979e-03\n",
            " 2.81057254e-03 2.80250596e-03 2.80028853e-03 2.79600323e-03\n",
            " 2.79420589e-03 2.78264986e-03 2.77700390e-03 2.74367396e-03\n",
            " 2.73655216e-03 2.71993501e-03 2.70701658e-03 2.62642737e-03\n",
            " 2.61703937e-03 2.59969715e-03 2.58416808e-03 2.57825058e-03\n",
            " 2.57219958e-03 2.56984128e-03 2.56935573e-03 2.56862218e-03\n",
            " 2.56408789e-03 2.56355153e-03 2.52014065e-03 2.51978679e-03\n",
            " 2.50213642e-03 2.49971671e-03 2.49146899e-03 2.48492022e-03\n",
            " 2.46393770e-03 2.44552472e-03 2.43943981e-03 2.43222454e-03\n",
            " 2.42314054e-03 2.40668652e-03 2.39943579e-03 2.39380219e-03\n",
            " 2.37259090e-03 2.36983559e-03 2.35543627e-03 2.33342734e-03\n",
            " 2.31585446e-03 2.31061142e-03 2.29917362e-03 2.29138834e-03\n",
            " 2.28470440e-03 2.28001485e-03 2.25006017e-03 2.24954371e-03\n",
            " 2.24565876e-03 2.23633409e-03 2.22604742e-03 2.21921444e-03\n",
            " 2.21491640e-03 2.21430312e-03 2.21179975e-03 2.19685257e-03\n",
            " 2.18311197e-03 2.17771054e-03 2.15969649e-03 2.15052417e-03\n",
            " 2.13673142e-03 2.12400515e-03 2.11961158e-03 2.11945145e-03\n",
            " 2.10795506e-03 2.10082371e-03 2.09525225e-03 2.07530085e-03\n",
            " 2.07110474e-03 2.06593679e-03 2.05465945e-03 2.05003860e-03\n",
            " 2.04500494e-03 2.04461733e-03 2.04229163e-03 2.03903766e-03\n",
            " 2.01794781e-03 2.00442899e-03 1.99319988e-03 1.97422461e-03\n",
            " 1.97409856e-03 1.96056369e-03 1.95165629e-03 1.93295630e-03\n",
            " 1.93152986e-03 1.93098225e-03 1.93084694e-03 1.93011254e-03\n",
            " 1.92938933e-03 1.92751099e-03 1.92399915e-03 1.92396784e-03\n",
            " 1.91614434e-03 1.91580371e-03 1.90628494e-03 1.90399966e-03\n",
            " 1.90339214e-03 1.90170834e-03 1.90008853e-03 1.89793787e-03\n",
            " 1.88508994e-03 1.87453269e-03 1.86485007e-03 1.86416897e-03\n",
            " 1.85283644e-03 1.84273613e-03 1.83915571e-03 1.80193069e-03\n",
            " 1.79855630e-03 1.78867922e-03 1.78205693e-03 1.77827921e-03\n",
            " 1.77578627e-03 1.77548621e-03 1.74631559e-03 1.74448331e-03\n",
            " 1.74265236e-03 1.74073891e-03 1.73911483e-03 1.73639123e-03\n",
            " 1.72870542e-03 1.72625239e-03 1.72387594e-03 1.71951519e-03\n",
            " 1.71674268e-03 1.71533609e-03 1.71517341e-03 1.71402459e-03\n",
            " 1.71124021e-03 1.70911950e-03 1.68963064e-03 1.68915602e-03\n",
            " 1.68854852e-03 1.68830263e-03 1.67308640e-03 1.66409738e-03\n",
            " 1.66238298e-03 1.66210264e-03 1.66030975e-03 1.65774153e-03\n",
            " 1.64667816e-03 1.64579489e-03 1.63970659e-03 1.63267869e-03\n",
            " 1.63210850e-03 1.63149749e-03 1.62896285e-03 1.62649690e-03\n",
            " 1.62407365e-03 1.61731058e-03 1.61249179e-03 1.60918542e-03\n",
            " 1.60793921e-03 1.59177730e-03 1.56448789e-03 1.56397922e-03\n",
            " 1.56369779e-03 1.55857340e-03 1.55743528e-03 1.55273375e-03\n",
            " 1.55236199e-03 1.54012964e-03 1.53572325e-03 1.53465694e-03\n",
            " 1.53042992e-03 1.52978690e-03 1.52706622e-03 1.52615674e-03\n",
            " 1.51517698e-03 1.50146881e-03 1.49886049e-03 1.49557058e-03\n",
            " 1.49454781e-03 1.49424948e-03 1.49077187e-03 1.48863069e-03\n",
            " 1.47852134e-03 1.47832387e-03 1.46954662e-03 1.46546772e-03\n",
            " 1.45659262e-03 1.44997735e-03 1.44977785e-03 1.44840889e-03\n",
            " 1.44827298e-03 1.44422788e-03 1.43971350e-03 1.43896938e-03\n",
            " 1.43892341e-03 1.43368499e-03 1.41418994e-03 1.41406052e-03\n",
            " 1.41376210e-03 1.41301695e-03 1.41287908e-03 1.41162467e-03\n",
            " 1.39388398e-03 1.38396639e-03 1.37558034e-03 1.37235074e-03\n",
            " 1.37018700e-03 1.36935949e-03 1.36746310e-03 1.35994503e-03\n",
            " 1.35887355e-03 1.35820175e-03 1.35566679e-03 1.35335845e-03\n",
            " 1.34779033e-03 1.34630798e-03 1.34340411e-03 1.33651267e-03\n",
            " 1.33374161e-03 1.33175804e-03 1.32800440e-03 1.32580945e-03\n",
            " 1.32404624e-03 1.31155048e-03 1.31089628e-03 1.30782234e-03\n",
            " 1.30718338e-03 1.30425072e-03 1.29925961e-03 1.28685494e-03\n",
            " 1.28509029e-03 1.28348195e-03 1.27815372e-03 1.27785242e-03\n",
            " 1.27783706e-03 1.27625875e-03 1.27447767e-03 1.26318872e-03\n",
            " 1.26180351e-03 1.26148277e-03 1.25831419e-03 1.25730348e-03\n",
            " 1.25269326e-03 1.25156076e-03 1.24805200e-03 1.24728379e-03\n",
            " 1.24417392e-03 1.24272926e-03 1.24019882e-03 1.23710172e-03\n",
            " 1.23377067e-03 1.23117827e-03 1.22661866e-03 1.21859224e-03\n",
            " 1.21558379e-03 1.21544277e-03 1.21397039e-03 1.21333833e-03\n",
            " 1.21013296e-03 1.20770964e-03 1.20770014e-03 1.20671728e-03\n",
            " 1.20424691e-03 1.20241351e-03 1.20179559e-03 1.20094190e-03\n",
            " 1.18852456e-03 1.18751375e-03 1.18217765e-03 1.17843409e-03\n",
            " 1.17440183e-03 1.17106838e-03 1.16422206e-03 1.16080683e-03\n",
            " 1.16048352e-03 1.15817247e-03 1.15619381e-03 1.15605851e-03\n",
            " 1.15363740e-03 1.15299402e-03 1.14909892e-03 1.14298266e-03\n",
            " 1.13952510e-03 1.13810674e-03 1.13602636e-03 1.13373521e-03\n",
            " 1.13121329e-03 1.13000586e-03 1.12072965e-03 1.11942970e-03\n",
            " 1.11715192e-03 1.11712947e-03 1.11167467e-03 1.10810600e-03\n",
            " 1.10561199e-03 1.10300921e-03 1.09764396e-03 1.09648543e-03\n",
            " 1.09556629e-03 1.09513745e-03 1.09368870e-03 1.07807939e-03\n",
            " 1.07527178e-03 1.07142649e-03 1.06829654e-03 1.06669278e-03\n",
            " 1.06466899e-03 1.05793836e-03 1.05749965e-03 1.05706368e-03\n",
            " 1.05531826e-03 1.05511974e-03 1.04899711e-03 1.04889095e-03\n",
            " 1.04565265e-03 1.03745773e-03 1.03657952e-03 1.03652928e-03\n",
            " 1.02852232e-03 1.02695326e-03 1.02609423e-03 1.02319913e-03\n",
            " 1.02003559e-03 1.01096838e-03 1.00873633e-03 1.00821601e-03\n",
            " 1.00793986e-03 1.00643213e-03 1.00437686e-03 9.99144813e-04\n",
            " 9.93639842e-04 9.93636433e-04 9.93312007e-04 9.90572908e-04\n",
            " 9.89711083e-04 9.81181057e-04 9.76572666e-04 9.74197773e-04\n",
            " 9.71586633e-04 9.70661896e-04 9.69226705e-04 9.68774336e-04\n",
            " 9.68253802e-04 9.57889403e-04 9.56980565e-04 9.55360744e-04\n",
            " 9.54035066e-04 9.52729974e-04 9.51104583e-04 9.50727229e-04\n",
            " 9.49244698e-04 9.47688470e-04 9.46745055e-04 9.38745488e-04\n",
            " 9.37740685e-04 9.37345492e-04 9.35097177e-04 9.32675412e-04\n",
            " 9.28899495e-04 9.28428815e-04 9.28086629e-04 9.27203482e-04\n",
            " 9.25699583e-04 9.23000612e-04 9.21565591e-04 9.19081414e-04\n",
            " 9.16871199e-04 9.15845473e-04 9.15718716e-04 9.14860040e-04\n",
            " 9.11586238e-04 9.09167045e-04 9.08615113e-04 9.08585990e-04\n",
            " 9.08032988e-04 9.04432323e-04 9.03708848e-04 9.00921323e-04\n",
            " 8.99875250e-04 8.99632107e-04 8.98193140e-04 8.93776496e-04\n",
            " 8.93208969e-04 8.92714779e-04 8.92238440e-04 8.87384201e-04\n",
            " 8.86356984e-04 8.86185927e-04 8.81574177e-04 8.80381341e-04\n",
            " 8.80175590e-04 8.79533685e-04 8.77833623e-04 8.76212059e-04\n",
            " 8.75540278e-04 8.73709461e-04 8.73204852e-04 8.65727899e-04\n",
            " 8.65295443e-04 8.60026281e-04 8.60020068e-04 8.57774028e-04\n",
            " 8.53586504e-04 8.51743466e-04 8.50731777e-04 8.47062103e-04\n",
            " 8.45464571e-04 8.41543427e-04 8.41509730e-04 8.38832015e-04\n",
            " 8.36184505e-04 8.35193151e-04 8.25863942e-04 8.25606372e-04\n",
            " 8.24258674e-04 8.23915018e-04 8.22844394e-04 8.22570307e-04\n",
            " 8.21929241e-04 8.21518929e-04 8.16193865e-04 8.12317645e-04\n",
            " 8.08917468e-04 8.06488275e-04 8.02681306e-04 7.96797730e-04\n",
            " 7.95655345e-04 7.95586003e-04 7.91423291e-04 7.89380275e-04\n",
            " 7.79626227e-04 7.74195860e-04 7.70976626e-04 7.69844119e-04\n",
            " 7.66305631e-04 7.66159971e-04 7.63322212e-04 7.54707677e-04\n",
            " 7.52418590e-04 7.52274939e-04 7.52113273e-04 7.49603339e-04\n",
            " 7.48830481e-04 7.45790787e-04 7.42943122e-04 7.40473577e-04\n",
            " 7.35712932e-04 7.33808172e-04 7.30127939e-04 7.29332527e-04\n",
            " 7.28723490e-04 7.24649378e-04 7.21975476e-04 7.20241218e-04\n",
            " 7.19844342e-04 7.18481863e-04 7.17901519e-04 7.15633593e-04\n",
            " 7.15025719e-04 7.14934083e-04 7.12940383e-04 7.10568560e-04\n",
            " 7.10403904e-04 7.09347432e-04 7.06579635e-04 7.01239922e-04\n",
            " 6.98301290e-04 6.94268805e-04 6.91818337e-04 6.81337140e-04\n",
            " 6.80945348e-04 6.77403127e-04 6.77239358e-04 6.76670278e-04\n",
            " 6.75184928e-04 6.73360521e-04 6.72789803e-04 6.71809271e-04\n",
            " 6.71792614e-04 6.70798452e-04 6.69080993e-04 6.67516559e-04\n",
            " 6.64889529e-04 6.59157274e-04 6.58612960e-04 6.58488827e-04\n",
            " 6.57413205e-04 6.55026549e-04 6.53712803e-04 6.50866852e-04\n",
            " 6.50841358e-04 6.47664480e-04 6.47195728e-04 6.43602763e-04\n",
            " 6.41975084e-04 6.40815382e-04 6.38006617e-04 6.36676753e-04\n",
            " 6.33795028e-04 6.26647248e-04 6.26122438e-04 6.25344360e-04\n",
            " 6.24425468e-04 6.23805609e-04 6.20127406e-04 6.19130002e-04\n",
            " 6.15042189e-04 6.12097677e-04 6.07166485e-04 6.06792211e-04\n",
            " 6.04511941e-04 6.03849226e-04 6.02324062e-04 6.02187088e-04\n",
            " 6.00773098e-04 5.98482895e-04 5.96232651e-04 5.92419956e-04\n",
            " 5.92304441e-04 5.91419631e-04 5.88408996e-04 5.86860524e-04\n",
            " 5.85574510e-04 5.84518492e-04 5.79729482e-04 5.77787101e-04\n",
            " 5.76649539e-04 5.76011838e-04 5.75532515e-04 5.69880056e-04\n",
            " 5.66905366e-04 5.66609402e-04 5.66134807e-04 5.60852749e-04\n",
            " 5.58741169e-04 5.56587139e-04 5.54266465e-04 5.50559321e-04\n",
            " 5.49692751e-04 5.45667288e-04 5.43484977e-04 5.42141424e-04\n",
            " 5.41812951e-04 5.41673406e-04 5.41426564e-04 5.40172257e-04\n",
            " 5.40145192e-04 5.39701619e-04 5.39422619e-04 5.36059710e-04\n",
            " 5.33142700e-04 5.24951127e-04 5.22045540e-04 5.19438042e-04\n",
            " 5.18704097e-04 5.15462775e-04 5.13688944e-04 5.12703919e-04\n",
            " 5.11332778e-04 5.10409569e-04 5.10406263e-04 5.10123873e-04\n",
            " 5.06176519e-04 5.05878683e-04 5.02968526e-04 4.97919254e-04\n",
            " 4.91409722e-04 4.90821349e-04 4.89614027e-04 4.82245895e-04\n",
            " 4.81642041e-04 4.81478351e-04 4.79391613e-04 4.79222041e-04\n",
            " 4.77523236e-04 4.75554529e-04 4.75463980e-04 4.74713149e-04\n",
            " 4.70907227e-04 4.70446104e-04 4.68257865e-04 4.67011588e-04\n",
            " 4.65587244e-04 4.62469805e-04 4.61116968e-04 4.60123912e-04\n",
            " 4.59976962e-04 4.58421244e-04 4.54872359e-04 4.54455608e-04\n",
            " 4.49812415e-04 4.49006680e-04 4.46478759e-04 4.45541556e-04\n",
            " 4.45485377e-04 4.44615811e-04 4.43822636e-04 4.41225594e-04\n",
            " 4.39601273e-04 4.36639968e-04 4.35415363e-04 4.34352778e-04\n",
            " 4.33527766e-04 4.32225483e-04 4.31957490e-04 4.30563817e-04\n",
            " 4.29788772e-04 4.26810705e-04 4.25737752e-04 4.25694917e-04\n",
            " 4.25205507e-04 4.24744836e-04 4.24148404e-04 4.20128124e-04\n",
            " 4.18355661e-04 4.15897309e-04 4.13528530e-04 4.07924045e-04\n",
            " 4.07835218e-04 4.04235416e-04 4.03719117e-04 4.03566978e-04\n",
            " 4.02817196e-04 4.00010372e-04 3.98993795e-04 3.97879642e-04\n",
            " 3.97715169e-04 3.97690660e-04 3.97160781e-04 3.97127153e-04\n",
            " 3.94087145e-04 3.93622432e-04 3.93040884e-04 3.91494195e-04\n",
            " 3.87740994e-04 3.83354250e-04 3.83052886e-04 3.80606517e-04\n",
            " 3.79292781e-04 3.75726476e-04 3.74066521e-04 3.73488129e-04\n",
            " 3.69268612e-04 3.69182137e-04 3.66120751e-04 3.65369837e-04\n",
            " 3.65174815e-04 3.63166090e-04 3.61627747e-04 3.59943991e-04\n",
            " 3.58699839e-04 3.57813496e-04 3.55887641e-04 3.54911622e-04\n",
            " 3.54408617e-04 3.54352270e-04 3.53847215e-04 3.53802394e-04\n",
            " 3.53194420e-04 3.53086371e-04 3.52972731e-04 3.46303840e-04\n",
            " 3.45733687e-04 3.41614374e-04 3.38872281e-04 3.38738929e-04\n",
            " 3.38499038e-04 3.37394477e-04 3.36717992e-04 3.36173996e-04\n",
            " 3.34907841e-04 3.32394962e-04 3.29275723e-04 3.27101981e-04\n",
            " 3.25686734e-04 3.25610262e-04 3.22949416e-04 3.22865903e-04\n",
            " 3.19893443e-04 3.17560946e-04 3.17463217e-04 3.16867283e-04\n",
            " 3.14075754e-04 3.12835236e-04 3.11939464e-04 3.11904171e-04\n",
            " 3.11758176e-04 3.11745460e-04 3.11287470e-04 3.09569222e-04\n",
            " 3.08983752e-04 3.07089731e-04 3.00449910e-04 3.00348882e-04\n",
            " 3.00116058e-04 2.97183567e-04 2.95713052e-04 2.94599169e-04\n",
            " 2.94165510e-04 2.93861713e-04 2.93614648e-04 2.91522599e-04\n",
            " 2.90910837e-04 2.90041173e-04 2.89849994e-04 2.87709173e-04\n",
            " 2.87628981e-04 2.87341479e-04 2.86441244e-04 2.84686905e-04\n",
            " 2.84410853e-04 2.83714261e-04 2.83397817e-04 2.79253476e-04\n",
            " 2.77347490e-04 2.73846996e-04 2.72336927e-04 2.69765530e-04\n",
            " 2.68456740e-04 2.66660108e-04 2.65590718e-04 2.65480470e-04\n",
            " 2.64367552e-04 2.62596743e-04 2.60407860e-04 2.60284382e-04\n",
            " 2.59426570e-04 2.58850098e-04 2.58277420e-04 2.57140996e-04\n",
            " 2.55948620e-04 2.53955394e-04 2.52766918e-04 2.51435604e-04\n",
            " 2.50577727e-04 2.49646142e-04 2.46352621e-04 2.42962724e-04\n",
            " 2.39320668e-04 2.36480012e-04 2.34187998e-04 2.34008935e-04\n",
            " 2.33022447e-04 2.32372164e-04 2.31062704e-04 2.30739102e-04\n",
            " 2.30322623e-04 2.30039014e-04 2.28943808e-04 2.24873131e-04\n",
            " 2.22378592e-04 2.22044693e-04 2.19340158e-04 2.16141848e-04\n",
            " 2.14441607e-04 2.14270656e-04 2.12247784e-04 2.11493865e-04\n",
            " 2.10583633e-04 2.08775499e-04 2.08708124e-04 2.07511535e-04\n",
            " 2.06981555e-04 2.06381212e-04 2.05168395e-04 2.02945727e-04\n",
            " 2.02697661e-04 1.99979096e-04 1.98670608e-04 1.98294032e-04\n",
            " 1.95122417e-04 1.93315475e-04 1.90496616e-04 1.90415116e-04\n",
            " 1.89015207e-04 1.87919880e-04 1.87260147e-04 1.86578554e-04\n",
            " 1.85047858e-04 1.84159863e-04 1.83143143e-04 1.80239517e-04\n",
            " 1.79701785e-04 1.78180653e-04 1.77808570e-04 1.73901423e-04\n",
            " 1.73216896e-04 1.72316236e-04 1.69954327e-04 1.68889471e-04\n",
            " 1.67591135e-04 1.66703805e-04 1.66674701e-04 1.66588943e-04\n",
            " 1.65624616e-04 1.65488354e-04 1.65359717e-04 1.64297259e-04\n",
            " 1.62582442e-04 1.62336861e-04 1.61884360e-04 1.59611364e-04\n",
            " 1.57587936e-04 1.57419971e-04 1.57087634e-04 1.56628470e-04\n",
            " 1.56097724e-04 1.55911455e-04 1.55418622e-04 1.55395084e-04\n",
            " 1.55315386e-04 1.54516953e-04 1.51052151e-04 1.50034011e-04\n",
            " 1.48273796e-04 1.47768263e-04 1.47014848e-04 1.46714446e-04\n",
            " 1.46501181e-04 1.44153814e-04 1.42031982e-04 1.41645714e-04\n",
            " 1.37150283e-04 1.36314564e-04 1.35250739e-04 1.34235353e-04\n",
            " 1.34158161e-04 1.32422896e-04 1.32144823e-04 1.29633435e-04\n",
            " 1.28648138e-04 1.28602850e-04 1.27698862e-04 1.25233339e-04\n",
            " 1.24366222e-04 1.22570320e-04 1.20235594e-04 1.19844463e-04\n",
            " 1.19784511e-04 1.15995235e-04 1.13594296e-04 1.11608028e-04\n",
            " 1.10958120e-04 1.07372146e-04 1.05902350e-04 1.05735688e-04\n",
            " 1.04119212e-04 1.03031084e-04 1.02228601e-04 1.00968603e-04\n",
            " 9.89154648e-05 9.83799669e-05 9.27563427e-05 9.16157250e-05\n",
            " 9.11085984e-05 8.81085079e-05 8.62079711e-05 8.60643402e-05\n",
            " 8.60539587e-05 8.47542651e-05 8.31075446e-05 8.30056784e-05\n",
            " 8.19488258e-05 8.19111062e-05 8.05888944e-05 8.01218047e-05\n",
            " 7.98580753e-05 7.93871095e-05 7.85075428e-05 7.67934078e-05\n",
            " 7.64809808e-05 7.38980194e-05 7.29831824e-05 7.24525753e-05\n",
            " 7.22781327e-05 7.10573517e-05 6.91166722e-05 6.88108488e-05\n",
            " 6.75775197e-05 6.68197800e-05 6.52934103e-05 6.41594956e-05\n",
            " 6.36641891e-05 6.30448371e-05 6.24992339e-05 5.99370399e-05\n",
            " 5.92988688e-05 5.83344125e-05 5.77360076e-05 5.71069272e-05\n",
            " 5.56668093e-05 5.52740918e-05 5.41738723e-05 5.24339609e-05\n",
            " 5.03938284e-05 4.97937124e-05 4.83702963e-05 4.77788997e-05\n",
            " 4.70822772e-05 4.64753377e-05 4.63383086e-05 4.60613998e-05\n",
            " 4.48664786e-05 4.42894926e-05 4.42029586e-05 4.41232942e-05\n",
            " 4.33478178e-05 4.26098085e-05 4.06155718e-05 4.05274690e-05\n",
            " 4.03492358e-05 3.86278554e-05 3.77334013e-05 3.66141525e-05\n",
            " 3.62871786e-05 3.50063512e-05 3.42118066e-05 3.24752825e-05\n",
            " 2.89569831e-05 2.70860268e-05 2.56522244e-05 2.42305868e-05\n",
            " 2.28236468e-05 2.05220658e-05 2.00970680e-05 1.93883059e-05\n",
            " 1.92404868e-05 1.85124748e-05 1.74656122e-05 1.71261298e-05\n",
            " 1.59397570e-05 1.56704372e-05 1.56599547e-05 1.36842168e-05\n",
            " 1.28980702e-05 1.22641234e-05 1.00736808e-05 1.00155076e-05\n",
            " 9.53376581e-06 9.21003315e-06 8.88013195e-06 8.39459839e-06\n",
            " 6.20188578e-06 3.12717883e-06 2.75990249e-06 1.61460999e-06\n",
            " 1.53741956e-06 1.08450883e-06 8.62850599e-07 1.02345458e-07]\n",
            "(10, 1000)\n"
          ]
        }
      ],
      "source": [
        "topic_vectors = np.random.dirichlet(vocab_size*[beta], n_topics)\n",
        "print('Ordered probabilities for the first topic vector:')\n",
        "print(np.sort(topic_vectors[0])[::-1])\n",
        "print(topic_vectors.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BaQEe-yD6vl_"
      },
      "source": [
        "## 1.4. Generation of document topic proportions and documents for each node\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N-BziCW56vFL",
        "outputId": "faf3278c-0cc9-41ae-bb65-b374bbab2f34",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.1, 0.1, 0.1]\n",
            "Ordered probabilities for the first document - node 0 :\n",
            "[3.55478479e-01 2.17517037e-01 1.57102425e-01 1.13366289e-01\n",
            " 1.11457115e-01 2.32896573e-02 2.05659742e-02 1.22302137e-03\n",
            " 2.25986345e-09 3.36255051e-12]\n",
            "Documents of node 0 generated.\n",
            "[0.5, 0.5, 0.5, 0.5, 0.1, 0.1, 0.1, 0.5, 0.5, 0.5]\n",
            "Ordered probabilities for the first document - node 1 :\n",
            "[3.10387128e-01 2.15851338e-01 2.00631637e-01 9.78134731e-02\n",
            " 8.51274101e-02 5.20457359e-02 3.40690316e-02 2.70630774e-03\n",
            " 1.36308207e-03 4.85675511e-06]\n",
            "Documents of node 1 generated.\n",
            "[0.5, 0.1, 0.1, 0.1, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5]\n",
            "Ordered probabilities for the first document - node 2 :\n",
            "[4.17142243e-01 2.67334446e-01 1.96157381e-01 5.08364732e-02\n",
            " 3.54288741e-02 1.34549834e-02 9.64114416e-03 8.48498233e-03\n",
            " 1.51947189e-03 4.07635232e-10]\n",
            "Documents of node 2 generated.\n",
            "[0.1, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.1, 0.1]\n",
            "Ordered probabilities for the first document - node 3 :\n",
            "[0.73799812 0.10732631 0.07543702 0.03902885 0.02138577 0.00689802\n",
            " 0.00507842 0.0046758  0.00113034 0.00104134]\n",
            "Documents of node 3 generated.\n",
            "[0.5, 0.5, 0.5, 0.5, 0.5, 0.1, 0.1, 0.1, 0.5, 0.5]\n",
            "Ordered probabilities for the first document - node 4 :\n",
            "[3.62770143e-01 2.41093704e-01 1.81207242e-01 1.21715546e-01\n",
            " 4.22005587e-02 3.91752659e-02 1.06471228e-02 1.17577688e-03\n",
            " 1.46401283e-05 3.39572128e-27]\n",
            "Documents of node 4 generated.\n"
          ]
        }
      ],
      "source": [
        "doc_topics_all_gt = []\n",
        "documents_all = []\n",
        "z_all = []\n",
        "for i in np.arange(n_nodes):\n",
        "  # Step 2 - generation of document topic proportions for each node\n",
        "  if dirichlet_symmetric:\n",
        "    doc_topics = np.random.dirichlet((n_topics)*[alpha], n_docs)\n",
        "  else:\n",
        "    doc_topics = np.random.dirichlet(prior, n_docs)\n",
        "    prior = rotateArray(prior, len(prior), 3)\n",
        "    print(prior)\n",
        "  print('Ordered probabilities for the first document - node', str(i), ':')\n",
        "  print(np.sort(doc_topics[0])[::-1])\n",
        "  doc_topics_all_gt.append(doc_topics)\n",
        "  # Step 3 - Document generation\n",
        "  documents = [] # Document words\n",
        "  z = [] # Assignments\n",
        "  for docid in np.arange(n_docs):\n",
        "      doc_len = np.random.randint(low=nwords[0], high=nwords[1])\n",
        "      this_doc_words = []\n",
        "      this_doc_assigns = []\n",
        "      for wd_idx in np.arange(doc_len):\n",
        "          tpc = np.nonzero(np.random.multinomial(1, doc_topics[docid]))[0][0]\n",
        "          this_doc_assigns.append(tpc)\n",
        "          word = np.nonzero(np.random.multinomial(1, topic_vectors[tpc]))[0][0]\n",
        "          this_doc_words.append('wd'+str(word))\n",
        "      z.append(this_doc_assigns)\n",
        "      documents.append(this_doc_words)\n",
        "  print(\"Documents of node\", str(i), \"generated.\")\n",
        "  documents_all.append(documents)\n",
        "  z_all.append(z)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HuJUlOIJ69iw"
      },
      "source": [
        "# 2. Preprocessing, generation of training dataset and training of a ProdLDA model at each node"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1XwiP814FZ5U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56882907-8c99-41a1-b043-9d245e96df5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "call_signature.png  LICENSE.md  \u001b[0m\u001b[01;34mpytorchavitm\u001b[0m/  README.md  train_abs.py\n",
            "\u001b[01;34mdata\u001b[0m/               \u001b[01;34moutputs\u001b[0m/    \u001b[01;34mPyTorchAVITM\u001b[0m/  setup.py   train.py\n"
          ]
        }
      ],
      "source": [
        "ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "14tnh0ndFpb0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d031e43b-1ea8-4c9e-bac6-abb10071787f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/PyTorchAVITM/pytorchavitm/datasets\n"
          ]
        }
      ],
      "source": [
        "cd /content/PyTorchAVITM/pytorchavitm/datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Is7SF6iQcqA"
      },
      "outputs": [],
      "source": [
        "from bow import BOWDataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k4okSYycQaOK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42de9a62-d4ed-4bbe-f0d7-1b4b78d31482"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/PyTorchAVITM\n"
          ]
        }
      ],
      "source": [
        "cd /content/PyTorchAVITM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JrcVglDoQhU0"
      },
      "outputs": [],
      "source": [
        "from pytorchavitm import AVITM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wchhyQ5bDIhK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34a63469-f91c-450c-dd25-d67ee4f59fbb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Settings: \n",
            "               N Components: 10\n",
            "               Topic Prior Mean: 0.0\n",
            "               Topic Prior Variance: 0.9\n",
            "               Model Type: prodLDA\n",
            "               Hidden Sizes: (100, 100)\n",
            "               Activation: softplus\n",
            "               Dropout: 0.2\n",
            "               Learn Priors: True\n",
            "               Learning Rate: 0.002\n",
            "               Momentum: 0.99\n",
            "               Reduce On Plateau: False\n",
            "               Save Dir: None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [1/100]\tSamples: [1000/100000]\tTrain Loss: 2222.313453125\tTime: 0:00:00.301616\n",
            "Epoch: [2/100]\tSamples: [2000/100000]\tTrain Loss: 2206.4955703125\tTime: 0:00:00.270453\n",
            "Epoch: [3/100]\tSamples: [3000/100000]\tTrain Loss: 2198.1028984375\tTime: 0:00:00.269072\n",
            "Epoch: [4/100]\tSamples: [4000/100000]\tTrain Loss: 2192.8098203125\tTime: 0:00:00.254640\n",
            "Epoch: [5/100]\tSamples: [5000/100000]\tTrain Loss: 2185.2210390625\tTime: 0:00:00.248859\n",
            "Epoch: [6/100]\tSamples: [6000/100000]\tTrain Loss: 2180.549234375\tTime: 0:00:00.237743\n",
            "Epoch: [7/100]\tSamples: [7000/100000]\tTrain Loss: 2175.238765625\tTime: 0:00:00.255502\n",
            "Epoch: [8/100]\tSamples: [8000/100000]\tTrain Loss: 2170.6970546875\tTime: 0:00:00.262276\n",
            "Epoch: [9/100]\tSamples: [9000/100000]\tTrain Loss: 2164.9206171875\tTime: 0:00:00.244289\n",
            "Epoch: [10/100]\tSamples: [10000/100000]\tTrain Loss: 2159.809453125\tTime: 0:00:00.231798\n",
            "Epoch: [11/100]\tSamples: [11000/100000]\tTrain Loss: 2156.9903984375\tTime: 0:00:00.243157\n",
            "Epoch: [12/100]\tSamples: [12000/100000]\tTrain Loss: 2154.45665625\tTime: 0:00:00.258548\n",
            "Epoch: [13/100]\tSamples: [13000/100000]\tTrain Loss: 2150.9135234375\tTime: 0:00:00.245615\n",
            "Epoch: [14/100]\tSamples: [14000/100000]\tTrain Loss: 2147.798921875\tTime: 0:00:00.253033\n",
            "Epoch: [15/100]\tSamples: [15000/100000]\tTrain Loss: 2146.5175546875\tTime: 0:00:00.259554\n",
            "Epoch: [16/100]\tSamples: [16000/100000]\tTrain Loss: 2143.974453125\tTime: 0:00:00.247862\n",
            "Epoch: [17/100]\tSamples: [17000/100000]\tTrain Loss: 2142.07525\tTime: 0:00:00.259509\n",
            "Epoch: [18/100]\tSamples: [18000/100000]\tTrain Loss: 2138.7658359375\tTime: 0:00:00.249849\n",
            "Epoch: [19/100]\tSamples: [19000/100000]\tTrain Loss: 2138.762078125\tTime: 0:00:00.269397\n",
            "Epoch: [20/100]\tSamples: [20000/100000]\tTrain Loss: 2136.350328125\tTime: 0:00:00.251179\n",
            "Epoch: [21/100]\tSamples: [21000/100000]\tTrain Loss: 2133.8050078125\tTime: 0:00:00.233481\n",
            "Epoch: [22/100]\tSamples: [22000/100000]\tTrain Loss: 2131.89703125\tTime: 0:00:00.256034\n",
            "Epoch: [23/100]\tSamples: [23000/100000]\tTrain Loss: 2128.812640625\tTime: 0:00:00.258467\n",
            "Epoch: [24/100]\tSamples: [24000/100000]\tTrain Loss: 2125.324125\tTime: 0:00:00.251786\n",
            "Epoch: [25/100]\tSamples: [25000/100000]\tTrain Loss: 2122.6530390625\tTime: 0:00:00.235507\n",
            "Epoch: [26/100]\tSamples: [26000/100000]\tTrain Loss: 2119.6751171875\tTime: 0:00:00.237202\n",
            "Epoch: [27/100]\tSamples: [27000/100000]\tTrain Loss: 2118.929171875\tTime: 0:00:00.252632\n",
            "Epoch: [28/100]\tSamples: [28000/100000]\tTrain Loss: 2112.962171875\tTime: 0:00:00.232521\n",
            "Epoch: [29/100]\tSamples: [29000/100000]\tTrain Loss: 2111.951046875\tTime: 0:00:00.256517\n",
            "Epoch: [30/100]\tSamples: [30000/100000]\tTrain Loss: 2107.465703125\tTime: 0:00:00.249191\n",
            "Epoch: [31/100]\tSamples: [31000/100000]\tTrain Loss: 2105.1247734375\tTime: 0:00:00.234435\n",
            "Epoch: [32/100]\tSamples: [32000/100000]\tTrain Loss: 2104.0859921875\tTime: 0:00:00.251894\n",
            "Epoch: [33/100]\tSamples: [33000/100000]\tTrain Loss: 2100.5941875\tTime: 0:00:00.234274\n",
            "Epoch: [34/100]\tSamples: [34000/100000]\tTrain Loss: 2098.5547421875\tTime: 0:00:00.242605\n",
            "Epoch: [35/100]\tSamples: [35000/100000]\tTrain Loss: 2097.16040625\tTime: 0:00:00.229535\n",
            "Epoch: [36/100]\tSamples: [36000/100000]\tTrain Loss: 2096.2826171875\tTime: 0:00:00.246592\n",
            "Epoch: [37/100]\tSamples: [37000/100000]\tTrain Loss: 2094.4579375\tTime: 0:00:00.246969\n",
            "Epoch: [38/100]\tSamples: [38000/100000]\tTrain Loss: 2094.5381328125\tTime: 0:00:00.231858\n",
            "Epoch: [39/100]\tSamples: [39000/100000]\tTrain Loss: 2092.7712421875\tTime: 0:00:00.242190\n",
            "Epoch: [40/100]\tSamples: [40000/100000]\tTrain Loss: 2091.4777421875\tTime: 0:00:00.270379\n",
            "Epoch: [41/100]\tSamples: [41000/100000]\tTrain Loss: 2091.558859375\tTime: 0:00:00.239815\n",
            "Epoch: [42/100]\tSamples: [42000/100000]\tTrain Loss: 2090.5887421875\tTime: 0:00:00.252009\n",
            "Epoch: [43/100]\tSamples: [43000/100000]\tTrain Loss: 2089.3148515625\tTime: 0:00:00.240489\n",
            "Epoch: [44/100]\tSamples: [44000/100000]\tTrain Loss: 2089.429765625\tTime: 0:00:00.257010\n",
            "Epoch: [45/100]\tSamples: [45000/100000]\tTrain Loss: 2088.4840546875\tTime: 0:00:00.253008\n",
            "Epoch: [46/100]\tSamples: [46000/100000]\tTrain Loss: 2087.93515625\tTime: 0:00:00.241310\n",
            "Epoch: [47/100]\tSamples: [47000/100000]\tTrain Loss: 2088.0464453125\tTime: 0:00:00.237392\n",
            "Epoch: [48/100]\tSamples: [48000/100000]\tTrain Loss: 2087.280765625\tTime: 0:00:00.268482\n",
            "Epoch: [49/100]\tSamples: [49000/100000]\tTrain Loss: 2087.5034609375\tTime: 0:00:00.252206\n",
            "Epoch: [50/100]\tSamples: [50000/100000]\tTrain Loss: 2086.7072265625\tTime: 0:00:00.239011\n",
            "Epoch: [51/100]\tSamples: [51000/100000]\tTrain Loss: 2086.8134375\tTime: 0:00:00.246431\n",
            "Epoch: [52/100]\tSamples: [52000/100000]\tTrain Loss: 2086.4337734375\tTime: 0:00:00.258823\n",
            "Epoch: [53/100]\tSamples: [53000/100000]\tTrain Loss: 2086.9112890625\tTime: 0:00:00.269798\n",
            "Epoch: [54/100]\tSamples: [54000/100000]\tTrain Loss: 2085.8285\tTime: 0:00:00.233352\n",
            "Epoch: [55/100]\tSamples: [55000/100000]\tTrain Loss: 2086.35521875\tTime: 0:00:00.244619\n",
            "Epoch: [56/100]\tSamples: [56000/100000]\tTrain Loss: 2085.7576875\tTime: 0:00:00.266174\n",
            "Epoch: [57/100]\tSamples: [57000/100000]\tTrain Loss: 2085.4308515625\tTime: 0:00:00.249678\n",
            "Epoch: [58/100]\tSamples: [58000/100000]\tTrain Loss: 2085.8932109375\tTime: 0:00:00.248979\n",
            "Epoch: [59/100]\tSamples: [59000/100000]\tTrain Loss: 2084.8236328125\tTime: 0:00:00.234926\n",
            "Epoch: [60/100]\tSamples: [60000/100000]\tTrain Loss: 2084.823375\tTime: 0:00:00.258513\n",
            "Epoch: [61/100]\tSamples: [61000/100000]\tTrain Loss: 2085.104625\tTime: 0:00:00.247423\n",
            "Epoch: [62/100]\tSamples: [62000/100000]\tTrain Loss: 2084.930140625\tTime: 0:00:00.235932\n",
            "Epoch: [63/100]\tSamples: [63000/100000]\tTrain Loss: 2085.0431484375\tTime: 0:00:00.230083\n",
            "Epoch: [64/100]\tSamples: [64000/100000]\tTrain Loss: 2083.74871875\tTime: 0:00:00.255370\n",
            "Epoch: [65/100]\tSamples: [65000/100000]\tTrain Loss: 2084.2710546875\tTime: 0:00:00.277879\n",
            "Epoch: [66/100]\tSamples: [66000/100000]\tTrain Loss: 2083.7488671875\tTime: 0:00:00.250792\n",
            "Epoch: [67/100]\tSamples: [67000/100000]\tTrain Loss: 2084.0506484375\tTime: 0:00:00.239174\n",
            "Epoch: [68/100]\tSamples: [68000/100000]\tTrain Loss: 2083.443734375\tTime: 0:00:00.234407\n",
            "Epoch: [69/100]\tSamples: [69000/100000]\tTrain Loss: 2083.7074921875\tTime: 0:00:00.264892\n",
            "Epoch: [70/100]\tSamples: [70000/100000]\tTrain Loss: 2082.9751171875\tTime: 0:00:00.247684\n",
            "Epoch: [71/100]\tSamples: [71000/100000]\tTrain Loss: 2083.448984375\tTime: 0:00:00.235089\n",
            "Epoch: [72/100]\tSamples: [72000/100000]\tTrain Loss: 2083.1740859375\tTime: 0:00:00.258150\n",
            "Epoch: [73/100]\tSamples: [73000/100000]\tTrain Loss: 2083.52771875\tTime: 0:00:00.261894\n",
            "Epoch: [74/100]\tSamples: [74000/100000]\tTrain Loss: 2083.59371875\tTime: 0:00:00.240710\n",
            "Epoch: [75/100]\tSamples: [75000/100000]\tTrain Loss: 2082.8123203125\tTime: 0:00:00.238766\n",
            "Epoch: [76/100]\tSamples: [76000/100000]\tTrain Loss: 2083.594109375\tTime: 0:00:00.259656\n",
            "Epoch: [77/100]\tSamples: [77000/100000]\tTrain Loss: 2083.2882890625\tTime: 0:00:00.257882\n",
            "Epoch: [78/100]\tSamples: [78000/100000]\tTrain Loss: 2082.780515625\tTime: 0:00:00.252119\n",
            "Epoch: [79/100]\tSamples: [79000/100000]\tTrain Loss: 2082.57253125\tTime: 0:00:00.225523\n",
            "Epoch: [80/100]\tSamples: [80000/100000]\tTrain Loss: 2082.695578125\tTime: 0:00:00.241655\n",
            "Epoch: [81/100]\tSamples: [81000/100000]\tTrain Loss: 2083.100546875\tTime: 0:00:00.264097\n",
            "Epoch: [82/100]\tSamples: [82000/100000]\tTrain Loss: 2083.151984375\tTime: 0:00:00.235246\n",
            "Epoch: [83/100]\tSamples: [83000/100000]\tTrain Loss: 2082.8966328125\tTime: 0:00:00.237883\n",
            "Epoch: [84/100]\tSamples: [84000/100000]\tTrain Loss: 2082.319765625\tTime: 0:00:00.230366\n",
            "Epoch: [85/100]\tSamples: [85000/100000]\tTrain Loss: 2082.1563828125\tTime: 0:00:00.256010\n",
            "Epoch: [86/100]\tSamples: [86000/100000]\tTrain Loss: 2082.293375\tTime: 0:00:00.244121\n",
            "Epoch: [87/100]\tSamples: [87000/100000]\tTrain Loss: 2083.0156953125\tTime: 0:00:00.240634\n",
            "Epoch: [88/100]\tSamples: [88000/100000]\tTrain Loss: 2082.2700546875\tTime: 0:00:00.242474\n",
            "Epoch: [89/100]\tSamples: [89000/100000]\tTrain Loss: 2082.441625\tTime: 0:00:00.251938\n",
            "Epoch: [90/100]\tSamples: [90000/100000]\tTrain Loss: 2082.343375\tTime: 0:00:00.249525\n",
            "Epoch: [91/100]\tSamples: [91000/100000]\tTrain Loss: 2082.3976640625\tTime: 0:00:00.229213\n",
            "Epoch: [92/100]\tSamples: [92000/100000]\tTrain Loss: 2082.747625\tTime: 0:00:00.245395\n",
            "Epoch: [93/100]\tSamples: [93000/100000]\tTrain Loss: 2082.2322265625\tTime: 0:00:00.238758\n",
            "Epoch: [94/100]\tSamples: [94000/100000]\tTrain Loss: 2082.293578125\tTime: 0:00:00.261351\n",
            "Epoch: [95/100]\tSamples: [95000/100000]\tTrain Loss: 2082.0506328125\tTime: 0:00:00.236800\n",
            "Epoch: [96/100]\tSamples: [96000/100000]\tTrain Loss: 2082.24646875\tTime: 0:00:00.240663\n",
            "Epoch: [97/100]\tSamples: [97000/100000]\tTrain Loss: 2082.3245078125\tTime: 0:00:00.254801\n",
            "Epoch: [98/100]\tSamples: [98000/100000]\tTrain Loss: 2081.859109375\tTime: 0:00:00.253524\n",
            "Epoch: [99/100]\tSamples: [99000/100000]\tTrain Loss: 2081.36734375\tTime: 0:00:00.256737\n",
            "Epoch: [100/100]\tSamples: [100000/100000]\tTrain Loss: 2082.18190625\tTime: 0:00:00.245669\n",
            "Settings: \n",
            "               N Components: 10\n",
            "               Topic Prior Mean: 0.0\n",
            "               Topic Prior Variance: 0.9\n",
            "               Model Type: prodLDA\n",
            "               Hidden Sizes: (100, 100)\n",
            "               Activation: softplus\n",
            "               Dropout: 0.2\n",
            "               Learn Priors: True\n",
            "               Learning Rate: 0.002\n",
            "               Momentum: 0.99\n",
            "               Reduce On Plateau: False\n",
            "               Save Dir: None\n",
            "Epoch: [1/100]\tSamples: [1000/100000]\tTrain Loss: 2136.277765625\tTime: 0:00:00.247697\n",
            "Epoch: [2/100]\tSamples: [2000/100000]\tTrain Loss: 2119.8247109375\tTime: 0:00:00.233325\n",
            "Epoch: [3/100]\tSamples: [3000/100000]\tTrain Loss: 2110.48646875\tTime: 0:00:00.245313\n",
            "Epoch: [4/100]\tSamples: [4000/100000]\tTrain Loss: 2103.1523046875\tTime: 0:00:00.246365\n",
            "Epoch: [5/100]\tSamples: [5000/100000]\tTrain Loss: 2097.475484375\tTime: 0:00:00.252058\n",
            "Epoch: [6/100]\tSamples: [6000/100000]\tTrain Loss: 2091.865515625\tTime: 0:00:00.241282\n",
            "Epoch: [7/100]\tSamples: [7000/100000]\tTrain Loss: 2087.78215625\tTime: 0:00:00.233578\n",
            "Epoch: [8/100]\tSamples: [8000/100000]\tTrain Loss: 2081.130015625\tTime: 0:00:00.244853\n",
            "Epoch: [9/100]\tSamples: [9000/100000]\tTrain Loss: 2075.7961875\tTime: 0:00:00.250098\n",
            "Epoch: [10/100]\tSamples: [10000/100000]\tTrain Loss: 2072.202375\tTime: 0:00:00.237711\n",
            "Epoch: [11/100]\tSamples: [11000/100000]\tTrain Loss: 2069.1734375\tTime: 0:00:00.237878\n",
            "Epoch: [12/100]\tSamples: [12000/100000]\tTrain Loss: 2065.9769140625\tTime: 0:00:00.247367\n",
            "Epoch: [13/100]\tSamples: [13000/100000]\tTrain Loss: 2060.8204765625\tTime: 0:00:00.248239\n",
            "Epoch: [14/100]\tSamples: [14000/100000]\tTrain Loss: 2056.6177265625\tTime: 0:00:00.260911\n",
            "Epoch: [15/100]\tSamples: [15000/100000]\tTrain Loss: 2053.8426640625\tTime: 0:00:00.238626\n",
            "Epoch: [16/100]\tSamples: [16000/100000]\tTrain Loss: 2048.426234375\tTime: 0:00:00.243254\n",
            "Epoch: [17/100]\tSamples: [17000/100000]\tTrain Loss: 2045.72925\tTime: 0:00:00.252829\n",
            "Epoch: [18/100]\tSamples: [18000/100000]\tTrain Loss: 2041.74084375\tTime: 0:00:00.251996\n",
            "Epoch: [19/100]\tSamples: [19000/100000]\tTrain Loss: 2038.4655625\tTime: 0:00:00.256256\n",
            "Epoch: [20/100]\tSamples: [20000/100000]\tTrain Loss: 2033.5415859375\tTime: 0:00:00.255520\n",
            "Epoch: [21/100]\tSamples: [21000/100000]\tTrain Loss: 2031.6331015625\tTime: 0:00:00.250200\n",
            "Epoch: [22/100]\tSamples: [22000/100000]\tTrain Loss: 2027.59334375\tTime: 0:00:00.250885\n",
            "Epoch: [23/100]\tSamples: [23000/100000]\tTrain Loss: 2025.3490546875\tTime: 0:00:00.248998\n",
            "Epoch: [24/100]\tSamples: [24000/100000]\tTrain Loss: 2022.6214375\tTime: 0:00:00.245978\n",
            "Epoch: [25/100]\tSamples: [25000/100000]\tTrain Loss: 2020.7026640625\tTime: 0:00:00.247354\n",
            "Epoch: [26/100]\tSamples: [26000/100000]\tTrain Loss: 2018.5424453125\tTime: 0:00:00.250962\n",
            "Epoch: [27/100]\tSamples: [27000/100000]\tTrain Loss: 2017.6228046875\tTime: 0:00:00.230785\n",
            "Epoch: [28/100]\tSamples: [28000/100000]\tTrain Loss: 2016.4884453125\tTime: 0:00:00.267171\n",
            "Epoch: [29/100]\tSamples: [29000/100000]\tTrain Loss: 2014.9371171875\tTime: 0:00:00.263279\n",
            "Epoch: [30/100]\tSamples: [30000/100000]\tTrain Loss: 2012.8220859375\tTime: 0:00:00.264591\n",
            "Epoch: [31/100]\tSamples: [31000/100000]\tTrain Loss: 2013.0018125\tTime: 0:00:00.249051\n",
            "Epoch: [32/100]\tSamples: [32000/100000]\tTrain Loss: 2012.474953125\tTime: 0:00:00.244536\n",
            "Epoch: [33/100]\tSamples: [33000/100000]\tTrain Loss: 2012.266296875\tTime: 0:00:00.246290\n",
            "Epoch: [34/100]\tSamples: [34000/100000]\tTrain Loss: 2011.048734375\tTime: 0:00:00.250148\n",
            "Epoch: [35/100]\tSamples: [35000/100000]\tTrain Loss: 2010.3953984375\tTime: 0:00:00.249812\n",
            "Epoch: [36/100]\tSamples: [36000/100000]\tTrain Loss: 2009.938890625\tTime: 0:00:00.246891\n",
            "Epoch: [37/100]\tSamples: [37000/100000]\tTrain Loss: 2009.75203125\tTime: 0:00:00.244448\n",
            "Epoch: [38/100]\tSamples: [38000/100000]\tTrain Loss: 2009.128984375\tTime: 0:00:00.253753\n",
            "Epoch: [39/100]\tSamples: [39000/100000]\tTrain Loss: 2008.6464453125\tTime: 0:00:00.245201\n",
            "Epoch: [40/100]\tSamples: [40000/100000]\tTrain Loss: 2007.9580546875\tTime: 0:00:00.236488\n",
            "Epoch: [41/100]\tSamples: [41000/100000]\tTrain Loss: 2007.8146640625\tTime: 0:00:00.239188\n",
            "Epoch: [42/100]\tSamples: [42000/100000]\tTrain Loss: 2007.1890234375\tTime: 0:00:00.238521\n",
            "Epoch: [43/100]\tSamples: [43000/100000]\tTrain Loss: 2006.882265625\tTime: 0:00:00.246767\n",
            "Epoch: [44/100]\tSamples: [44000/100000]\tTrain Loss: 2006.823359375\tTime: 0:00:00.260469\n",
            "Epoch: [45/100]\tSamples: [45000/100000]\tTrain Loss: 2006.23128125\tTime: 0:00:00.261688\n",
            "Epoch: [46/100]\tSamples: [46000/100000]\tTrain Loss: 2006.15903125\tTime: 0:00:00.268908\n",
            "Epoch: [47/100]\tSamples: [47000/100000]\tTrain Loss: 2005.6297734375\tTime: 0:00:00.244908\n",
            "Epoch: [48/100]\tSamples: [48000/100000]\tTrain Loss: 2005.7176953125\tTime: 0:00:00.238486\n",
            "Epoch: [49/100]\tSamples: [49000/100000]\tTrain Loss: 2005.517953125\tTime: 0:00:00.247129\n",
            "Epoch: [50/100]\tSamples: [50000/100000]\tTrain Loss: 2004.49453125\tTime: 0:00:00.240111\n",
            "Epoch: [51/100]\tSamples: [51000/100000]\tTrain Loss: 2005.1170546875\tTime: 0:00:00.257968\n",
            "Epoch: [52/100]\tSamples: [52000/100000]\tTrain Loss: 2004.5295390625\tTime: 0:00:00.254501\n",
            "Epoch: [53/100]\tSamples: [53000/100000]\tTrain Loss: 2004.4916796875\tTime: 0:00:00.247403\n",
            "Epoch: [54/100]\tSamples: [54000/100000]\tTrain Loss: 2003.900421875\tTime: 0:00:00.245340\n",
            "Epoch: [55/100]\tSamples: [55000/100000]\tTrain Loss: 2004.328234375\tTime: 0:00:00.252315\n",
            "Epoch: [56/100]\tSamples: [56000/100000]\tTrain Loss: 2003.62275\tTime: 0:00:00.248391\n",
            "Epoch: [57/100]\tSamples: [57000/100000]\tTrain Loss: 2003.0\tTime: 0:00:00.257640\n",
            "Epoch: [58/100]\tSamples: [58000/100000]\tTrain Loss: 2002.7268828125\tTime: 0:00:00.242420\n",
            "Epoch: [59/100]\tSamples: [59000/100000]\tTrain Loss: 2002.3295703125\tTime: 0:00:00.249850\n",
            "Epoch: [60/100]\tSamples: [60000/100000]\tTrain Loss: 2002.7087578125\tTime: 0:00:00.249590\n",
            "Epoch: [61/100]\tSamples: [61000/100000]\tTrain Loss: 2002.40621875\tTime: 0:00:00.256742\n",
            "Epoch: [62/100]\tSamples: [62000/100000]\tTrain Loss: 2002.9183984375\tTime: 0:00:00.251039\n",
            "Epoch: [63/100]\tSamples: [63000/100000]\tTrain Loss: 2002.5182265625\tTime: 0:00:00.256271\n",
            "Epoch: [64/100]\tSamples: [64000/100000]\tTrain Loss: 2002.410953125\tTime: 0:00:00.254769\n",
            "Epoch: [65/100]\tSamples: [65000/100000]\tTrain Loss: 2001.194859375\tTime: 0:00:00.246736\n",
            "Epoch: [66/100]\tSamples: [66000/100000]\tTrain Loss: 2002.255203125\tTime: 0:00:00.240315\n",
            "Epoch: [67/100]\tSamples: [67000/100000]\tTrain Loss: 2000.9313125\tTime: 0:00:00.264567\n",
            "Epoch: [68/100]\tSamples: [68000/100000]\tTrain Loss: 2002.1892265625\tTime: 0:00:00.255698\n",
            "Epoch: [69/100]\tSamples: [69000/100000]\tTrain Loss: 2000.8872421875\tTime: 0:00:00.242689\n",
            "Epoch: [70/100]\tSamples: [70000/100000]\tTrain Loss: 2000.9131484375\tTime: 0:00:00.241902\n",
            "Epoch: [71/100]\tSamples: [71000/100000]\tTrain Loss: 2001.436265625\tTime: 0:00:00.258058\n",
            "Epoch: [72/100]\tSamples: [72000/100000]\tTrain Loss: 2000.6108828125\tTime: 0:00:00.246063\n",
            "Epoch: [73/100]\tSamples: [73000/100000]\tTrain Loss: 2001.3740078125\tTime: 0:00:00.253134\n",
            "Epoch: [74/100]\tSamples: [74000/100000]\tTrain Loss: 2001.75228125\tTime: 0:00:00.246614\n",
            "Epoch: [75/100]\tSamples: [75000/100000]\tTrain Loss: 2001.3022265625\tTime: 0:00:00.278796\n",
            "Epoch: [76/100]\tSamples: [76000/100000]\tTrain Loss: 2000.7469609375\tTime: 0:00:00.265075\n",
            "Epoch: [77/100]\tSamples: [77000/100000]\tTrain Loss: 2000.7534921875\tTime: 0:00:00.252792\n",
            "Epoch: [78/100]\tSamples: [78000/100000]\tTrain Loss: 2001.25665625\tTime: 0:00:00.253116\n",
            "Epoch: [79/100]\tSamples: [79000/100000]\tTrain Loss: 2000.9210625\tTime: 0:00:00.271279\n",
            "Epoch: [80/100]\tSamples: [80000/100000]\tTrain Loss: 2000.4620703125\tTime: 0:00:00.259300\n",
            "Epoch: [81/100]\tSamples: [81000/100000]\tTrain Loss: 2000.663515625\tTime: 0:00:00.246027\n",
            "Epoch: [82/100]\tSamples: [82000/100000]\tTrain Loss: 2000.6152109375\tTime: 0:00:00.241618\n",
            "Epoch: [83/100]\tSamples: [83000/100000]\tTrain Loss: 2000.913125\tTime: 0:00:00.260345\n",
            "Epoch: [84/100]\tSamples: [84000/100000]\tTrain Loss: 2000.0567890625\tTime: 0:00:00.256182\n",
            "Epoch: [85/100]\tSamples: [85000/100000]\tTrain Loss: 2000.723609375\tTime: 0:00:00.241384\n",
            "Epoch: [86/100]\tSamples: [86000/100000]\tTrain Loss: 2000.3565625\tTime: 0:00:00.236205\n",
            "Epoch: [87/100]\tSamples: [87000/100000]\tTrain Loss: 2000.7267421875\tTime: 0:00:00.259695\n",
            "Epoch: [88/100]\tSamples: [88000/100000]\tTrain Loss: 2000.168375\tTime: 0:00:00.241831\n",
            "Epoch: [89/100]\tSamples: [89000/100000]\tTrain Loss: 2000.6833828125\tTime: 0:00:00.232010\n",
            "Epoch: [90/100]\tSamples: [90000/100000]\tTrain Loss: 1999.9576328125\tTime: 0:00:00.246093\n",
            "Epoch: [91/100]\tSamples: [91000/100000]\tTrain Loss: 2000.262109375\tTime: 0:00:00.255331\n",
            "Epoch: [92/100]\tSamples: [92000/100000]\tTrain Loss: 2000.0502734375\tTime: 0:00:00.262896\n",
            "Epoch: [93/100]\tSamples: [93000/100000]\tTrain Loss: 1999.9680234375\tTime: 0:00:00.253518\n",
            "Epoch: [94/100]\tSamples: [94000/100000]\tTrain Loss: 1999.734453125\tTime: 0:00:00.252727\n",
            "Epoch: [95/100]\tSamples: [95000/100000]\tTrain Loss: 2000.796078125\tTime: 0:00:00.260864\n",
            "Epoch: [96/100]\tSamples: [96000/100000]\tTrain Loss: 2000.3301640625\tTime: 0:00:00.250281\n",
            "Epoch: [97/100]\tSamples: [97000/100000]\tTrain Loss: 2000.5334140625\tTime: 0:00:00.250317\n",
            "Epoch: [98/100]\tSamples: [98000/100000]\tTrain Loss: 2000.4965078125\tTime: 0:00:00.249272\n",
            "Epoch: [99/100]\tSamples: [99000/100000]\tTrain Loss: 2000.4136640625\tTime: 0:00:00.264937\n",
            "Epoch: [100/100]\tSamples: [100000/100000]\tTrain Loss: 1999.3554140625\tTime: 0:00:00.252794\n",
            "Settings: \n",
            "               N Components: 10\n",
            "               Topic Prior Mean: 0.0\n",
            "               Topic Prior Variance: 0.9\n",
            "               Model Type: prodLDA\n",
            "               Hidden Sizes: (100, 100)\n",
            "               Activation: softplus\n",
            "               Dropout: 0.2\n",
            "               Learn Priors: True\n",
            "               Learning Rate: 0.002\n",
            "               Momentum: 0.99\n",
            "               Reduce On Plateau: False\n",
            "               Save Dir: None\n",
            "Epoch: [1/100]\tSamples: [1000/100000]\tTrain Loss: 2192.89278125\tTime: 0:00:00.237251\n",
            "Epoch: [2/100]\tSamples: [2000/100000]\tTrain Loss: 2179.593390625\tTime: 0:00:00.264635\n",
            "Epoch: [3/100]\tSamples: [3000/100000]\tTrain Loss: 2168.5125546875\tTime: 0:00:00.242534\n",
            "Epoch: [4/100]\tSamples: [4000/100000]\tTrain Loss: 2162.7146015625\tTime: 0:00:00.234407\n",
            "Epoch: [5/100]\tSamples: [5000/100000]\tTrain Loss: 2154.455296875\tTime: 0:00:00.235714\n",
            "Epoch: [6/100]\tSamples: [6000/100000]\tTrain Loss: 2149.3229765625\tTime: 0:00:00.249304\n",
            "Epoch: [7/100]\tSamples: [7000/100000]\tTrain Loss: 2143.296109375\tTime: 0:00:00.271240\n",
            "Epoch: [8/100]\tSamples: [8000/100000]\tTrain Loss: 2138.7211953125\tTime: 0:00:00.243623\n",
            "Epoch: [9/100]\tSamples: [9000/100000]\tTrain Loss: 2132.965390625\tTime: 0:00:00.257641\n",
            "Epoch: [10/100]\tSamples: [10000/100000]\tTrain Loss: 2126.9429296875\tTime: 0:00:00.241979\n",
            "Epoch: [11/100]\tSamples: [11000/100000]\tTrain Loss: 2122.5579375\tTime: 0:00:00.260898\n",
            "Epoch: [12/100]\tSamples: [12000/100000]\tTrain Loss: 2118.7214140625\tTime: 0:00:00.235703\n",
            "Epoch: [13/100]\tSamples: [13000/100000]\tTrain Loss: 2113.312203125\tTime: 0:00:00.239806\n",
            "Epoch: [14/100]\tSamples: [14000/100000]\tTrain Loss: 2110.2621953125\tTime: 0:00:00.239048\n",
            "Epoch: [15/100]\tSamples: [15000/100000]\tTrain Loss: 2105.172234375\tTime: 0:00:00.292768\n",
            "Epoch: [16/100]\tSamples: [16000/100000]\tTrain Loss: 2100.3598359375\tTime: 0:00:00.243973\n",
            "Epoch: [17/100]\tSamples: [17000/100000]\tTrain Loss: 2096.9962890625\tTime: 0:00:00.247951\n",
            "Epoch: [18/100]\tSamples: [18000/100000]\tTrain Loss: 2095.518140625\tTime: 0:00:00.244707\n",
            "Epoch: [19/100]\tSamples: [19000/100000]\tTrain Loss: 2090.82921875\tTime: 0:00:00.262272\n",
            "Epoch: [20/100]\tSamples: [20000/100000]\tTrain Loss: 2088.863859375\tTime: 0:00:00.240743\n",
            "Epoch: [21/100]\tSamples: [21000/100000]\tTrain Loss: 2085.2444765625\tTime: 0:00:00.244226\n",
            "Epoch: [22/100]\tSamples: [22000/100000]\tTrain Loss: 2082.4651640625\tTime: 0:00:00.245196\n",
            "Epoch: [23/100]\tSamples: [23000/100000]\tTrain Loss: 2081.098921875\tTime: 0:00:00.262647\n",
            "Epoch: [24/100]\tSamples: [24000/100000]\tTrain Loss: 2079.164078125\tTime: 0:00:00.240433\n",
            "Epoch: [25/100]\tSamples: [25000/100000]\tTrain Loss: 2076.453984375\tTime: 0:00:00.249624\n",
            "Epoch: [26/100]\tSamples: [26000/100000]\tTrain Loss: 2074.3644921875\tTime: 0:00:00.239637\n",
            "Epoch: [27/100]\tSamples: [27000/100000]\tTrain Loss: 2071.3473671875\tTime: 0:00:00.258781\n",
            "Epoch: [28/100]\tSamples: [28000/100000]\tTrain Loss: 2071.4211640625\tTime: 0:00:00.242116\n",
            "Epoch: [29/100]\tSamples: [29000/100000]\tTrain Loss: 2069.8548125\tTime: 0:00:00.232237\n",
            "Epoch: [30/100]\tSamples: [30000/100000]\tTrain Loss: 2068.5425\tTime: 0:00:00.245720\n",
            "Epoch: [31/100]\tSamples: [31000/100000]\tTrain Loss: 2067.63534375\tTime: 0:00:00.263499\n",
            "Epoch: [32/100]\tSamples: [32000/100000]\tTrain Loss: 2065.9929921875\tTime: 0:00:00.234492\n",
            "Epoch: [33/100]\tSamples: [33000/100000]\tTrain Loss: 2065.8963828125\tTime: 0:00:00.253605\n",
            "Epoch: [34/100]\tSamples: [34000/100000]\tTrain Loss: 2064.798109375\tTime: 0:00:00.246761\n",
            "Epoch: [35/100]\tSamples: [35000/100000]\tTrain Loss: 2063.8716953125\tTime: 0:00:00.245335\n",
            "Epoch: [36/100]\tSamples: [36000/100000]\tTrain Loss: 2062.8060234375\tTime: 0:00:00.254445\n",
            "Epoch: [37/100]\tSamples: [37000/100000]\tTrain Loss: 2062.2118671875\tTime: 0:00:00.237674\n",
            "Epoch: [38/100]\tSamples: [38000/100000]\tTrain Loss: 2062.5729453125\tTime: 0:00:00.252675\n",
            "Epoch: [39/100]\tSamples: [39000/100000]\tTrain Loss: 2061.473265625\tTime: 0:00:00.261440\n",
            "Epoch: [40/100]\tSamples: [40000/100000]\tTrain Loss: 2061.3250859375\tTime: 0:00:00.250475\n",
            "Epoch: [41/100]\tSamples: [41000/100000]\tTrain Loss: 2061.289734375\tTime: 0:00:00.244342\n",
            "Epoch: [42/100]\tSamples: [42000/100000]\tTrain Loss: 2060.5431640625\tTime: 0:00:00.250817\n",
            "Epoch: [43/100]\tSamples: [43000/100000]\tTrain Loss: 2060.7409609375\tTime: 0:00:00.248301\n",
            "Epoch: [44/100]\tSamples: [44000/100000]\tTrain Loss: 2059.9416484375\tTime: 0:00:00.261294\n",
            "Epoch: [45/100]\tSamples: [45000/100000]\tTrain Loss: 2060.459703125\tTime: 0:00:00.239506\n",
            "Epoch: [46/100]\tSamples: [46000/100000]\tTrain Loss: 2059.6048203125\tTime: 0:00:00.241631\n",
            "Epoch: [47/100]\tSamples: [47000/100000]\tTrain Loss: 2058.7492578125\tTime: 0:00:00.272587\n",
            "Epoch: [48/100]\tSamples: [48000/100000]\tTrain Loss: 2059.029734375\tTime: 0:00:00.256100\n",
            "Epoch: [49/100]\tSamples: [49000/100000]\tTrain Loss: 2058.6579140625\tTime: 0:00:00.238138\n",
            "Epoch: [50/100]\tSamples: [50000/100000]\tTrain Loss: 2058.55453125\tTime: 0:00:00.247545\n",
            "Epoch: [51/100]\tSamples: [51000/100000]\tTrain Loss: 2058.6855703125\tTime: 0:00:00.238611\n",
            "Epoch: [52/100]\tSamples: [52000/100000]\tTrain Loss: 2057.7066484375\tTime: 0:00:00.250596\n",
            "Epoch: [53/100]\tSamples: [53000/100000]\tTrain Loss: 2058.17725\tTime: 0:00:00.253930\n",
            "Epoch: [54/100]\tSamples: [54000/100000]\tTrain Loss: 2058.0186015625\tTime: 0:00:00.245651\n",
            "Epoch: [55/100]\tSamples: [55000/100000]\tTrain Loss: 2058.2168671875\tTime: 0:00:00.253264\n",
            "Epoch: [56/100]\tSamples: [56000/100000]\tTrain Loss: 2057.5415390625\tTime: 0:00:00.271246\n",
            "Epoch: [57/100]\tSamples: [57000/100000]\tTrain Loss: 2057.449125\tTime: 0:00:00.251588\n",
            "Epoch: [58/100]\tSamples: [58000/100000]\tTrain Loss: 2057.6343046875\tTime: 0:00:00.256602\n",
            "Epoch: [59/100]\tSamples: [59000/100000]\tTrain Loss: 2056.9767109375\tTime: 0:00:00.255255\n",
            "Epoch: [60/100]\tSamples: [60000/100000]\tTrain Loss: 2057.1745859375\tTime: 0:00:00.252431\n",
            "Epoch: [61/100]\tSamples: [61000/100000]\tTrain Loss: 2057.100375\tTime: 0:00:00.246612\n",
            "Epoch: [62/100]\tSamples: [62000/100000]\tTrain Loss: 2056.7397578125\tTime: 0:00:00.243272\n",
            "Epoch: [63/100]\tSamples: [63000/100000]\tTrain Loss: 2056.8138203125\tTime: 0:00:00.254102\n",
            "Epoch: [64/100]\tSamples: [64000/100000]\tTrain Loss: 2057.1764609375\tTime: 0:00:00.248759\n",
            "Epoch: [65/100]\tSamples: [65000/100000]\tTrain Loss: 2056.6729453125\tTime: 0:00:00.237797\n",
            "Epoch: [66/100]\tSamples: [66000/100000]\tTrain Loss: 2056.5268515625\tTime: 0:00:00.244208\n",
            "Epoch: [67/100]\tSamples: [67000/100000]\tTrain Loss: 2056.1573515625\tTime: 0:00:00.247263\n",
            "Epoch: [68/100]\tSamples: [68000/100000]\tTrain Loss: 2056.3108046875\tTime: 0:00:00.253398\n",
            "Epoch: [69/100]\tSamples: [69000/100000]\tTrain Loss: 2056.2445390625\tTime: 0:00:00.244931\n",
            "Epoch: [70/100]\tSamples: [70000/100000]\tTrain Loss: 2057.527359375\tTime: 0:00:00.257727\n",
            "Epoch: [71/100]\tSamples: [71000/100000]\tTrain Loss: 2056.423125\tTime: 0:00:00.254801\n",
            "Epoch: [72/100]\tSamples: [72000/100000]\tTrain Loss: 2056.243828125\tTime: 0:00:00.262195\n",
            "Epoch: [73/100]\tSamples: [73000/100000]\tTrain Loss: 2055.6058359375\tTime: 0:00:00.245639\n",
            "Epoch: [74/100]\tSamples: [74000/100000]\tTrain Loss: 2056.468296875\tTime: 0:00:00.268388\n",
            "Epoch: [75/100]\tSamples: [75000/100000]\tTrain Loss: 2056.281375\tTime: 0:00:00.258556\n",
            "Epoch: [76/100]\tSamples: [76000/100000]\tTrain Loss: 2056.829078125\tTime: 0:00:00.262194\n",
            "Epoch: [77/100]\tSamples: [77000/100000]\tTrain Loss: 2055.3499375\tTime: 0:00:00.229608\n",
            "Epoch: [78/100]\tSamples: [78000/100000]\tTrain Loss: 2056.38359375\tTime: 0:00:00.239879\n",
            "Epoch: [79/100]\tSamples: [79000/100000]\tTrain Loss: 2055.852578125\tTime: 0:00:00.261462\n",
            "Epoch: [80/100]\tSamples: [80000/100000]\tTrain Loss: 2055.3147890625\tTime: 0:00:00.243714\n",
            "Epoch: [81/100]\tSamples: [81000/100000]\tTrain Loss: 2055.8687734375\tTime: 0:00:00.268029\n",
            "Epoch: [82/100]\tSamples: [82000/100000]\tTrain Loss: 2055.5740546875\tTime: 0:00:00.246742\n",
            "Epoch: [83/100]\tSamples: [83000/100000]\tTrain Loss: 2055.7678046875\tTime: 0:00:00.268467\n",
            "Epoch: [84/100]\tSamples: [84000/100000]\tTrain Loss: 2055.448578125\tTime: 0:00:00.245773\n",
            "Epoch: [85/100]\tSamples: [85000/100000]\tTrain Loss: 2054.9338203125\tTime: 0:00:00.240629\n",
            "Epoch: [86/100]\tSamples: [86000/100000]\tTrain Loss: 2055.4094375\tTime: 0:00:00.246157\n",
            "Epoch: [87/100]\tSamples: [87000/100000]\tTrain Loss: 2055.6585234375\tTime: 0:00:00.268123\n",
            "Epoch: [88/100]\tSamples: [88000/100000]\tTrain Loss: 2055.6891875\tTime: 0:00:00.235454\n",
            "Epoch: [89/100]\tSamples: [89000/100000]\tTrain Loss: 2055.5137109375\tTime: 0:00:00.258075\n",
            "Epoch: [90/100]\tSamples: [90000/100000]\tTrain Loss: 2055.169421875\tTime: 0:00:00.259088\n",
            "Epoch: [91/100]\tSamples: [91000/100000]\tTrain Loss: 2055.1381796875\tTime: 0:00:00.248748\n",
            "Epoch: [92/100]\tSamples: [92000/100000]\tTrain Loss: 2056.011265625\tTime: 0:00:00.252786\n",
            "Epoch: [93/100]\tSamples: [93000/100000]\tTrain Loss: 2055.16\tTime: 0:00:00.271270\n",
            "Epoch: [94/100]\tSamples: [94000/100000]\tTrain Loss: 2055.26165625\tTime: 0:00:00.248074\n",
            "Epoch: [95/100]\tSamples: [95000/100000]\tTrain Loss: 2055.3676015625\tTime: 0:00:00.245292\n",
            "Epoch: [96/100]\tSamples: [96000/100000]\tTrain Loss: 2055.915484375\tTime: 0:00:00.241068\n",
            "Epoch: [97/100]\tSamples: [97000/100000]\tTrain Loss: 2055.4868515625\tTime: 0:00:00.263974\n",
            "Epoch: [98/100]\tSamples: [98000/100000]\tTrain Loss: 2054.810796875\tTime: 0:00:00.246049\n",
            "Epoch: [99/100]\tSamples: [99000/100000]\tTrain Loss: 2055.981\tTime: 0:00:00.248495\n",
            "Epoch: [100/100]\tSamples: [100000/100000]\tTrain Loss: 2054.6928515625\tTime: 0:00:00.248094\n",
            "Settings: \n",
            "               N Components: 10\n",
            "               Topic Prior Mean: 0.0\n",
            "               Topic Prior Variance: 0.9\n",
            "               Model Type: prodLDA\n",
            "               Hidden Sizes: (100, 100)\n",
            "               Activation: softplus\n",
            "               Dropout: 0.2\n",
            "               Learn Priors: True\n",
            "               Learning Rate: 0.002\n",
            "               Momentum: 0.99\n",
            "               Reduce On Plateau: False\n",
            "               Save Dir: None\n",
            "Epoch: [1/100]\tSamples: [1000/100000]\tTrain Loss: 2231.783515625\tTime: 0:00:00.248011\n",
            "Epoch: [2/100]\tSamples: [2000/100000]\tTrain Loss: 2216.0904765625\tTime: 0:00:00.246263\n",
            "Epoch: [3/100]\tSamples: [3000/100000]\tTrain Loss: 2203.83353125\tTime: 0:00:00.242967\n",
            "Epoch: [4/100]\tSamples: [4000/100000]\tTrain Loss: 2196.4250546875\tTime: 0:00:00.262761\n",
            "Epoch: [5/100]\tSamples: [5000/100000]\tTrain Loss: 2191.6525390625\tTime: 0:00:00.241196\n",
            "Epoch: [6/100]\tSamples: [6000/100000]\tTrain Loss: 2183.2236640625\tTime: 0:00:00.245853\n",
            "Epoch: [7/100]\tSamples: [7000/100000]\tTrain Loss: 2178.676515625\tTime: 0:00:00.243341\n",
            "Epoch: [8/100]\tSamples: [8000/100000]\tTrain Loss: 2174.826125\tTime: 0:00:00.271224\n",
            "Epoch: [9/100]\tSamples: [9000/100000]\tTrain Loss: 2168.843890625\tTime: 0:00:00.243509\n",
            "Epoch: [10/100]\tSamples: [10000/100000]\tTrain Loss: 2163.688890625\tTime: 0:00:00.254054\n",
            "Epoch: [11/100]\tSamples: [11000/100000]\tTrain Loss: 2159.008484375\tTime: 0:00:00.242451\n",
            "Epoch: [12/100]\tSamples: [12000/100000]\tTrain Loss: 2154.2790546875\tTime: 0:00:00.252680\n",
            "Epoch: [13/100]\tSamples: [13000/100000]\tTrain Loss: 2149.082796875\tTime: 0:00:00.259659\n",
            "Epoch: [14/100]\tSamples: [14000/100000]\tTrain Loss: 2146.448109375\tTime: 0:00:00.255291\n",
            "Epoch: [15/100]\tSamples: [15000/100000]\tTrain Loss: 2141.667328125\tTime: 0:00:00.241278\n",
            "Epoch: [16/100]\tSamples: [16000/100000]\tTrain Loss: 2137.4464765625\tTime: 0:00:00.267978\n",
            "Epoch: [17/100]\tSamples: [17000/100000]\tTrain Loss: 2132.972515625\tTime: 0:00:00.239237\n",
            "Epoch: [18/100]\tSamples: [18000/100000]\tTrain Loss: 2130.439046875\tTime: 0:00:00.254074\n",
            "Epoch: [19/100]\tSamples: [19000/100000]\tTrain Loss: 2125.7005390625\tTime: 0:00:00.246915\n",
            "Epoch: [20/100]\tSamples: [20000/100000]\tTrain Loss: 2124.0279921875\tTime: 0:00:00.270171\n",
            "Epoch: [21/100]\tSamples: [21000/100000]\tTrain Loss: 2119.740296875\tTime: 0:00:00.245674\n",
            "Epoch: [22/100]\tSamples: [22000/100000]\tTrain Loss: 2117.6057890625\tTime: 0:00:00.237484\n",
            "Epoch: [23/100]\tSamples: [23000/100000]\tTrain Loss: 2114.9892578125\tTime: 0:00:00.243771\n",
            "Epoch: [24/100]\tSamples: [24000/100000]\tTrain Loss: 2112.5155703125\tTime: 0:00:00.265482\n",
            "Epoch: [25/100]\tSamples: [25000/100000]\tTrain Loss: 2112.2911328125\tTime: 0:00:00.245359\n",
            "Epoch: [26/100]\tSamples: [26000/100000]\tTrain Loss: 2109.4802890625\tTime: 0:00:00.246967\n",
            "Epoch: [27/100]\tSamples: [27000/100000]\tTrain Loss: 2107.990359375\tTime: 0:00:00.235934\n",
            "Epoch: [28/100]\tSamples: [28000/100000]\tTrain Loss: 2106.327046875\tTime: 0:00:00.240199\n",
            "Epoch: [29/100]\tSamples: [29000/100000]\tTrain Loss: 2105.4615\tTime: 0:00:00.258760\n",
            "Epoch: [30/100]\tSamples: [30000/100000]\tTrain Loss: 2105.2973515625\tTime: 0:00:00.248063\n",
            "Epoch: [31/100]\tSamples: [31000/100000]\tTrain Loss: 2103.326234375\tTime: 0:00:00.245540\n",
            "Epoch: [32/100]\tSamples: [32000/100000]\tTrain Loss: 2102.8962734375\tTime: 0:00:00.243293\n",
            "Epoch: [33/100]\tSamples: [33000/100000]\tTrain Loss: 2101.7835\tTime: 0:00:00.252816\n",
            "Epoch: [34/100]\tSamples: [34000/100000]\tTrain Loss: 2102.0797421875\tTime: 0:00:00.251560\n",
            "Epoch: [35/100]\tSamples: [35000/100000]\tTrain Loss: 2101.64559375\tTime: 0:00:00.252130\n",
            "Epoch: [36/100]\tSamples: [36000/100000]\tTrain Loss: 2100.799984375\tTime: 0:00:00.255684\n",
            "Epoch: [37/100]\tSamples: [37000/100000]\tTrain Loss: 2099.813546875\tTime: 0:00:00.261348\n",
            "Epoch: [38/100]\tSamples: [38000/100000]\tTrain Loss: 2099.64784375\tTime: 0:00:00.233293\n",
            "Epoch: [39/100]\tSamples: [39000/100000]\tTrain Loss: 2099.1777734375\tTime: 0:00:00.238682\n",
            "Epoch: [40/100]\tSamples: [40000/100000]\tTrain Loss: 2098.33328125\tTime: 0:00:00.242347\n",
            "Epoch: [41/100]\tSamples: [41000/100000]\tTrain Loss: 2098.1438125\tTime: 0:00:00.266452\n",
            "Epoch: [42/100]\tSamples: [42000/100000]\tTrain Loss: 2097.87875\tTime: 0:00:00.257834\n",
            "Epoch: [43/100]\tSamples: [43000/100000]\tTrain Loss: 2097.75459375\tTime: 0:00:00.247914\n",
            "Epoch: [44/100]\tSamples: [44000/100000]\tTrain Loss: 2097.0843359375\tTime: 0:00:00.239163\n",
            "Epoch: [45/100]\tSamples: [45000/100000]\tTrain Loss: 2097.75884375\tTime: 0:00:00.269244\n",
            "Epoch: [46/100]\tSamples: [46000/100000]\tTrain Loss: 2097.0043046875\tTime: 0:00:00.239768\n",
            "Epoch: [47/100]\tSamples: [47000/100000]\tTrain Loss: 2096.47409375\tTime: 0:00:00.243908\n",
            "Epoch: [48/100]\tSamples: [48000/100000]\tTrain Loss: 2096.6005703125\tTime: 0:00:00.253254\n",
            "Epoch: [49/100]\tSamples: [49000/100000]\tTrain Loss: 2096.2704921875\tTime: 0:00:00.253149\n",
            "Epoch: [50/100]\tSamples: [50000/100000]\tTrain Loss: 2096.026015625\tTime: 0:00:00.261448\n",
            "Epoch: [51/100]\tSamples: [51000/100000]\tTrain Loss: 2094.983171875\tTime: 0:00:00.251674\n",
            "Epoch: [52/100]\tSamples: [52000/100000]\tTrain Loss: 2095.9554921875\tTime: 0:00:00.253371\n",
            "Epoch: [53/100]\tSamples: [53000/100000]\tTrain Loss: 2095.406546875\tTime: 0:00:00.251220\n",
            "Epoch: [54/100]\tSamples: [54000/100000]\tTrain Loss: 2094.825875\tTime: 0:00:00.236310\n",
            "Epoch: [55/100]\tSamples: [55000/100000]\tTrain Loss: 2094.50875\tTime: 0:00:00.247247\n",
            "Epoch: [56/100]\tSamples: [56000/100000]\tTrain Loss: 2094.50534375\tTime: 0:00:00.240357\n",
            "Epoch: [57/100]\tSamples: [57000/100000]\tTrain Loss: 2095.1972421875\tTime: 0:00:00.261636\n",
            "Epoch: [58/100]\tSamples: [58000/100000]\tTrain Loss: 2094.4638984375\tTime: 0:00:00.257263\n",
            "Epoch: [59/100]\tSamples: [59000/100000]\tTrain Loss: 2094.4432734375\tTime: 0:00:00.242211\n",
            "Epoch: [60/100]\tSamples: [60000/100000]\tTrain Loss: 2094.497640625\tTime: 0:00:00.256515\n",
            "Epoch: [61/100]\tSamples: [61000/100000]\tTrain Loss: 2094.48353125\tTime: 0:00:00.250642\n",
            "Epoch: [62/100]\tSamples: [62000/100000]\tTrain Loss: 2094.5304765625\tTime: 0:00:00.238947\n",
            "Epoch: [63/100]\tSamples: [63000/100000]\tTrain Loss: 2094.1921328125\tTime: 0:00:00.254762\n",
            "Epoch: [64/100]\tSamples: [64000/100000]\tTrain Loss: 2094.1195703125\tTime: 0:00:00.253351\n",
            "Epoch: [65/100]\tSamples: [65000/100000]\tTrain Loss: 2093.8136953125\tTime: 0:00:00.260383\n",
            "Epoch: [66/100]\tSamples: [66000/100000]\tTrain Loss: 2094.286609375\tTime: 0:00:00.256476\n",
            "Epoch: [67/100]\tSamples: [67000/100000]\tTrain Loss: 2093.1476875\tTime: 0:00:00.244544\n",
            "Epoch: [68/100]\tSamples: [68000/100000]\tTrain Loss: 2093.62\tTime: 0:00:00.264229\n",
            "Epoch: [69/100]\tSamples: [69000/100000]\tTrain Loss: 2093.3299296875\tTime: 0:00:00.257809\n",
            "Epoch: [70/100]\tSamples: [70000/100000]\tTrain Loss: 2093.9031953125\tTime: 0:00:00.235838\n",
            "Epoch: [71/100]\tSamples: [71000/100000]\tTrain Loss: 2093.3916953125\tTime: 0:00:00.244971\n",
            "Epoch: [72/100]\tSamples: [72000/100000]\tTrain Loss: 2093.331796875\tTime: 0:00:00.246710\n",
            "Epoch: [73/100]\tSamples: [73000/100000]\tTrain Loss: 2093.040921875\tTime: 0:00:00.259574\n",
            "Epoch: [74/100]\tSamples: [74000/100000]\tTrain Loss: 2093.8892109375\tTime: 0:00:00.267021\n",
            "Epoch: [75/100]\tSamples: [75000/100000]\tTrain Loss: 2093.3470234375\tTime: 0:00:00.243205\n",
            "Epoch: [76/100]\tSamples: [76000/100000]\tTrain Loss: 2093.096046875\tTime: 0:00:00.259627\n",
            "Epoch: [77/100]\tSamples: [77000/100000]\tTrain Loss: 2093.2253046875\tTime: 0:00:00.257009\n",
            "Epoch: [78/100]\tSamples: [78000/100000]\tTrain Loss: 2092.4777421875\tTime: 0:00:00.261079\n",
            "Epoch: [79/100]\tSamples: [79000/100000]\tTrain Loss: 2092.66025\tTime: 0:00:00.247999\n",
            "Epoch: [80/100]\tSamples: [80000/100000]\tTrain Loss: 2092.9799453125\tTime: 0:00:00.250036\n",
            "Epoch: [81/100]\tSamples: [81000/100000]\tTrain Loss: 2092.256984375\tTime: 0:00:00.236083\n",
            "Epoch: [82/100]\tSamples: [82000/100000]\tTrain Loss: 2093.2427265625\tTime: 0:00:00.259014\n",
            "Epoch: [83/100]\tSamples: [83000/100000]\tTrain Loss: 2092.188484375\tTime: 0:00:00.236345\n",
            "Epoch: [84/100]\tSamples: [84000/100000]\tTrain Loss: 2092.8185703125\tTime: 0:00:00.256132\n",
            "Epoch: [85/100]\tSamples: [85000/100000]\tTrain Loss: 2092.0438984375\tTime: 0:00:00.247804\n",
            "Epoch: [86/100]\tSamples: [86000/100000]\tTrain Loss: 2091.9649375\tTime: 0:00:00.263489\n",
            "Epoch: [87/100]\tSamples: [87000/100000]\tTrain Loss: 2092.239609375\tTime: 0:00:00.250546\n",
            "Epoch: [88/100]\tSamples: [88000/100000]\tTrain Loss: 2092.0124765625\tTime: 0:00:00.240786\n",
            "Epoch: [89/100]\tSamples: [89000/100000]\tTrain Loss: 2093.1680078125\tTime: 0:00:00.247346\n",
            "Epoch: [90/100]\tSamples: [90000/100000]\tTrain Loss: 2092.243875\tTime: 0:00:00.258285\n",
            "Epoch: [91/100]\tSamples: [91000/100000]\tTrain Loss: 2091.8333125\tTime: 0:00:00.236605\n",
            "Epoch: [92/100]\tSamples: [92000/100000]\tTrain Loss: 2090.9956328125\tTime: 0:00:00.246055\n",
            "Epoch: [93/100]\tSamples: [93000/100000]\tTrain Loss: 2091.5375625\tTime: 0:00:00.245665\n",
            "Epoch: [94/100]\tSamples: [94000/100000]\tTrain Loss: 2092.194234375\tTime: 0:00:00.250242\n",
            "Epoch: [95/100]\tSamples: [95000/100000]\tTrain Loss: 2091.0960390625\tTime: 0:00:00.243844\n",
            "Epoch: [96/100]\tSamples: [96000/100000]\tTrain Loss: 2091.8930703125\tTime: 0:00:00.237792\n",
            "Epoch: [97/100]\tSamples: [97000/100000]\tTrain Loss: 2091.749296875\tTime: 0:00:00.251926\n",
            "Epoch: [98/100]\tSamples: [98000/100000]\tTrain Loss: 2091.7805625\tTime: 0:00:00.268680\n",
            "Epoch: [99/100]\tSamples: [99000/100000]\tTrain Loss: 2090.6272890625\tTime: 0:00:00.252091\n",
            "Epoch: [100/100]\tSamples: [100000/100000]\tTrain Loss: 2092.0810859375\tTime: 0:00:00.255834\n",
            "Settings: \n",
            "               N Components: 10\n",
            "               Topic Prior Mean: 0.0\n",
            "               Topic Prior Variance: 0.9\n",
            "               Model Type: prodLDA\n",
            "               Hidden Sizes: (100, 100)\n",
            "               Activation: softplus\n",
            "               Dropout: 0.2\n",
            "               Learn Priors: True\n",
            "               Learning Rate: 0.002\n",
            "               Momentum: 0.99\n",
            "               Reduce On Plateau: False\n",
            "               Save Dir: None\n",
            "Epoch: [1/100]\tSamples: [1000/100000]\tTrain Loss: 2235.2393046875\tTime: 0:00:00.265645\n",
            "Epoch: [2/100]\tSamples: [2000/100000]\tTrain Loss: 2219.5362109375\tTime: 0:00:00.234683\n",
            "Epoch: [3/100]\tSamples: [3000/100000]\tTrain Loss: 2209.0646875\tTime: 0:00:00.246308\n",
            "Epoch: [4/100]\tSamples: [4000/100000]\tTrain Loss: 2201.4536953125\tTime: 0:00:00.247188\n",
            "Epoch: [5/100]\tSamples: [5000/100000]\tTrain Loss: 2195.405\tTime: 0:00:00.257653\n",
            "Epoch: [6/100]\tSamples: [6000/100000]\tTrain Loss: 2189.450046875\tTime: 0:00:00.249177\n",
            "Epoch: [7/100]\tSamples: [7000/100000]\tTrain Loss: 2185.635015625\tTime: 0:00:00.234320\n",
            "Epoch: [8/100]\tSamples: [8000/100000]\tTrain Loss: 2179.858140625\tTime: 0:00:00.248290\n",
            "Epoch: [9/100]\tSamples: [9000/100000]\tTrain Loss: 2176.4211796875\tTime: 0:00:00.243651\n",
            "Epoch: [10/100]\tSamples: [10000/100000]\tTrain Loss: 2173.424171875\tTime: 0:00:00.254061\n",
            "Epoch: [11/100]\tSamples: [11000/100000]\tTrain Loss: 2170.26359375\tTime: 0:00:00.242495\n",
            "Epoch: [12/100]\tSamples: [12000/100000]\tTrain Loss: 2166.3522890625\tTime: 0:00:00.241315\n",
            "Epoch: [13/100]\tSamples: [13000/100000]\tTrain Loss: 2164.729453125\tTime: 0:00:00.251876\n",
            "Epoch: [14/100]\tSamples: [14000/100000]\tTrain Loss: 2162.375046875\tTime: 0:00:00.258444\n",
            "Epoch: [15/100]\tSamples: [15000/100000]\tTrain Loss: 2157.9934296875\tTime: 0:00:00.268931\n",
            "Epoch: [16/100]\tSamples: [16000/100000]\tTrain Loss: 2158.2356328125\tTime: 0:00:00.252490\n",
            "Epoch: [17/100]\tSamples: [17000/100000]\tTrain Loss: 2157.1204921875\tTime: 0:00:00.249169\n",
            "Epoch: [18/100]\tSamples: [18000/100000]\tTrain Loss: 2155.4342578125\tTime: 0:00:00.253606\n",
            "Epoch: [19/100]\tSamples: [19000/100000]\tTrain Loss: 2152.6673671875\tTime: 0:00:00.262687\n",
            "Epoch: [20/100]\tSamples: [20000/100000]\tTrain Loss: 2149.1705\tTime: 0:00:00.254621\n",
            "Epoch: [21/100]\tSamples: [21000/100000]\tTrain Loss: 2148.578796875\tTime: 0:00:00.253846\n",
            "Epoch: [22/100]\tSamples: [22000/100000]\tTrain Loss: 2146.818078125\tTime: 0:00:00.261509\n",
            "Epoch: [23/100]\tSamples: [23000/100000]\tTrain Loss: 2146.1330859375\tTime: 0:00:00.243403\n",
            "Epoch: [24/100]\tSamples: [24000/100000]\tTrain Loss: 2141.2824140625\tTime: 0:00:00.245732\n",
            "Epoch: [25/100]\tSamples: [25000/100000]\tTrain Loss: 2139.7861953125\tTime: 0:00:00.249094\n",
            "Epoch: [26/100]\tSamples: [26000/100000]\tTrain Loss: 2138.641125\tTime: 0:00:00.247785\n",
            "Epoch: [27/100]\tSamples: [27000/100000]\tTrain Loss: 2136.10275\tTime: 0:00:00.246300\n",
            "Epoch: [28/100]\tSamples: [28000/100000]\tTrain Loss: 2134.6315078125\tTime: 0:00:00.251648\n",
            "Epoch: [29/100]\tSamples: [29000/100000]\tTrain Loss: 2131.798359375\tTime: 0:00:00.244671\n",
            "Epoch: [30/100]\tSamples: [30000/100000]\tTrain Loss: 2128.9294140625\tTime: 0:00:00.263569\n",
            "Epoch: [31/100]\tSamples: [31000/100000]\tTrain Loss: 2126.502765625\tTime: 0:00:00.262497\n",
            "Epoch: [32/100]\tSamples: [32000/100000]\tTrain Loss: 2122.714890625\tTime: 0:00:00.258776\n",
            "Epoch: [33/100]\tSamples: [33000/100000]\tTrain Loss: 2121.5034765625\tTime: 0:00:00.242390\n",
            "Epoch: [34/100]\tSamples: [34000/100000]\tTrain Loss: 2119.3501484375\tTime: 0:00:00.265983\n",
            "Epoch: [35/100]\tSamples: [35000/100000]\tTrain Loss: 2118.3511484375\tTime: 0:00:00.239990\n",
            "Epoch: [36/100]\tSamples: [36000/100000]\tTrain Loss: 2115.4516796875\tTime: 0:00:00.247933\n",
            "Epoch: [37/100]\tSamples: [37000/100000]\tTrain Loss: 2115.11709375\tTime: 0:00:00.248938\n",
            "Epoch: [38/100]\tSamples: [38000/100000]\tTrain Loss: 2113.6362421875\tTime: 0:00:00.262245\n",
            "Epoch: [39/100]\tSamples: [39000/100000]\tTrain Loss: 2111.4608828125\tTime: 0:00:00.252957\n",
            "Epoch: [40/100]\tSamples: [40000/100000]\tTrain Loss: 2110.4830703125\tTime: 0:00:00.247942\n",
            "Epoch: [41/100]\tSamples: [41000/100000]\tTrain Loss: 2108.347046875\tTime: 0:00:00.248524\n",
            "Epoch: [42/100]\tSamples: [42000/100000]\tTrain Loss: 2107.6464296875\tTime: 0:00:00.248020\n",
            "Epoch: [43/100]\tSamples: [43000/100000]\tTrain Loss: 2106.558046875\tTime: 0:00:00.248485\n",
            "Epoch: [44/100]\tSamples: [44000/100000]\tTrain Loss: 2105.91325\tTime: 0:00:00.251603\n",
            "Epoch: [45/100]\tSamples: [45000/100000]\tTrain Loss: 2105.120734375\tTime: 0:00:00.256595\n",
            "Epoch: [46/100]\tSamples: [46000/100000]\tTrain Loss: 2104.2461484375\tTime: 0:00:00.267675\n",
            "Epoch: [47/100]\tSamples: [47000/100000]\tTrain Loss: 2103.5813125\tTime: 0:00:00.250149\n",
            "Epoch: [48/100]\tSamples: [48000/100000]\tTrain Loss: 2103.1407421875\tTime: 0:00:00.250825\n",
            "Epoch: [49/100]\tSamples: [49000/100000]\tTrain Loss: 2102.3175390625\tTime: 0:00:00.253147\n",
            "Epoch: [50/100]\tSamples: [50000/100000]\tTrain Loss: 2102.5741171875\tTime: 0:00:00.254589\n",
            "Epoch: [51/100]\tSamples: [51000/100000]\tTrain Loss: 2101.7996796875\tTime: 0:00:00.249208\n",
            "Epoch: [52/100]\tSamples: [52000/100000]\tTrain Loss: 2100.89240625\tTime: 0:00:00.244803\n",
            "Epoch: [53/100]\tSamples: [53000/100000]\tTrain Loss: 2101.046828125\tTime: 0:00:00.249863\n",
            "Epoch: [54/100]\tSamples: [54000/100000]\tTrain Loss: 2100.6350234375\tTime: 0:00:00.266486\n",
            "Epoch: [55/100]\tSamples: [55000/100000]\tTrain Loss: 2100.4013359375\tTime: 0:00:00.252189\n",
            "Epoch: [56/100]\tSamples: [56000/100000]\tTrain Loss: 2100.566328125\tTime: 0:00:00.244238\n",
            "Epoch: [57/100]\tSamples: [57000/100000]\tTrain Loss: 2099.508859375\tTime: 0:00:00.244320\n",
            "Epoch: [58/100]\tSamples: [58000/100000]\tTrain Loss: 2099.5924765625\tTime: 0:00:00.262662\n",
            "Epoch: [59/100]\tSamples: [59000/100000]\tTrain Loss: 2099.0387734375\tTime: 0:00:00.236298\n",
            "Epoch: [60/100]\tSamples: [60000/100000]\tTrain Loss: 2098.8958203125\tTime: 0:00:00.252921\n",
            "Epoch: [61/100]\tSamples: [61000/100000]\tTrain Loss: 2098.056234375\tTime: 0:00:00.252963\n",
            "Epoch: [62/100]\tSamples: [62000/100000]\tTrain Loss: 2098.6604765625\tTime: 0:00:00.266495\n",
            "Epoch: [63/100]\tSamples: [63000/100000]\tTrain Loss: 2098.28665625\tTime: 0:00:00.257374\n",
            "Epoch: [64/100]\tSamples: [64000/100000]\tTrain Loss: 2098.6396640625\tTime: 0:00:00.260979\n",
            "Epoch: [65/100]\tSamples: [65000/100000]\tTrain Loss: 2097.756171875\tTime: 0:00:00.240177\n",
            "Epoch: [66/100]\tSamples: [66000/100000]\tTrain Loss: 2097.3837421875\tTime: 0:00:00.249566\n",
            "Epoch: [67/100]\tSamples: [67000/100000]\tTrain Loss: 2098.4750234375\tTime: 0:00:00.253609\n",
            "Epoch: [68/100]\tSamples: [68000/100000]\tTrain Loss: 2098.3610859375\tTime: 0:00:00.251949\n",
            "Epoch: [69/100]\tSamples: [69000/100000]\tTrain Loss: 2097.3753203125\tTime: 0:00:00.246922\n",
            "Epoch: [70/100]\tSamples: [70000/100000]\tTrain Loss: 2097.228265625\tTime: 0:00:00.240985\n",
            "Epoch: [71/100]\tSamples: [71000/100000]\tTrain Loss: 2097.268625\tTime: 0:00:00.271456\n",
            "Epoch: [72/100]\tSamples: [72000/100000]\tTrain Loss: 2097.6898046875\tTime: 0:00:00.247410\n",
            "Epoch: [73/100]\tSamples: [73000/100000]\tTrain Loss: 2097.1901015625\tTime: 0:00:00.250153\n",
            "Epoch: [74/100]\tSamples: [74000/100000]\tTrain Loss: 2097.0616484375\tTime: 0:00:00.261379\n",
            "Epoch: [75/100]\tSamples: [75000/100000]\tTrain Loss: 2097.10071875\tTime: 0:00:00.271659\n",
            "Epoch: [76/100]\tSamples: [76000/100000]\tTrain Loss: 2096.2956796875\tTime: 0:00:00.245334\n",
            "Epoch: [77/100]\tSamples: [77000/100000]\tTrain Loss: 2096.7251640625\tTime: 0:00:00.242113\n",
            "Epoch: [78/100]\tSamples: [78000/100000]\tTrain Loss: 2096.7295546875\tTime: 0:00:00.262930\n",
            "Epoch: [79/100]\tSamples: [79000/100000]\tTrain Loss: 2097.1882578125\tTime: 0:00:00.275962\n",
            "Epoch: [80/100]\tSamples: [80000/100000]\tTrain Loss: 2096.95915625\tTime: 0:00:00.252853\n",
            "Epoch: [81/100]\tSamples: [81000/100000]\tTrain Loss: 2096.578453125\tTime: 0:00:00.247976\n",
            "Epoch: [82/100]\tSamples: [82000/100000]\tTrain Loss: 2096.5603515625\tTime: 0:00:00.249303\n",
            "Epoch: [83/100]\tSamples: [83000/100000]\tTrain Loss: 2096.137171875\tTime: 0:00:00.254569\n",
            "Epoch: [84/100]\tSamples: [84000/100000]\tTrain Loss: 2096.8459921875\tTime: 0:00:00.253640\n",
            "Epoch: [85/100]\tSamples: [85000/100000]\tTrain Loss: 2096.2779609375\tTime: 0:00:00.251367\n",
            "Epoch: [86/100]\tSamples: [86000/100000]\tTrain Loss: 2096.7748359375\tTime: 0:00:00.262359\n",
            "Epoch: [87/100]\tSamples: [87000/100000]\tTrain Loss: 2096.1149375\tTime: 0:00:00.256377\n",
            "Epoch: [88/100]\tSamples: [88000/100000]\tTrain Loss: 2096.7978359375\tTime: 0:00:00.239408\n",
            "Epoch: [89/100]\tSamples: [89000/100000]\tTrain Loss: 2096.24165625\tTime: 0:00:00.251615\n",
            "Epoch: [90/100]\tSamples: [90000/100000]\tTrain Loss: 2096.45653125\tTime: 0:00:00.259527\n",
            "Epoch: [91/100]\tSamples: [91000/100000]\tTrain Loss: 2096.4185546875\tTime: 0:00:00.278304\n",
            "Epoch: [92/100]\tSamples: [92000/100000]\tTrain Loss: 2095.9335859375\tTime: 0:00:00.259798\n",
            "Epoch: [93/100]\tSamples: [93000/100000]\tTrain Loss: 2096.6010625\tTime: 0:00:00.265657\n",
            "Epoch: [94/100]\tSamples: [94000/100000]\tTrain Loss: 2096.4676015625\tTime: 0:00:00.251897\n",
            "Epoch: [95/100]\tSamples: [95000/100000]\tTrain Loss: 2095.252953125\tTime: 0:00:00.251655\n",
            "Epoch: [96/100]\tSamples: [96000/100000]\tTrain Loss: 2095.9282890625\tTime: 0:00:00.240987\n",
            "Epoch: [97/100]\tSamples: [97000/100000]\tTrain Loss: 2095.7285234375\tTime: 0:00:00.232012\n",
            "Epoch: [98/100]\tSamples: [98000/100000]\tTrain Loss: 2095.2259453125\tTime: 0:00:00.250302\n",
            "Epoch: [99/100]\tSamples: [99000/100000]\tTrain Loss: 2095.6666640625\tTime: 0:00:00.256155\n",
            "Epoch: [100/100]\tSamples: [100000/100000]\tTrain Loss: 2095.5893515625\tTime: 0:00:00.252041\n"
          ]
        }
      ],
      "source": [
        "train_datasets = []\n",
        "avitms = []\n",
        "id2tokens = []\n",
        "for corpus_node in documents_all:\n",
        "  cv = CountVectorizer(input='content', lowercase=True, stop_words='english',\n",
        "                      max_df=0.99, min_df=0.01, binary=False)\n",
        "  \n",
        "  docs = [\" \".join(corpus_node[i]) for i in np.arange(len(corpus_node))]\n",
        "\n",
        "  train_bow = cv.fit_transform(docs)\n",
        "  train_bow = train_bow.toarray()\n",
        "\n",
        "  idx2token = cv.get_feature_names()\n",
        "  input_size = len(idx2token)\n",
        "\n",
        "  id2token = {k: v for k, v in zip(range(0, len(idx2token)), idx2token)}\n",
        "  id2tokens.append(id2token)\n",
        "\n",
        "  train_data = BOWDataset(train_bow, idx2token)\n",
        "\n",
        "  avitm = AVITM(input_size=input_size, n_components=10, model_type='prodLDA',\n",
        "                hidden_sizes=(100, 100), activation='softplus', dropout=0.2,\n",
        "                learn_priors=True, batch_size=64, lr=2e-3, momentum=0.99,\n",
        "                solver='adam', num_epochs=100, reduce_on_plateau=False)\n",
        "\n",
        "  avitm.fit(train_data)\n",
        "  avitms.append(avitm)\n",
        "\n",
        "  train_datasets.append(train_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CD0GfefPvJD"
      },
      "source": [
        "# 2.1 Topics at each node"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-X9g0dNGk0bC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "9a67502d-605a-47cf-9e26-4e9fc5eb35e6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-41811cd0-0fd7-481b-b73d-0ba4509761f5\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>wd665</td>\n",
              "      <td>wd855</td>\n",
              "      <td>wd30</td>\n",
              "      <td>wd103</td>\n",
              "      <td>wd536</td>\n",
              "      <td>wd891</td>\n",
              "      <td>wd245</td>\n",
              "      <td>wd490</td>\n",
              "      <td>wd846</td>\n",
              "      <td>wd612</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>wd12</td>\n",
              "      <td>wd161</td>\n",
              "      <td>wd487</td>\n",
              "      <td>wd382</td>\n",
              "      <td>wd543</td>\n",
              "      <td>wd217</td>\n",
              "      <td>wd674</td>\n",
              "      <td>wd754</td>\n",
              "      <td>wd63</td>\n",
              "      <td>wd748</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>wd340</td>\n",
              "      <td>wd92</td>\n",
              "      <td>wd977</td>\n",
              "      <td>wd358</td>\n",
              "      <td>wd350</td>\n",
              "      <td>wd141</td>\n",
              "      <td>wd190</td>\n",
              "      <td>wd828</td>\n",
              "      <td>wd950</td>\n",
              "      <td>wd971</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>wd588</td>\n",
              "      <td>wd476</td>\n",
              "      <td>wd542</td>\n",
              "      <td>wd579</td>\n",
              "      <td>wd244</td>\n",
              "      <td>wd826</td>\n",
              "      <td>wd275</td>\n",
              "      <td>wd189</td>\n",
              "      <td>wd9</td>\n",
              "      <td>wd378</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>wd960</td>\n",
              "      <td>wd935</td>\n",
              "      <td>wd543</td>\n",
              "      <td>wd839</td>\n",
              "      <td>wd518</td>\n",
              "      <td>wd416</td>\n",
              "      <td>wd380</td>\n",
              "      <td>wd428</td>\n",
              "      <td>wd903</td>\n",
              "      <td>wd969</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>wd107</td>\n",
              "      <td>wd505</td>\n",
              "      <td>wd336</td>\n",
              "      <td>wd240</td>\n",
              "      <td>wd542</td>\n",
              "      <td>wd579</td>\n",
              "      <td>wd476</td>\n",
              "      <td>wd922</td>\n",
              "      <td>wd576</td>\n",
              "      <td>wd502</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>wd648</td>\n",
              "      <td>wd100</td>\n",
              "      <td>wd324</td>\n",
              "      <td>wd477</td>\n",
              "      <td>wd161</td>\n",
              "      <td>wd47</td>\n",
              "      <td>wd16</td>\n",
              "      <td>wd616</td>\n",
              "      <td>wd209</td>\n",
              "      <td>wd748</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>wd232</td>\n",
              "      <td>wd709</td>\n",
              "      <td>wd522</td>\n",
              "      <td>wd888</td>\n",
              "      <td>wd48</td>\n",
              "      <td>wd551</td>\n",
              "      <td>wd951</td>\n",
              "      <td>wd144</td>\n",
              "      <td>wd914</td>\n",
              "      <td>wd146</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>wd62</td>\n",
              "      <td>wd716</td>\n",
              "      <td>wd331</td>\n",
              "      <td>wd464</td>\n",
              "      <td>wd814</td>\n",
              "      <td>wd923</td>\n",
              "      <td>wd817</td>\n",
              "      <td>wd791</td>\n",
              "      <td>wd668</td>\n",
              "      <td>wd268</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>wd756</td>\n",
              "      <td>wd428</td>\n",
              "      <td>wd151</td>\n",
              "      <td>wd826</td>\n",
              "      <td>wd947</td>\n",
              "      <td>wd851</td>\n",
              "      <td>wd666</td>\n",
              "      <td>wd224</td>\n",
              "      <td>wd839</td>\n",
              "      <td>wd345</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-41811cd0-0fd7-481b-b73d-0ba4509761f5')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-41811cd0-0fd7-481b-b73d-0ba4509761f5 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-41811cd0-0fd7-481b-b73d-0ba4509761f5');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "       0      1      2      3      4      5      6      7      8      9\n",
              "0  wd665  wd855   wd30  wd103  wd536  wd891  wd245  wd490  wd846  wd612\n",
              "1   wd12  wd161  wd487  wd382  wd543  wd217  wd674  wd754   wd63  wd748\n",
              "2  wd340   wd92  wd977  wd358  wd350  wd141  wd190  wd828  wd950  wd971\n",
              "3  wd588  wd476  wd542  wd579  wd244  wd826  wd275  wd189    wd9  wd378\n",
              "4  wd960  wd935  wd543  wd839  wd518  wd416  wd380  wd428  wd903  wd969\n",
              "5  wd107  wd505  wd336  wd240  wd542  wd579  wd476  wd922  wd576  wd502\n",
              "6  wd648  wd100  wd324  wd477  wd161   wd47   wd16  wd616  wd209  wd748\n",
              "7  wd232  wd709  wd522  wd888   wd48  wd551  wd951  wd144  wd914  wd146\n",
              "8   wd62  wd716  wd331  wd464  wd814  wd923  wd817  wd791  wd668  wd268\n",
              "9  wd756  wd428  wd151  wd826  wd947  wd851  wd666  wd224  wd839  wd345"
            ]
          },
          "metadata": {},
          "execution_count": 94
        }
      ],
      "source": [
        "topics_all = []\n",
        "for node in np.arange(n_nodes):\n",
        "  topics = pd.DataFrame(avitm.get_topics(10)).T\n",
        "  topics_all.append(topics)\n",
        "topics_all[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4kezlbmRaRV"
      },
      "source": [
        "## 2.2 Document-topic distributions at each node"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "orpK--hqxQkG"
      },
      "outputs": [],
      "source": [
        "def get_doc_topic_distribution(avitm, dataset, n_samples=20):\n",
        "    avitm.model.eval()\n",
        "\n",
        "    loader = DataLoader(\n",
        "            avitm.train_data, batch_size=avitm.batch_size, shuffle=True,\n",
        "            num_workers=mp.cpu_count())\n",
        "\n",
        "    pbar = tqdm(n_samples, position=0, leave=True)\n",
        "\n",
        "    final_thetas = []\n",
        "    for sample_index in range(n_samples):\n",
        "        with torch.no_grad():\n",
        "            collect_theta = []\n",
        "\n",
        "            for batch_samples in loader:\n",
        "                X = batch_samples['X']\n",
        "\n",
        "                if avitm.USE_CUDA:\n",
        "                  X = X.cuda()\n",
        "\n",
        "                # forward pass\n",
        "                avitm.model.zero_grad()\n",
        "                \n",
        "                with torch.no_grad():\n",
        "                  posterior_mu, posterior_log_sigma = avitm.model.inf_net(X)\n",
        "\n",
        "                  # Generate samples from theta\n",
        "                  theta = F.softmax(\n",
        "                          avitm.model.reparameterize(posterior_mu, posterior_log_sigma), dim=1)\n",
        "                  theta = avitm.model.drop_theta(theta)\n",
        "\n",
        "                collect_theta.extend(theta.cpu().numpy().tolist())\n",
        "\n",
        "            pbar.update(1)\n",
        "            pbar.set_description(\"Sampling: [{}/{}]\".format(sample_index + 1, n_samples))\n",
        "\n",
        "            final_thetas.append(np.array(collect_theta))\n",
        "    pbar.close()\n",
        "    return np.sum(final_thetas, axis=0) / n_samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PXTjce2LlVRG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1645b6f4-e1bb-46d8-f11c-fcc6b766c11f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Sampling: [5/5]: : 5it [00:00,  5.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document-topic distribution node 0 \n",
            "(1000, 10)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Sampling: [5/5]: : 5it [00:00,  5.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document-topic distribution node 1 \n",
            "(1000, 10)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Sampling: [5/5]: : 5it [00:00,  5.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document-topic distribution node 2 \n",
            "(1000, 10)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Sampling: [5/5]: : 5it [00:00,  5.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document-topic distribution node 3 \n",
            "(1000, 10)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Sampling: [5/5]: : 5it [00:00,  5.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document-topic distribution node 4 \n",
            "(1000, 10)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "doc_topic_all = []\n",
        "for node in np.arange(n_nodes):\n",
        "  doc_topic = get_doc_topic_distribution(avitms[node], train_datasets[node], n_samples=5) # get all the topic predictions\n",
        "  print(\"Document-topic distribution node\", str(node), \"\")\n",
        "  doc_topic_all.append(doc_topic)\n",
        "  print(np.array(doc_topics).shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aEE_kyedRgYv"
      },
      "source": [
        "## 2.3 Word-topic distributions attained at each node"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "douV1llyTM4b"
      },
      "outputs": [],
      "source": [
        "def get_topic_word_distribution(avtim_model):\n",
        "  topic_word_matrix = avtim_model.model.beta.cpu().detach().numpy()\n",
        "  return softmax(topic_word_matrix, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BdM2jxRJUvk3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3ca1d6e-6643-471a-a7a1-af77c25dbdfb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['wd1', 'wd2', 'wd3', 'wd4', 'wd5', 'wd6', 'wd7', 'wd8', 'wd9', 'wd10', 'wd11', 'wd12', 'wd13', 'wd14', 'wd15', 'wd16', 'wd17', 'wd18', 'wd19', 'wd20', 'wd21', 'wd22', 'wd23', 'wd24', 'wd25', 'wd26', 'wd27', 'wd28', 'wd29', 'wd30', 'wd31', 'wd32', 'wd33', 'wd34', 'wd35', 'wd36', 'wd37', 'wd38', 'wd39', 'wd40', 'wd41', 'wd42', 'wd43', 'wd44', 'wd45', 'wd46', 'wd47', 'wd48', 'wd49', 'wd50', 'wd51', 'wd52', 'wd53', 'wd54', 'wd55', 'wd56', 'wd57', 'wd58', 'wd59', 'wd60', 'wd61', 'wd62', 'wd63', 'wd64', 'wd65', 'wd66', 'wd67', 'wd68', 'wd69', 'wd70', 'wd71', 'wd72', 'wd73', 'wd74', 'wd75', 'wd76', 'wd77', 'wd78', 'wd79', 'wd80', 'wd81', 'wd82', 'wd83', 'wd84', 'wd85', 'wd86', 'wd87', 'wd88', 'wd89', 'wd90', 'wd91', 'wd92', 'wd93', 'wd94', 'wd95', 'wd96', 'wd97', 'wd98', 'wd99', 'wd100', 'wd101', 'wd102', 'wd103', 'wd104', 'wd105', 'wd106', 'wd107', 'wd108', 'wd109', 'wd110', 'wd111', 'wd112', 'wd113', 'wd114', 'wd115', 'wd116', 'wd117', 'wd118', 'wd119', 'wd120', 'wd121', 'wd122', 'wd123', 'wd124', 'wd125', 'wd126', 'wd127', 'wd128', 'wd129', 'wd130', 'wd131', 'wd132', 'wd133', 'wd134', 'wd135', 'wd136', 'wd137', 'wd138', 'wd139', 'wd140', 'wd141', 'wd142', 'wd143', 'wd144', 'wd145', 'wd146', 'wd147', 'wd148', 'wd149', 'wd150', 'wd151', 'wd152', 'wd153', 'wd154', 'wd155', 'wd156', 'wd157', 'wd158', 'wd159', 'wd160', 'wd161', 'wd162', 'wd163', 'wd164', 'wd165', 'wd166', 'wd167', 'wd168', 'wd169', 'wd170', 'wd171', 'wd172', 'wd173', 'wd174', 'wd175', 'wd176', 'wd177', 'wd178', 'wd179', 'wd180', 'wd181', 'wd182', 'wd183', 'wd184', 'wd185', 'wd186', 'wd187', 'wd188', 'wd189', 'wd190', 'wd191', 'wd192', 'wd193', 'wd194', 'wd195', 'wd196', 'wd197', 'wd198', 'wd199', 'wd200', 'wd201', 'wd202', 'wd203', 'wd204', 'wd205', 'wd206', 'wd207', 'wd208', 'wd209', 'wd210', 'wd211', 'wd212', 'wd213', 'wd214', 'wd215', 'wd216', 'wd217', 'wd218', 'wd219', 'wd220', 'wd221', 'wd222', 'wd223', 'wd224', 'wd225', 'wd226', 'wd227', 'wd228', 'wd229', 'wd230', 'wd231', 'wd232', 'wd233', 'wd234', 'wd235', 'wd236', 'wd237', 'wd238', 'wd239', 'wd240', 'wd241', 'wd242', 'wd243', 'wd244', 'wd245', 'wd246', 'wd247', 'wd248', 'wd249', 'wd250', 'wd251', 'wd252', 'wd253', 'wd254', 'wd255', 'wd256', 'wd257', 'wd258', 'wd259', 'wd260', 'wd261', 'wd262', 'wd263', 'wd264', 'wd265', 'wd266', 'wd267', 'wd268', 'wd269', 'wd270', 'wd271', 'wd272', 'wd273', 'wd274', 'wd275', 'wd276', 'wd277', 'wd278', 'wd279', 'wd280', 'wd281', 'wd282', 'wd283', 'wd284', 'wd285', 'wd286', 'wd287', 'wd288', 'wd289', 'wd290', 'wd291', 'wd292', 'wd293', 'wd294', 'wd295', 'wd296', 'wd297', 'wd298', 'wd299', 'wd300', 'wd301', 'wd302', 'wd303', 'wd304', 'wd305', 'wd306', 'wd307', 'wd308', 'wd309', 'wd310', 'wd311', 'wd312', 'wd313', 'wd314', 'wd315', 'wd316', 'wd317', 'wd318', 'wd319', 'wd320', 'wd321', 'wd322', 'wd323', 'wd324', 'wd325', 'wd326', 'wd327', 'wd328', 'wd329', 'wd330', 'wd331', 'wd332', 'wd333', 'wd334', 'wd335', 'wd336', 'wd337', 'wd338', 'wd339', 'wd340', 'wd341', 'wd342', 'wd343', 'wd344', 'wd345', 'wd346', 'wd347', 'wd348', 'wd349', 'wd350', 'wd351', 'wd352', 'wd353', 'wd354', 'wd355', 'wd356', 'wd357', 'wd358', 'wd359', 'wd360', 'wd361', 'wd362', 'wd363', 'wd364', 'wd365', 'wd366', 'wd367', 'wd368', 'wd369', 'wd370', 'wd371', 'wd372', 'wd373', 'wd374', 'wd375', 'wd376', 'wd377', 'wd378', 'wd379', 'wd380', 'wd381', 'wd382', 'wd383', 'wd384', 'wd385', 'wd386', 'wd387', 'wd388', 'wd389', 'wd390', 'wd391', 'wd392', 'wd393', 'wd394', 'wd395', 'wd396', 'wd397', 'wd398', 'wd399', 'wd400', 'wd401', 'wd402', 'wd403', 'wd404', 'wd405', 'wd406', 'wd407', 'wd408', 'wd409', 'wd410', 'wd411', 'wd412', 'wd413', 'wd414', 'wd415', 'wd416', 'wd417', 'wd418', 'wd419', 'wd420', 'wd421', 'wd422', 'wd423', 'wd424', 'wd425', 'wd426', 'wd427', 'wd428', 'wd429', 'wd430', 'wd431', 'wd432', 'wd433', 'wd434', 'wd435', 'wd436', 'wd437', 'wd438', 'wd439', 'wd440', 'wd441', 'wd442', 'wd443', 'wd444', 'wd445', 'wd446', 'wd447', 'wd448', 'wd449', 'wd450', 'wd451', 'wd452', 'wd453', 'wd454', 'wd455', 'wd456', 'wd457', 'wd458', 'wd459', 'wd460', 'wd461', 'wd462', 'wd463', 'wd464', 'wd465', 'wd466', 'wd467', 'wd468', 'wd469', 'wd470', 'wd471', 'wd472', 'wd473', 'wd474', 'wd475', 'wd476', 'wd477', 'wd478', 'wd479', 'wd480', 'wd481', 'wd482', 'wd483', 'wd484', 'wd485', 'wd486', 'wd487', 'wd488', 'wd489', 'wd490', 'wd491', 'wd492', 'wd493', 'wd494', 'wd495', 'wd496', 'wd497', 'wd498', 'wd499', 'wd500', 'wd501', 'wd502', 'wd503', 'wd504', 'wd505', 'wd506', 'wd507', 'wd508', 'wd509', 'wd510', 'wd511', 'wd512', 'wd513', 'wd514', 'wd515', 'wd516', 'wd517', 'wd518', 'wd519', 'wd520', 'wd521', 'wd522', 'wd523', 'wd524', 'wd525', 'wd526', 'wd527', 'wd528', 'wd529', 'wd530', 'wd531', 'wd532', 'wd533', 'wd534', 'wd535', 'wd536', 'wd537', 'wd538', 'wd539', 'wd540', 'wd541', 'wd542', 'wd543', 'wd544', 'wd545', 'wd546', 'wd547', 'wd548', 'wd549', 'wd550', 'wd551', 'wd552', 'wd553', 'wd554', 'wd555', 'wd556', 'wd557', 'wd558', 'wd559', 'wd560', 'wd561', 'wd562', 'wd563', 'wd564', 'wd565', 'wd566', 'wd567', 'wd568', 'wd569', 'wd570', 'wd571', 'wd572', 'wd573', 'wd574', 'wd575', 'wd576', 'wd577', 'wd578', 'wd579', 'wd580', 'wd581', 'wd582', 'wd583', 'wd584', 'wd585', 'wd586', 'wd587', 'wd588', 'wd589', 'wd590', 'wd591', 'wd592', 'wd593', 'wd594', 'wd595', 'wd596', 'wd597', 'wd598', 'wd599', 'wd600', 'wd601', 'wd602', 'wd603', 'wd604', 'wd605', 'wd606', 'wd607', 'wd608', 'wd609', 'wd610', 'wd611', 'wd612', 'wd613', 'wd614', 'wd615', 'wd616', 'wd617', 'wd618', 'wd619', 'wd620', 'wd621', 'wd622', 'wd623', 'wd624', 'wd625', 'wd626', 'wd627', 'wd628', 'wd629', 'wd630', 'wd631', 'wd632', 'wd633', 'wd634', 'wd635', 'wd636', 'wd637', 'wd638', 'wd639', 'wd640', 'wd641', 'wd642', 'wd643', 'wd644', 'wd645', 'wd646', 'wd647', 'wd648', 'wd649', 'wd650', 'wd651', 'wd652', 'wd653', 'wd654', 'wd655', 'wd656', 'wd657', 'wd658', 'wd659', 'wd660', 'wd661', 'wd662', 'wd663', 'wd664', 'wd665', 'wd666', 'wd667', 'wd668', 'wd669', 'wd670', 'wd671', 'wd672', 'wd673', 'wd674', 'wd675', 'wd676', 'wd677', 'wd678', 'wd679', 'wd680', 'wd681', 'wd682', 'wd683', 'wd684', 'wd685', 'wd686', 'wd687', 'wd688', 'wd689', 'wd690', 'wd691', 'wd692', 'wd693', 'wd694', 'wd695', 'wd696', 'wd697', 'wd698', 'wd699', 'wd700', 'wd701', 'wd702', 'wd703', 'wd704', 'wd705', 'wd706', 'wd707', 'wd708', 'wd709', 'wd710', 'wd711', 'wd712', 'wd713', 'wd714', 'wd715', 'wd716', 'wd717', 'wd718', 'wd719', 'wd720', 'wd721', 'wd722', 'wd723', 'wd724', 'wd725', 'wd726', 'wd727', 'wd728', 'wd729', 'wd730', 'wd731', 'wd732', 'wd733', 'wd734', 'wd735', 'wd736', 'wd737', 'wd738', 'wd739', 'wd740', 'wd741', 'wd742', 'wd743', 'wd744', 'wd745', 'wd746', 'wd747', 'wd748', 'wd749', 'wd750', 'wd751', 'wd752', 'wd753', 'wd754', 'wd755', 'wd756', 'wd757', 'wd758', 'wd759', 'wd760', 'wd761', 'wd762', 'wd763', 'wd764', 'wd765', 'wd766', 'wd767', 'wd768', 'wd769', 'wd770', 'wd771', 'wd772', 'wd773', 'wd774', 'wd775', 'wd776', 'wd777', 'wd778', 'wd779', 'wd780', 'wd781', 'wd782', 'wd783', 'wd784', 'wd785', 'wd786', 'wd787', 'wd788', 'wd789', 'wd790', 'wd791', 'wd792', 'wd793', 'wd794', 'wd795', 'wd796', 'wd797', 'wd798', 'wd799', 'wd800', 'wd801', 'wd802', 'wd803', 'wd804', 'wd805', 'wd806', 'wd807', 'wd808', 'wd809', 'wd810', 'wd811', 'wd812', 'wd813', 'wd814', 'wd815', 'wd816', 'wd817', 'wd818', 'wd819', 'wd820', 'wd821', 'wd822', 'wd823', 'wd824', 'wd825', 'wd826', 'wd827', 'wd828', 'wd829', 'wd830', 'wd831', 'wd832', 'wd833', 'wd834', 'wd835', 'wd836', 'wd837', 'wd838', 'wd839', 'wd840', 'wd841', 'wd842', 'wd843', 'wd844', 'wd845', 'wd846', 'wd847', 'wd848', 'wd849', 'wd850', 'wd851', 'wd852', 'wd853', 'wd854', 'wd855', 'wd856', 'wd857', 'wd858', 'wd859', 'wd860', 'wd861', 'wd862', 'wd863', 'wd864', 'wd865', 'wd866', 'wd867', 'wd868', 'wd869', 'wd870', 'wd871', 'wd872', 'wd873', 'wd874', 'wd875', 'wd876', 'wd877', 'wd878', 'wd879', 'wd880', 'wd881', 'wd882', 'wd883', 'wd884', 'wd885', 'wd886', 'wd887', 'wd888', 'wd889', 'wd890', 'wd891', 'wd892', 'wd893', 'wd894', 'wd895', 'wd896', 'wd897', 'wd898', 'wd899', 'wd900', 'wd901', 'wd902', 'wd903', 'wd904', 'wd905', 'wd906', 'wd907', 'wd908', 'wd909', 'wd910', 'wd911', 'wd912', 'wd913', 'wd914', 'wd915', 'wd916', 'wd917', 'wd918', 'wd919', 'wd920', 'wd921', 'wd922', 'wd923', 'wd924', 'wd925', 'wd926', 'wd927', 'wd928', 'wd929', 'wd930', 'wd931', 'wd932', 'wd933', 'wd934', 'wd935', 'wd936', 'wd937', 'wd938', 'wd939', 'wd940', 'wd941', 'wd942', 'wd943', 'wd944', 'wd945', 'wd946', 'wd947', 'wd948', 'wd949', 'wd950', 'wd951', 'wd952', 'wd953', 'wd954', 'wd955', 'wd956', 'wd957', 'wd958', 'wd959', 'wd960', 'wd961', 'wd962', 'wd963', 'wd964', 'wd965', 'wd966', 'wd967', 'wd968', 'wd969', 'wd970', 'wd971', 'wd972', 'wd973', 'wd974', 'wd975', 'wd976', 'wd977', 'wd978', 'wd979', 'wd980', 'wd981', 'wd982', 'wd983', 'wd984', 'wd985', 'wd986', 'wd987', 'wd988', 'wd989', 'wd990', 'wd991', 'wd992', 'wd993', 'wd994', 'wd995', 'wd996', 'wd997', 'wd998', 'wd999', 'wd1000']\n"
          ]
        }
      ],
      "source": [
        "all_words = []\n",
        "for word in np.arange(vocab_size+1):\n",
        "  if word > 0:\n",
        "    all_words.append('wd'+str(word))\n",
        "print(all_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MJfQe7PdUmkC"
      },
      "outputs": [],
      "source": [
        "topic_word_all = []\n",
        "for node in np.arange(n_nodes):\n",
        "  w_t_distrib = np.zeros((10,vocab_size), dtype=np.float64) \n",
        "  wd = get_topic_word_distribution(avitms[node])\n",
        "  for i in np.arange(10):\n",
        "    for idx, word in id2tokens[node].items():\n",
        "      for j in np.arange(len(all_words)):\n",
        "        if all_words[j] == word:\n",
        "          w_t_distrib[i,j] = wd[i][idx]\n",
        "          break\n",
        "  sum_of_rows = w_t_distrib.sum(axis=1)\n",
        "  normalized_array = w_t_distrib / sum_of_rows[:, np.newaxis]\n",
        "  topic_word_all.append(normalized_array)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QieP5DdU7zY0"
      },
      "source": [
        "# 3. Centralized model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wTQrxfRpRrD1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6774f27b-f850-4ece-972b-0ed35ba64c60"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5000"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ],
      "source": [
        "documents_centr = [*documents_all[0], *documents_all[1], *documents_all[2], *documents_all[3], *documents_all[4]]\n",
        "len(documents_centr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kxW_tMtFRyfX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b60ffc3-c385-4d18-a688-640d64613594"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Settings: \n",
            "               N Components: 10\n",
            "               Topic Prior Mean: 0.0\n",
            "               Topic Prior Variance: 0.9\n",
            "               Model Type: prodLDA\n",
            "               Hidden Sizes: (100, 100)\n",
            "               Activation: softplus\n",
            "               Dropout: 0.2\n",
            "               Learn Priors: True\n",
            "               Learning Rate: 0.002\n",
            "               Momentum: 0.99\n",
            "               Reduce On Plateau: False\n",
            "               Save Dir: None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [1/100]\tSamples: [5000/500000]\tTrain Loss: 2182.6769125\tTime: 0:00:00.731813\n",
            "Epoch: [2/100]\tSamples: [10000/500000]\tTrain Loss: 2151.7567015625\tTime: 0:00:00.686643\n",
            "Epoch: [3/100]\tSamples: [15000/500000]\tTrain Loss: 2126.15091015625\tTime: 0:00:00.672844\n",
            "Epoch: [4/100]\tSamples: [20000/500000]\tTrain Loss: 2106.966294140625\tTime: 0:00:00.703287\n",
            "Epoch: [5/100]\tSamples: [25000/500000]\tTrain Loss: 2093.3443927734374\tTime: 0:00:00.694402\n",
            "Epoch: [6/100]\tSamples: [30000/500000]\tTrain Loss: 2085.500573046875\tTime: 0:00:00.678359\n",
            "Epoch: [7/100]\tSamples: [35000/500000]\tTrain Loss: 2080.1787390625\tTime: 0:00:00.716755\n",
            "Epoch: [8/100]\tSamples: [40000/500000]\tTrain Loss: 2076.3357255859373\tTime: 0:00:00.665941\n",
            "Epoch: [9/100]\tSamples: [45000/500000]\tTrain Loss: 2073.87744453125\tTime: 0:00:00.671495\n",
            "Epoch: [10/100]\tSamples: [50000/500000]\tTrain Loss: 2072.29969609375\tTime: 0:00:00.713119\n",
            "Epoch: [11/100]\tSamples: [55000/500000]\tTrain Loss: 2071.294019140625\tTime: 0:00:00.700074\n",
            "Epoch: [12/100]\tSamples: [60000/500000]\tTrain Loss: 2070.698065625\tTime: 0:00:00.695178\n",
            "Epoch: [13/100]\tSamples: [65000/500000]\tTrain Loss: 2069.756076171875\tTime: 0:00:00.703198\n",
            "Epoch: [14/100]\tSamples: [70000/500000]\tTrain Loss: 2069.567804296875\tTime: 0:00:00.702095\n",
            "Epoch: [15/100]\tSamples: [75000/500000]\tTrain Loss: 2068.72355\tTime: 0:00:00.689914\n",
            "Epoch: [16/100]\tSamples: [80000/500000]\tTrain Loss: 2068.1477611328123\tTime: 0:00:00.712423\n",
            "Epoch: [17/100]\tSamples: [85000/500000]\tTrain Loss: 2067.7392265625\tTime: 0:00:00.690870\n",
            "Epoch: [18/100]\tSamples: [90000/500000]\tTrain Loss: 2067.370769921875\tTime: 0:00:00.725453\n",
            "Epoch: [19/100]\tSamples: [95000/500000]\tTrain Loss: 2066.843028515625\tTime: 0:00:00.699376\n",
            "Epoch: [20/100]\tSamples: [100000/500000]\tTrain Loss: 2066.745421875\tTime: 0:00:00.695413\n",
            "Epoch: [21/100]\tSamples: [105000/500000]\tTrain Loss: 2066.392498046875\tTime: 0:00:00.688705\n",
            "Epoch: [22/100]\tSamples: [110000/500000]\tTrain Loss: 2066.389818359375\tTime: 0:00:00.702462\n",
            "Epoch: [23/100]\tSamples: [115000/500000]\tTrain Loss: 2066.164064453125\tTime: 0:00:00.690257\n",
            "Epoch: [24/100]\tSamples: [120000/500000]\tTrain Loss: 2066.21593046875\tTime: 0:00:00.672157\n",
            "Epoch: [25/100]\tSamples: [125000/500000]\tTrain Loss: 2066.319053125\tTime: 0:00:00.683977\n",
            "Epoch: [26/100]\tSamples: [130000/500000]\tTrain Loss: 2066.077576953125\tTime: 0:00:00.686189\n",
            "Epoch: [27/100]\tSamples: [135000/500000]\tTrain Loss: 2066.11187421875\tTime: 0:00:00.677665\n",
            "Epoch: [28/100]\tSamples: [140000/500000]\tTrain Loss: 2066.016581640625\tTime: 0:00:00.687006\n",
            "Epoch: [29/100]\tSamples: [145000/500000]\tTrain Loss: 2066.120925390625\tTime: 0:00:00.684757\n",
            "Epoch: [30/100]\tSamples: [150000/500000]\tTrain Loss: 2065.85243359375\tTime: 0:00:00.683612\n",
            "Epoch: [31/100]\tSamples: [155000/500000]\tTrain Loss: 2066.01754765625\tTime: 0:00:00.701333\n",
            "Epoch: [32/100]\tSamples: [160000/500000]\tTrain Loss: 2065.947141796875\tTime: 0:00:00.673361\n",
            "Epoch: [33/100]\tSamples: [165000/500000]\tTrain Loss: 2066.0399603515625\tTime: 0:00:00.688401\n",
            "Epoch: [34/100]\tSamples: [170000/500000]\tTrain Loss: 2065.761990234375\tTime: 0:00:00.673061\n",
            "Epoch: [35/100]\tSamples: [175000/500000]\tTrain Loss: 2065.59989921875\tTime: 0:00:00.700971\n",
            "Epoch: [36/100]\tSamples: [180000/500000]\tTrain Loss: 2065.750608203125\tTime: 0:00:00.695303\n",
            "Epoch: [37/100]\tSamples: [185000/500000]\tTrain Loss: 2065.32457265625\tTime: 0:00:00.701764\n",
            "Epoch: [38/100]\tSamples: [190000/500000]\tTrain Loss: 2065.565952734375\tTime: 0:00:00.692413\n",
            "Epoch: [39/100]\tSamples: [195000/500000]\tTrain Loss: 2065.594330078125\tTime: 0:00:00.673106\n",
            "Epoch: [40/100]\tSamples: [200000/500000]\tTrain Loss: 2065.537658984375\tTime: 0:00:00.693917\n",
            "Epoch: [41/100]\tSamples: [205000/500000]\tTrain Loss: 2065.400709375\tTime: 0:00:00.688228\n",
            "Epoch: [42/100]\tSamples: [210000/500000]\tTrain Loss: 2065.372977734375\tTime: 0:00:00.702465\n",
            "Epoch: [43/100]\tSamples: [215000/500000]\tTrain Loss: 2065.26556328125\tTime: 0:00:00.701820\n",
            "Epoch: [44/100]\tSamples: [220000/500000]\tTrain Loss: 2065.426362109375\tTime: 0:00:00.695601\n",
            "Epoch: [45/100]\tSamples: [225000/500000]\tTrain Loss: 2065.58701328125\tTime: 0:00:00.718165\n",
            "Epoch: [46/100]\tSamples: [230000/500000]\tTrain Loss: 2065.04643515625\tTime: 0:00:00.679708\n",
            "Epoch: [47/100]\tSamples: [235000/500000]\tTrain Loss: 2065.635734765625\tTime: 0:00:00.733949\n",
            "Epoch: [48/100]\tSamples: [240000/500000]\tTrain Loss: 2065.256001953125\tTime: 0:00:00.709866\n",
            "Epoch: [49/100]\tSamples: [245000/500000]\tTrain Loss: 2065.326871875\tTime: 0:00:00.679472\n",
            "Epoch: [50/100]\tSamples: [250000/500000]\tTrain Loss: 2065.511227734375\tTime: 0:00:00.696141\n",
            "Epoch: [51/100]\tSamples: [255000/500000]\tTrain Loss: 2065.176216796875\tTime: 0:00:00.688142\n",
            "Epoch: [52/100]\tSamples: [260000/500000]\tTrain Loss: 2065.2894697265624\tTime: 0:00:00.652297\n",
            "Epoch: [53/100]\tSamples: [265000/500000]\tTrain Loss: 2065.534677734375\tTime: 0:00:00.716440\n",
            "Epoch: [54/100]\tSamples: [270000/500000]\tTrain Loss: 2065.2096203125\tTime: 0:00:00.703089\n",
            "Epoch: [55/100]\tSamples: [275000/500000]\tTrain Loss: 2065.329305859375\tTime: 0:00:00.681693\n",
            "Epoch: [56/100]\tSamples: [280000/500000]\tTrain Loss: 2065.284222265625\tTime: 0:00:00.693454\n",
            "Epoch: [57/100]\tSamples: [285000/500000]\tTrain Loss: 2065.210575390625\tTime: 0:00:00.700199\n",
            "Epoch: [58/100]\tSamples: [290000/500000]\tTrain Loss: 2065.372925390625\tTime: 0:00:00.670791\n",
            "Epoch: [59/100]\tSamples: [295000/500000]\tTrain Loss: 2065.411741015625\tTime: 0:00:00.716599\n",
            "Epoch: [60/100]\tSamples: [300000/500000]\tTrain Loss: 2065.192978515625\tTime: 0:00:00.664519\n",
            "Epoch: [61/100]\tSamples: [305000/500000]\tTrain Loss: 2065.046483203125\tTime: 0:00:00.675074\n",
            "Epoch: [62/100]\tSamples: [310000/500000]\tTrain Loss: 2065.22380859375\tTime: 0:00:00.702088\n",
            "Epoch: [63/100]\tSamples: [315000/500000]\tTrain Loss: 2064.836160546875\tTime: 0:00:00.699922\n",
            "Epoch: [64/100]\tSamples: [320000/500000]\tTrain Loss: 2065.187352734375\tTime: 0:00:00.710865\n",
            "Epoch: [65/100]\tSamples: [325000/500000]\tTrain Loss: 2065.2257951171873\tTime: 0:00:00.713752\n",
            "Epoch: [66/100]\tSamples: [330000/500000]\tTrain Loss: 2065.55165234375\tTime: 0:00:00.691796\n",
            "Epoch: [67/100]\tSamples: [335000/500000]\tTrain Loss: 2065.37635625\tTime: 0:00:00.692159\n",
            "Epoch: [68/100]\tSamples: [340000/500000]\tTrain Loss: 2065.122550390625\tTime: 0:00:00.715701\n",
            "Epoch: [69/100]\tSamples: [345000/500000]\tTrain Loss: 2065.19168984375\tTime: 0:00:00.699366\n",
            "Epoch: [70/100]\tSamples: [350000/500000]\tTrain Loss: 2064.857532421875\tTime: 0:00:00.710353\n",
            "Epoch: [71/100]\tSamples: [355000/500000]\tTrain Loss: 2065.00145703125\tTime: 0:00:00.703275\n",
            "Epoch: [72/100]\tSamples: [360000/500000]\tTrain Loss: 2064.975887109375\tTime: 0:00:00.685391\n",
            "Epoch: [73/100]\tSamples: [365000/500000]\tTrain Loss: 2065.16627109375\tTime: 0:00:00.719912\n",
            "Epoch: [74/100]\tSamples: [370000/500000]\tTrain Loss: 2065.1730296875\tTime: 0:00:00.681189\n",
            "Epoch: [75/100]\tSamples: [375000/500000]\tTrain Loss: 2065.05016484375\tTime: 0:00:00.710165\n",
            "Epoch: [76/100]\tSamples: [380000/500000]\tTrain Loss: 2064.755198828125\tTime: 0:00:00.729841\n",
            "Epoch: [77/100]\tSamples: [385000/500000]\tTrain Loss: 2064.9417818359375\tTime: 0:00:00.685397\n",
            "Epoch: [78/100]\tSamples: [390000/500000]\tTrain Loss: 2065.058066015625\tTime: 0:00:00.679237\n",
            "Epoch: [79/100]\tSamples: [395000/500000]\tTrain Loss: 2065.1949775390626\tTime: 0:00:00.715997\n",
            "Epoch: [80/100]\tSamples: [400000/500000]\tTrain Loss: 2065.0460828125\tTime: 0:00:00.682931\n",
            "Epoch: [81/100]\tSamples: [405000/500000]\tTrain Loss: 2065.32093515625\tTime: 0:00:00.695040\n",
            "Epoch: [82/100]\tSamples: [410000/500000]\tTrain Loss: 2064.802633203125\tTime: 0:00:00.717157\n",
            "Epoch: [83/100]\tSamples: [415000/500000]\tTrain Loss: 2065.052616796875\tTime: 0:00:00.691389\n",
            "Epoch: [84/100]\tSamples: [420000/500000]\tTrain Loss: 2064.8822265625\tTime: 0:00:00.681627\n",
            "Epoch: [85/100]\tSamples: [425000/500000]\tTrain Loss: 2064.808341796875\tTime: 0:00:00.737880\n",
            "Epoch: [86/100]\tSamples: [430000/500000]\tTrain Loss: 2064.776709375\tTime: 0:00:00.686884\n",
            "Epoch: [87/100]\tSamples: [435000/500000]\tTrain Loss: 2064.99309296875\tTime: 0:00:00.689274\n",
            "Epoch: [88/100]\tSamples: [440000/500000]\tTrain Loss: 2065.207502734375\tTime: 0:00:00.703455\n",
            "Epoch: [89/100]\tSamples: [445000/500000]\tTrain Loss: 2065.08651640625\tTime: 0:00:00.681244\n",
            "Epoch: [90/100]\tSamples: [450000/500000]\tTrain Loss: 2064.972176953125\tTime: 0:00:00.702090\n",
            "Epoch: [91/100]\tSamples: [455000/500000]\tTrain Loss: 2065.386517578125\tTime: 0:00:00.698983\n",
            "Epoch: [92/100]\tSamples: [460000/500000]\tTrain Loss: 2064.6849951171876\tTime: 0:00:00.674127\n",
            "Epoch: [93/100]\tSamples: [465000/500000]\tTrain Loss: 2065.0480892578125\tTime: 0:00:00.730268\n",
            "Epoch: [94/100]\tSamples: [470000/500000]\tTrain Loss: 2064.99219765625\tTime: 0:00:00.678290\n",
            "Epoch: [95/100]\tSamples: [475000/500000]\tTrain Loss: 2064.9585244140626\tTime: 0:00:00.672343\n",
            "Epoch: [96/100]\tSamples: [480000/500000]\tTrain Loss: 2064.825635546875\tTime: 0:00:00.707980\n",
            "Epoch: [97/100]\tSamples: [485000/500000]\tTrain Loss: 2064.839328515625\tTime: 0:00:00.708762\n",
            "Epoch: [98/100]\tSamples: [490000/500000]\tTrain Loss: 2064.94541484375\tTime: 0:00:00.683157\n",
            "Epoch: [99/100]\tSamples: [495000/500000]\tTrain Loss: 2064.6014279296874\tTime: 0:00:00.737089\n",
            "Epoch: [100/100]\tSamples: [500000/500000]\tTrain Loss: 2064.833450390625\tTime: 0:00:00.742308\n"
          ]
        }
      ],
      "source": [
        "cv = CountVectorizer(input='content', lowercase=True, stop_words='english',\n",
        "                      max_df=0.99, min_df=0.01, binary=False)\n",
        "  \n",
        "docs_centr = [\" \".join(documents_centr[i]) for i in np.arange(len(documents_centr))]\n",
        "\n",
        "train_bow_centr = cv.fit_transform(docs_centr)\n",
        "train_bow_centr = train_bow_centr.toarray()\n",
        "\n",
        "idx2token_centr = cv.get_feature_names()\n",
        "input_size_centr = len(idx2token_centr)\n",
        "\n",
        "id2token_centr = {k: v for k, v in zip(range(0, len(idx2token_centr)), idx2token_centr)}\n",
        "\n",
        "train_data_centr = BOWDataset(train_bow_centr, idx2token_centr)\n",
        "\n",
        "avitm_centr = AVITM(input_size=input_size_centr, n_components=10, model_type='prodLDA',\n",
        "              hidden_sizes=(100, 100), activation='softplus', dropout=0.2,\n",
        "              learn_priors=True, batch_size=64, lr=2e-3, momentum=0.99,\n",
        "              solver='adam', num_epochs=100, reduce_on_plateau=False)\n",
        "\n",
        "avitm_centr.fit(train_data_centr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A3_xM2LUSYAN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "eff92ca3-feb2-4371-ad74-3220e8f14597"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-9317745d-821c-4bd9-a9a0-9eaa734d92d9\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>wd247</td>\n",
              "      <td>wd925</td>\n",
              "      <td>wd373</td>\n",
              "      <td>wd166</td>\n",
              "      <td>wd606</td>\n",
              "      <td>wd653</td>\n",
              "      <td>wd449</td>\n",
              "      <td>wd48</td>\n",
              "      <td>wd367</td>\n",
              "      <td>wd900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>wd177</td>\n",
              "      <td>wd399</td>\n",
              "      <td>wd518</td>\n",
              "      <td>wd824</td>\n",
              "      <td>wd722</td>\n",
              "      <td>wd307</td>\n",
              "      <td>wd914</td>\n",
              "      <td>wd590</td>\n",
              "      <td>wd190</td>\n",
              "      <td>wd327</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>wd579</td>\n",
              "      <td>wd476</td>\n",
              "      <td>wd922</td>\n",
              "      <td>wd107</td>\n",
              "      <td>wd588</td>\n",
              "      <td>wd240</td>\n",
              "      <td>wd38</td>\n",
              "      <td>wd542</td>\n",
              "      <td>wd708</td>\n",
              "      <td>wd780</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>wd795</td>\n",
              "      <td>wd720</td>\n",
              "      <td>wd614</td>\n",
              "      <td>wd51</td>\n",
              "      <td>wd411</td>\n",
              "      <td>wd258</td>\n",
              "      <td>wd732</td>\n",
              "      <td>wd529</td>\n",
              "      <td>wd756</td>\n",
              "      <td>wd252</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>wd216</td>\n",
              "      <td>wd874</td>\n",
              "      <td>wd640</td>\n",
              "      <td>wd522</td>\n",
              "      <td>wd248</td>\n",
              "      <td>wd839</td>\n",
              "      <td>wd829</td>\n",
              "      <td>wd564</td>\n",
              "      <td>wd424</td>\n",
              "      <td>wd951</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>wd836</td>\n",
              "      <td>wd161</td>\n",
              "      <td>wd681</td>\n",
              "      <td>wd139</td>\n",
              "      <td>wd312</td>\n",
              "      <td>wd998</td>\n",
              "      <td>wd198</td>\n",
              "      <td>wd471</td>\n",
              "      <td>wd332</td>\n",
              "      <td>wd657</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>wd490</td>\n",
              "      <td>wd891</td>\n",
              "      <td>wd665</td>\n",
              "      <td>wd157</td>\n",
              "      <td>wd705</td>\n",
              "      <td>wd354</td>\n",
              "      <td>wd31</td>\n",
              "      <td>wd855</td>\n",
              "      <td>wd304</td>\n",
              "      <td>wd575</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>wd791</td>\n",
              "      <td>wd62</td>\n",
              "      <td>wd151</td>\n",
              "      <td>wd492</td>\n",
              "      <td>wd412</td>\n",
              "      <td>wd564</td>\n",
              "      <td>wd753</td>\n",
              "      <td>wd191</td>\n",
              "      <td>wd688</td>\n",
              "      <td>wd811</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>wd497</td>\n",
              "      <td>wd340</td>\n",
              "      <td>wd977</td>\n",
              "      <td>wd825</td>\n",
              "      <td>wd350</td>\n",
              "      <td>wd141</td>\n",
              "      <td>wd928</td>\n",
              "      <td>wd586</td>\n",
              "      <td>wd498</td>\n",
              "      <td>wd971</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>wd44</td>\n",
              "      <td>wd405</td>\n",
              "      <td>wd593</td>\n",
              "      <td>wd965</td>\n",
              "      <td>wd741</td>\n",
              "      <td>wd733</td>\n",
              "      <td>wd893</td>\n",
              "      <td>wd191</td>\n",
              "      <td>wd783</td>\n",
              "      <td>wd112</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9317745d-821c-4bd9-a9a0-9eaa734d92d9')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-9317745d-821c-4bd9-a9a0-9eaa734d92d9 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-9317745d-821c-4bd9-a9a0-9eaa734d92d9');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "       0      1      2      3      4      5      6      7      8      9\n",
              "0  wd247  wd925  wd373  wd166  wd606  wd653  wd449   wd48  wd367  wd900\n",
              "1  wd177  wd399  wd518  wd824  wd722  wd307  wd914  wd590  wd190  wd327\n",
              "2  wd579  wd476  wd922  wd107  wd588  wd240   wd38  wd542  wd708  wd780\n",
              "3  wd795  wd720  wd614   wd51  wd411  wd258  wd732  wd529  wd756  wd252\n",
              "4  wd216  wd874  wd640  wd522  wd248  wd839  wd829  wd564  wd424  wd951\n",
              "5  wd836  wd161  wd681  wd139  wd312  wd998  wd198  wd471  wd332  wd657\n",
              "6  wd490  wd891  wd665  wd157  wd705  wd354   wd31  wd855  wd304  wd575\n",
              "7  wd791   wd62  wd151  wd492  wd412  wd564  wd753  wd191  wd688  wd811\n",
              "8  wd497  wd340  wd977  wd825  wd350  wd141  wd928  wd586  wd498  wd971\n",
              "9   wd44  wd405  wd593  wd965  wd741  wd733  wd893  wd191  wd783  wd112"
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ],
      "source": [
        "topics_centr = pd.DataFrame(avitm_centr.get_topics(10)).T\n",
        "topics_centr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z1w7cb9r7QAl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cdb8b122-06ee-4de6-d2aa-1c244c4ee880"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Sampling: [5/5]: : 5it [00:02,  2.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5000, 10)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "doc_topic_centr = get_doc_topic_distribution(avitm_centr, train_data_centr, n_samples=5) # get all the topic predictions\n",
        "print(doc_topic_centr.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YHry56Bz7sbV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "383db7b9-b594-4c81-89bb-55879d7e2cee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.00101796 0.00102315 0.00098507 ... 0.00093692 0.0010132  0.        ]\n",
            " [0.00104618 0.00105535 0.00095726 ... 0.00093917 0.00104994 0.        ]\n",
            " [0.00098471 0.00101512 0.00097898 ... 0.00093543 0.00101423 0.        ]\n",
            " ...\n",
            " [0.00107475 0.00096881 0.00117104 ... 0.00111766 0.00106301 0.        ]\n",
            " [0.00090882 0.00099511 0.00077688 ... 0.00093722 0.00099679 0.        ]\n",
            " [0.00106231 0.00104983 0.00093807 ... 0.00094112 0.00096766 0.        ]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0000000000000009"
            ]
          },
          "metadata": {},
          "execution_count": 104
        }
      ],
      "source": [
        "w_t_distrib_centr = np.zeros((10,vocab_size), dtype=np.float64) # vocab_size = 10000\n",
        "wd = get_topic_word_distribution(avitm_centr)\n",
        "for i in np.arange(10):\n",
        "  for idx, word in id2token_centr.items():\n",
        "    for j in np.arange(len(all_words)):\n",
        "      if all_words[j] == word:\n",
        "        w_t_distrib_centr[i,j] = wd[i][idx]\n",
        "        break\n",
        "sum_of_rows = w_t_distrib_centr.sum(axis=1)\n",
        "w_t_distrib_centr_norm = w_t_distrib_centr / sum_of_rows[:, np.newaxis]\n",
        "print(w_t_distrib_centr_norm)\n",
        "sum(w_t_distrib_centr_norm[8,:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usiERbR-8Roe"
      },
      "source": [
        "# 4. Get similarity through Frobenius"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ng1ykc1A9wkn"
      },
      "outputs": [],
      "source": [
        "doc_topic_centr_all = []\n",
        "doc_topic_centr_all.append(doc_topic_centr[0:1000,:])\n",
        "doc_topic_centr_all.append(doc_topic_centr[1000:2000,:])\n",
        "doc_topic_centr_all.append(doc_topic_centr[2000:3000,:])\n",
        "doc_topic_centr_all.append(doc_topic_centr[3000:4000,:])\n",
        "doc_topic_centr_all.append(doc_topic_centr[4000:5000,:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-NR5xZgQ8S8W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64b1790b-de20-4508-f146-e128e7688bea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NODE 0\n",
            "GT vs inferred in node: 267.1632296928841\n",
            "GT vs centralized in node 270.21938677231856\n",
            "***************************************************************\n",
            "NODE 1\n",
            "GT vs inferred in node: 271.0859365577861\n",
            "GT vs centralized in node 272.891673092457\n",
            "***************************************************************\n",
            "NODE 2\n",
            "GT vs inferred in node: 271.08456189554033\n",
            "GT vs centralized in node 278.5870288770558\n",
            "***************************************************************\n",
            "NODE 3\n",
            "GT vs inferred in node: 276.38301079591804\n",
            "GT vs centralized in node 279.2848359018112\n",
            "***************************************************************\n",
            "NODE 4\n",
            "GT vs inferred in node: 272.73272789093295\n",
            "GT vs centralized in node 276.90860309079216\n",
            "***************************************************************\n"
          ]
        }
      ],
      "source": [
        "for node in np.arange(n_nodes):\n",
        "  # Ground truth in node vs inferred in node\n",
        "  doc_topics_avitm_sqrt_node = np.sqrt(doc_topic_all[node])\n",
        "  similarity_avitm_node = doc_topics_avitm_sqrt_node.dot(doc_topics_avitm_sqrt_node.T)\n",
        "\n",
        "  doc_topics_gt_sqrt_node = np.sqrt(doc_topics_all_gt[node])\n",
        "  similarity_gt = doc_topics_gt_sqrt_node.dot(doc_topics_gt_sqrt_node.T)\n",
        "\n",
        "  diff_sims = similarity_avitm_node - similarity_gt\n",
        "  frobenius_diff_sims_node = np.linalg.norm(diff_sims,'fro')\n",
        "\n",
        "  # Ground truth in node vs centralized (for documents of such a node)\n",
        "  doc_topics_avitm_sqrt_centr_node = np.sqrt(doc_topic_centr_all[node])\n",
        "  similarity_avitm_centr = doc_topics_avitm_sqrt_centr_node.dot(doc_topics_avitm_sqrt_centr_node.T)\n",
        "\n",
        "  diff_sims = similarity_avitm_centr - similarity_gt\n",
        "  frobenius_diff_sims_avg = np.linalg.norm(diff_sims,'fro')\n",
        "\n",
        "  print(\"NODE\", str(node))\n",
        "  print(\"GT vs inferred in node:\", frobenius_diff_sims_node)\n",
        "  print(\"GT vs centralized in node\", frobenius_diff_sims_avg)\n",
        "  print(\"***************************************************************\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uQ6YfIbw-72y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b811565-cc7d-40e9-e14b-3537cba54854"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NODE 0\n",
            "Original vs inferred in node sum max row: 8.84163122705035\n",
            "***************************************************************\n",
            "NODE 1\n",
            "Original vs inferred in node sum max row: 8.843043981573329\n",
            "***************************************************************\n",
            "NODE 2\n",
            "Original vs inferred in node sum max row: 8.843346109519839\n",
            "***************************************************************\n",
            "NODE 3\n",
            "Original vs inferred in node sum max row: 8.842268636349548\n",
            "***************************************************************\n",
            "NODE 4\n",
            "Original vs inferred in node sum max row: 8.842929085873063\n",
            "***************************************************************\n",
            "CENTRALIZED\n",
            "Original vs avg of inferred in nodes sum max row 8.841334609077165\n"
          ]
        }
      ],
      "source": [
        "for node in np.arange(n_nodes):\n",
        "  # GT vs inferred in node\n",
        "  topic_words_gt_sqrt = np.sqrt(topic_vectors)\n",
        "  topic_words_avtim_node_sqrt = np.sqrt(topic_word_all[node])\n",
        "  simmat_t_w = topic_words_gt_sqrt.dot(topic_words_avtim_node_sqrt.T)\n",
        "\n",
        "  simmat_t_w_pd = pd.DataFrame(simmat_t_w)\n",
        "  maxValues_rows = simmat_t_w_pd.max(axis = 1)\n",
        "  max_values_rows_sum = maxValues_rows.sum()\n",
        "\n",
        "  print(\"NODE\", str(node))\n",
        "  print(\"Original vs inferred in node sum max row:\", max_values_rows_sum)\n",
        "  print(\"***************************************************************\")\n",
        "\n",
        "# GT vs centralized\n",
        "topic_words_avtim_centr_sqrt = np.sqrt(w_t_distrib_centr_norm)\n",
        "simmat_t_w = topic_words_gt_sqrt.dot(topic_words_avtim_centr_sqrt.T)\n",
        "\n",
        "simmat_t_w_pd = pd.DataFrame(simmat_t_w)\n",
        "maxValues_rows = simmat_t_w_pd.max(axis = 1)\n",
        "max_values_rows_sum_centr = maxValues_rows.sum()\n",
        "\n",
        "print(\"CENTRALIZED\")\n",
        "print(\"Original vs avg of inferred in nodes sum max row\", max_values_rows_sum_centr)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.imshow(simmat_t_w)\n",
        "plt.colorbar()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "f62uaxhLKIOF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "ae8fc922-2d42-4c57-c650-5e20abefa866"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Centralized-ProdLDA-vocab-1000_beta-1_prior-0.5.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}